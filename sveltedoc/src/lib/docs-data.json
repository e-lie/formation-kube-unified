{
  "docs": [
    {
      "slug": "part1_simple_aws_server",
      "title": "TP partie 1 - server simple avec AWS",
      "weight": 4,
      "metadata": {
        "title": "TP partie 1 - server simple avec AWS",
        "weight": 4
      },
      "content": "\n\nDans cette première partie de TP nous allons provisionner un simple serveur ubuntu avec AWS. C'est un peu un hello world de terraform.\n\n\n## Un déploiement avec un simple fichier\n\nCréez un dossier TP1 et ouvrez le dans VSCode. Créez à l'intérieur un fichier `main.tf`.\n\n- initialisez le un dépot git avec `git init`\n\n- Ajoutez un fichier `.gitignore` avec le contenu suivant adapté à terraform :\n\n```\n# Local .terraform directories\n**/.terraform/*\n\nhashicorp\n\n# .tfstate files\n*.tfstate\n*.tfstate.*\n\n# Crash log files\ncrash.log\ncrash.*.log\n\n# Exclude all .tfvars files, which are likely to contain sensitive data, such as\n# password, private keys, and other secrets. These should not be part of version \n# control as they are data points which are potentially sensitive and subject \n# to change depending on the environment.\n*.tfvars\n*.tfvars.json\n\n# Ignore override files as they are usually used to override resources locally and so\n# are not checked in\noverride.tf\noverride.tf.json\n*_override.tf\n*_override.tf.json\n\n# Include override files you do wish to add to version control using negated pattern\n# !example_override.tf\n\n# Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan\n# example: *tfplan*\n\n# Ignore CLI configuration files\n.terraformrc\nterraform.rc\n```\n\nPour pouvoir utiliser le provider AWS nous devons d'abord définir une dépendance au provider terraform avec le code suivant :\n\n```coffee\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n```\n\n- `terraform {}` : Bloc de configuration global de Terraform\n- `required_providers` : Définit les providers nécessaires\n- `source` : Registre officiel HashiCorp pour le provider AWS\n- `version` : Version ~> 5.0 (compatible avec 5.x mais pas 6.x)\n\n### Configuration du provider AWS\n\nPour nous connecter au provider AWS il faut bien sur un compte de cloud chez ce fournisseur et une façon de nous authentifier à son API. Il existe de nombreuses façon de faire cela mais les plus classiques sont :\n\n- utiliser des credentials au format:\n\n```coffee\n  access_key = \"my-access-key\"\n  secret_key = \"my-secret-key\"\n```\n\nCela permet de se connecter sans dépendre de la CLI AWS mais nécessite de gérer ces crédentials comme des secrets et éviter absolument de les ajouter au dépot Git. On pourrait également les fournir en ligne de commande ou sous forme de variables d'environnement au moment d'executer le code terraform.\n\n\nUne autre méthode classique, souvent plus simple et sécure consiste a utiliser un profil de connexion configuré au niveau de la CLI AWS.\n\n- Vérifiez que vous avez bien un profil awscli a votre nom préconfiguré sur la VM en lançant:\n\n```\naws configure list-profiles\naws s3 ls --profile <votreprenom> # ne devrait pas renvoyer d'erreur (rien du tout en fait)\n```\n\n- `export AWS_PROFILE=mon-profil` permet de définir le profil par défaut pour le shell en cours pour éviter de devoir ajouter l'argument --profile\n\n- Vous pouvez ensuite ajouter le bloc de code suivant en remplaçant le nom du profil par le votre :\n\n\n```coffee\nprovider \"aws\" {\n  region = \"us-east-1\"\n  profile = \"<awsprofile-votreprenom>\"\n}\n```\n- `provider \"aws\"` : Configuration du provider AWS\n- `region` : Région AWS où créer les ressources (Virginie du Nord)\n\n### Ressource instance EC2\n\nNous pouvons ensuite ajouter un bloc resource pour demander la création d'une instance (un serveur) :\n\n```coffee\nresource \"aws_instance\" \"web_server\" {\n  ami           = \"<identifiant d'un image server>\"\n  instance_type = \"t2.micro\"\n\n  tags = {\n    Name = \"Simple Web Server\"\n  }\n}\n```\n- `resource \"aws_instance\"` : Création d'une instance EC2\n- `ami` : Une AMI (Amazon Machine Image) est un modèle préconfiguré qui contient toutes les informations nécessaires pour lancer une instance EC2 dans AWS.\n- `instance_type` : Type t2.micro (utilisable dans le free tier amazon)\n- `tags` : Métadonnées pour identifier la ressource et la retrouver ensuite dans le cloud\n\n### 3. Source de données AMI Ubuntu\n\nIl nous faut donc récupérer l'identifiant d'une image de VM amazon. La bonne pratique pour cela est d'utiliser une source donnée terraform ou block `data`. Ce bloc déclenchera un appel à l'API pour récupérer dynamiquement l'identifiant de l'image ce qui permet d'avoir un identifiant a jour et de changer facilement l'image de base pour nos VM a runtime.\n\n```coffee\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  # \"099720109477\" is Canonical's official AWS account ID\n  owners = [\"099720109477\"]\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-22.04-amd64-server-*\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n```\n\n- `data \"aws_ami\"` : Recherche d'une AMI existante\n- `most_recent = true` : Prend la version la plus récente\n- `owners` : ID du compte Canonical (créateur d'Ubuntu)\n- Premier `filter` : Recherche les AMIs Ubuntu 22.04 LTS\n- Deuxième `filter` : Virtualisation hardware (HVM)\n\n\nMaintenant il nous faut modifier le paramètre `ami` pour l'instance de VM en utilisant l'identifiant récupéré par le block data:\n\n```coffee\nami = data.aws_ami.ubuntu.id\n```\n\n\n### 5. Sorties (Outputs)\n\nAjoutez les sorties suivantes pour afficher des informations concernant la resource créé (la sortie de votre module terraform)\n\n```coffee\noutput \"instance_id\" {\n  value = aws_instance.web_server.id\n}\n\noutput \"instance_public_ip\" {\n  value = aws_instance.web_server.public_ip\n}\n\noutput \"instance_public_dns\" {\n  value = aws_instance.web_server.public_dns\n}\n```\n\n- `instance_id` : ID unique de l'instance\n- `instance_public_ip` : Adresse IP publique\n- `instance_public_dns` : Nom DNS public\n\n## Commandes de déploiement\n\nExecutons et observons le résultat des commandes classiques\n\n1. `terraform init`\n2. `terraform plan` \n3. `terraform apply`\n\n\nPour vérifier que notre serveur a bien été créé on peut utiliser la CLI: \n\n`aws ec2 describe-instances --profile votreprenom`\n",
      "html": "<p>Dans cette première partie de TP nous allons provisionner un simple serveur ubuntu avec AWS. C&#39;est un peu un hello world de terraform.</p>\n<h2>Un déploiement avec un simple fichier</h2>\n<p>Créez un dossier TP1 et ouvrez le dans VSCode. Créez à l&#39;intérieur un fichier <code>main.tf</code>.</p>\n<ul>\n<li><p>initialisez le un dépot git avec <code>git init</code></p>\n</li>\n<li><p>Ajoutez un fichier <code>.gitignore</code> avec le contenu suivant adapté à terraform :</p>\n</li>\n</ul>\n<pre><code># Local .terraform directories\n**/.terraform/*\n\nhashicorp\n\n# .tfstate files\n*.tfstate\n*.tfstate.*\n\n# Crash log files\ncrash.log\ncrash.*.log\n\n# Exclude all .tfvars files, which are likely to contain sensitive data, such as\n# password, private keys, and other secrets. These should not be part of version \n# control as they are data points which are potentially sensitive and subject \n# to change depending on the environment.\n*.tfvars\n*.tfvars.json\n\n# Ignore override files as they are usually used to override resources locally and so\n# are not checked in\noverride.tf\noverride.tf.json\n*_override.tf\n*_override.tf.json\n\n# Include override files you do wish to add to version control using negated pattern\n# !example_override.tf\n\n# Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan\n# example: *tfplan*\n\n# Ignore CLI configuration files\n.terraformrc\nterraform.rc\n</code></pre>\n<p>Pour pouvoir utiliser le provider AWS nous devons d&#39;abord définir une dépendance au provider terraform avec le code suivant :</p>\n<pre><code class=\"language-coffee\">terraform {\n  required_providers {\n    aws = {\n      source  = &quot;hashicorp/aws&quot;\n      version = &quot;~&gt; 5.0&quot;\n    }\n  }\n}\n</code></pre>\n<ul>\n<li><code>terraform {}</code> : Bloc de configuration global de Terraform</li>\n<li><code>required_providers</code> : Définit les providers nécessaires</li>\n<li><code>source</code> : Registre officiel HashiCorp pour le provider AWS</li>\n<li><code>version</code> : Version ~&gt; 5.0 (compatible avec 5.x mais pas 6.x)</li>\n</ul>\n<h3>Configuration du provider AWS</h3>\n<p>Pour nous connecter au provider AWS il faut bien sur un compte de cloud chez ce fournisseur et une façon de nous authentifier à son API. Il existe de nombreuses façon de faire cela mais les plus classiques sont :</p>\n<ul>\n<li>utiliser des credentials au format:</li>\n</ul>\n<pre><code class=\"language-coffee\">  access_key = &quot;my-access-key&quot;\n  secret_key = &quot;my-secret-key&quot;\n</code></pre>\n<p>Cela permet de se connecter sans dépendre de la CLI AWS mais nécessite de gérer ces crédentials comme des secrets et éviter absolument de les ajouter au dépot Git. On pourrait également les fournir en ligne de commande ou sous forme de variables d&#39;environnement au moment d&#39;executer le code terraform.</p>\n<p>Une autre méthode classique, souvent plus simple et sécure consiste a utiliser un profil de connexion configuré au niveau de la CLI AWS.</p>\n<ul>\n<li>Vérifiez que vous avez bien un profil awscli a votre nom préconfiguré sur la VM en lançant:</li>\n</ul>\n<pre><code>aws configure list-profiles\naws s3 ls --profile &lt;votreprenom&gt; # ne devrait pas renvoyer d&#39;erreur (rien du tout en fait)\n</code></pre>\n<ul>\n<li><p><code>export AWS_PROFILE=mon-profil</code> permet de définir le profil par défaut pour le shell en cours pour éviter de devoir ajouter l&#39;argument --profile</p>\n</li>\n<li><p>Vous pouvez ensuite ajouter le bloc de code suivant en remplaçant le nom du profil par le votre :</p>\n</li>\n</ul>\n<pre><code class=\"language-coffee\">provider &quot;aws&quot; {\n  region = &quot;us-east-1&quot;\n  profile = &quot;&lt;awsprofile-votreprenom&gt;&quot;\n}\n</code></pre>\n<ul>\n<li><code>provider &quot;aws&quot;</code> : Configuration du provider AWS</li>\n<li><code>region</code> : Région AWS où créer les ressources (Virginie du Nord)</li>\n</ul>\n<h3>Ressource instance EC2</h3>\n<p>Nous pouvons ensuite ajouter un bloc resource pour demander la création d&#39;une instance (un serveur) :</p>\n<pre><code class=\"language-coffee\">resource &quot;aws_instance&quot; &quot;web_server&quot; {\n  ami           = &quot;&lt;identifiant d&#39;un image server&gt;&quot;\n  instance_type = &quot;t2.micro&quot;\n\n  tags = {\n    Name = &quot;Simple Web Server&quot;\n  }\n}\n</code></pre>\n<ul>\n<li><code>resource &quot;aws_instance&quot;</code> : Création d&#39;une instance EC2</li>\n<li><code>ami</code> : Une AMI (Amazon Machine Image) est un modèle préconfiguré qui contient toutes les informations nécessaires pour lancer une instance EC2 dans AWS.</li>\n<li><code>instance_type</code> : Type t2.micro (utilisable dans le free tier amazon)</li>\n<li><code>tags</code> : Métadonnées pour identifier la ressource et la retrouver ensuite dans le cloud</li>\n</ul>\n<h3>3. Source de données AMI Ubuntu</h3>\n<p>Il nous faut donc récupérer l&#39;identifiant d&#39;une image de VM amazon. La bonne pratique pour cela est d&#39;utiliser une source donnée terraform ou block <code>data</code>. Ce bloc déclenchera un appel à l&#39;API pour récupérer dynamiquement l&#39;identifiant de l&#39;image ce qui permet d&#39;avoir un identifiant a jour et de changer facilement l&#39;image de base pour nos VM a runtime.</p>\n<pre><code class=\"language-coffee\">data &quot;aws_ami&quot; &quot;ubuntu&quot; {\n  most_recent = true\n  # &quot;099720109477&quot; is Canonical&#39;s official AWS account ID\n  owners = [&quot;099720109477&quot;]\n\n  filter {\n    name   = &quot;name&quot;\n    values = [&quot;ubuntu/images/hvm-ssd/ubuntu-22.04-amd64-server-*&quot;]\n  }\n\n  filter {\n    name   = &quot;virtualization-type&quot;\n    values = [&quot;hvm&quot;]\n  }\n}\n</code></pre>\n<ul>\n<li><code>data &quot;aws_ami&quot;</code> : Recherche d&#39;une AMI existante</li>\n<li><code>most_recent = true</code> : Prend la version la plus récente</li>\n<li><code>owners</code> : ID du compte Canonical (créateur d&#39;Ubuntu)</li>\n<li>Premier <code>filter</code> : Recherche les AMIs Ubuntu 22.04 LTS</li>\n<li>Deuxième <code>filter</code> : Virtualisation hardware (HVM)</li>\n</ul>\n<p>Maintenant il nous faut modifier le paramètre <code>ami</code> pour l&#39;instance de VM en utilisant l&#39;identifiant récupéré par le block data:</p>\n<pre><code class=\"language-coffee\">ami = data.aws_ami.ubuntu.id\n</code></pre>\n<h3>5. Sorties (Outputs)</h3>\n<p>Ajoutez les sorties suivantes pour afficher des informations concernant la resource créé (la sortie de votre module terraform)</p>\n<pre><code class=\"language-coffee\">output &quot;instance_id&quot; {\n  value = aws_instance.web_server.id\n}\n\noutput &quot;instance_public_ip&quot; {\n  value = aws_instance.web_server.public_ip\n}\n\noutput &quot;instance_public_dns&quot; {\n  value = aws_instance.web_server.public_dns\n}\n</code></pre>\n<ul>\n<li><code>instance_id</code> : ID unique de l&#39;instance</li>\n<li><code>instance_public_ip</code> : Adresse IP publique</li>\n<li><code>instance_public_dns</code> : Nom DNS public</li>\n</ul>\n<h2>Commandes de déploiement</h2>\n<p>Executons et observons le résultat des commandes classiques</p>\n<ol>\n<li><code>terraform init</code></li>\n<li><code>terraform plan</code> </li>\n<li><code>terraform apply</code></li>\n</ol>\n<p>Pour vérifier que notre serveur a bien été créé on peut utiliser la CLI: </p>\n<p><code>aws ec2 describe-instances --profile votreprenom</code></p>\n"
    },
    {
      "slug": "part2_packer_ami",
      "title": "TP partie 2 - AMI personnalisée avec Packer",
      "weight": 5,
      "metadata": {
        "title": "TP partie 2 - AMI personnalisée avec Packer",
        "weight": 5
      },
      "content": "\nDans cette deuxième partie de TP nous allons créer une AMI personnalisée avec Packer puis l'utiliser dans Terraform. Nous partons d'Ubuntu 24.04 et ajoutons une clé SSH publique.\n\n## Structure du projet\n\n```\npart2_packer_ami/\n├── packer/\n│   └── ubuntu-custom.pkr.hcl\n├── main.tf\n└── part2.md\n```\n\n## Génération d'une paire de clés SSH\n\nAvant de créer notre AMI personnalisée, nous devons générer une paire de clés SSH qui sera utilisée pour l'accès à nos serveurs. Cette clé sera intégrée dans l'AMI et permettra une connexion sécurisée sans mot de passe.\n\n### Création de la clé SSH\n\nGénérez une nouvelle paire de clés SSH sans phrase de passe avec la commande suivante :\n\n```bash\nssh-keygen -t ed25519 -f ~/.ssh/id_terraform -N \"\"\n```\n\nCette commande crée deux fichiers :\n- `~/.ssh/id_terraform` : la clé privée (à garder secrète)\n- `~/.ssh/id_terraform.pub` : la clé publique (à intégrer dans l'AMI)\n\nAffichez le contenu de la clé publique pour l'utiliser dans Packer :\n\n```bash\ncat ~/.ssh/id_terraform.pub\n```\n\n## Création de l'AMI avec Packer\n\nPacker est un outil open-source qui permet de créer des images de machines automatiquement à partir d'une configuration déclarative. Dans cette partie nous allons créer une AMI Ubuntu 22.04 personnalisée avec notre clé SSH publique pré-installée.\n\n### Configuration Packer\n\nCréez un dossier `packer` et ajoutez-y un fichier `ubuntu-custom.pkr.hcl` avec le contenu suivant :\n\n```coffee\npacker {\n  required_plugins {\n    amazon = {\n      version = \">= 1.2.8\"\n      source  = \"github.com/hashicorp/amazon\"\n    }\n  }\n}\n\nvariable \"region\" {\n  type    = string\n  default = \"eu-west-3\"\n}\n\nvariable \"profile\" {\n  type    = string\n  default = \"<awsprofile-votreprenom>\"\n}\n\ndata \"amazon-ami\" \"ubuntu\" {\n  filters = {\n    name                = \"ubuntu/images/hvm-ssd/ubuntu-*-24.04-amd64-server-*\"\n    root-device-type    = \"ebs\"\n    virtualization-type = \"hvm\"\n  }\n  most_recent = true\n  owners      = [\"099720109477\"]\n  region      = var.region\n  profile     = var.profile\n}\n\nsource \"amazon-ebs\" \"ubuntu\" {\n  ami_name      = \"ubuntu-24.04-custom-{{timestamp}}\"\n  instance_type = \"t2.micro\"\n  region        = var.region\n  profile       = var.profile\n  source_ami    = data.amazon-ami.ubuntu.id\n  ssh_username  = \"ubuntu\"\n\n  tags = {\n    Name = \"Ubuntu 24.04 Custom AMI\"\n    OS   = \"Ubuntu\"\n    Version = \"24.04\"\n  }\n}\n\nbuild {\n  name = \"ubuntu-custom\"\n  sources = [\"source.amazon-ebs.ubuntu\"]\n\n  provisioner \"shell\" {\n    inline = [\n      \"sleep 30\",\n      \"sudo apt-get update\",\n      \"sudo mkdir -p /root/.ssh\",\n      \"echo '<VOTRE_CLÉ_PUBLIQUE>' | sudo tee /root/.ssh/authorized_keys\",\n      \"sudo chmod 700 /root/.ssh\",\n      \"sudo chmod 600 /root/.ssh/authorized_keys\",\n      \"sudo chown -R root:root /root/.ssh\",\n      \"echo 'Custom AMI build completed'\"\n    ]\n  }\n}\n```\n\nCe fichier définit le plugin Amazon nécessaire, les variables de configuration (région et profil AWS), la recherche de l'AMI Ubuntu 22.04 de base, la configuration de construction et les commandes à exécuter pour personnaliser l'image. Remplacez `<VOTRE_CLÉ_PUBLIQUE>` par le contenu de votre fichier `~/.ssh/id_terraform.pub` obtenu précédemment.\n\n### Construction de l'AMI\n\nUne fois le fichier de configuration créé, vous pouvez construire votre AMI personnalisée avec les commandes suivantes :\n\n```bash\ncd packer\npacker init ubuntu-custom.pkr.hcl\npacker validate ubuntu-custom.pkr.hcl\npacker build ubuntu-custom.pkr.hcl\n```\n\n## Utilisation de l'AMI avec Terraform\n\nMaintenant que nous avons créé notre AMI personnalisée, nous allons l'utiliser dans un projet Terraform. Cette fois-ci au lieu de rechercher une AMI publique Ubuntu, nous allons utiliser notre propre AMI qui contient déjà la clé SSH configurée.\n\n\n### Configuration du fichier main.tf\n\nModifiez un fichier `main.tf` à la racine du projet (pas dans le dossier packer) avec le contenu suivant :\n\n```coffee\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region  = \"eu-west-3\"\n  profile = \"<awsprofile-votreprenom>\"\n}\n\ndata \"aws_ami\" \"custom_ubuntu\" {\n  most_recent = true\n  owners      = [\"self\"]\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu-24.04-custom-*\"]\n  }\n\n  filter {\n    name   = \"state\"\n    values = [\"available\"]\n  }\n}\n\nresource \"aws_instance\" \"web_server\" {\n  ami           = data.aws_ami.custom_ubuntu.id\n  instance_type = \"t2.micro\"\n\n  tags = {\n    Name = \"Custom Ubuntu Server\"\n    Type = \"Packer-built\"\n  }\n}\n\noutput \"instance_id\" {\n  value = aws_instance.web_server.id\n}\n\noutput \"instance_public_ip\" {\n  value = aws_instance.web_server.public_ip\n}\n\noutput \"instance_public_dns\" {\n  value = aws_instance.web_server.public_dns\n}\n\noutput \"ami_id\" {\n  value = data.aws_ami.custom_ubuntu.id\n}\n\noutput \"ami_name\" {\n  value = data.aws_ami.custom_ubuntu.name\n}\n```\n\nLa différence principale avec la partie 1 est que le bloc `data \"aws_ami\"` utilise `owners = [\"self\"]` pour rechercher uniquement dans vos AMI personnelles au lieu des AMI publiques d'Ubuntu. Le filtre sur le nom correspond au pattern défini dans la configuration Packer.\n\n### Se connecter en SSH à notre serveur\n\nPour vous connecter à l'instance, utilisez la clé privée que nous avons générée :\n\n```sh\nssh -i ~/.ssh/id_terraform root@<ip publique de la sortie terraform>\n```\n\nNous n'arrivons pas à nous connecter parce que l'instance EC2 n'a pas de règles de sécurité réseau autorisant l'accès SSH. Un Security Group AWS est un pare-feu virtuel qui contrôle le trafic réseau entrant et sortant des instances EC2.\n\n## Configuration du Security Group\n\nPour autoriser l'accès SSH à notre instance, nous devons créer un Security Group avec les règles appropriées et l'associer à notre instance EC2.\n\n### Ajout du Security Group au fichier main.tf\n\nModifiez votre fichier `main.tf` pour ajouter le Security Group avant la ressource instance :\n\n```hcl\nresource \"aws_security_group\" \"web_ssh_access\" {\n  name        = \"web-ssh-access\"\n  description = \"Allow SSH and HTTP access\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"Web and SSH Access\"\n  }\n}\n\nresource \"aws_instance\" \"web_server\" {\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = \"t2.micro\"\n  vpc_security_group_ids = [aws_security_group.web_ssh_access.id]\n\n  tags = {\n    Name = \"Custom Ubuntu Server\"\n    Type = \"Packer-built\"\n  }\n}\n```\n\nCe Security Group définit deux règles d'entrée (`ingress`) : une qui autorise le trafic TCP sur le port 22 (SSH) et une autre sur le port 80 (HTTP) depuis n'importe quelle adresse IP (`0.0.0.0/0`). La règle de sortie (`egress`) autorise tout le trafic sortant. L'instance EC2 est maintenant associée à ce Security Group via le paramètre `vpc_security_group_ids`.\n\n### Application des modifications\n\nAprès avoir modifié le fichier, appliquez les changements avec :\n\n```bash\nterraform plan\nterraform apply\n```\n\nTerraform va créer le Security Group et modifier l'instance pour l'y associer. Une fois l'application terminée, vous devriez pouvoir vous connecter en SSH à votre instance.",
      "html": "<p>Dans cette deuxième partie de TP nous allons créer une AMI personnalisée avec Packer puis l&#39;utiliser dans Terraform. Nous partons d&#39;Ubuntu 24.04 et ajoutons une clé SSH publique.</p>\n<h2>Structure du projet</h2>\n<pre><code>part2_packer_ami/\n├── packer/\n│   └── ubuntu-custom.pkr.hcl\n├── main.tf\n└── part2.md\n</code></pre>\n<h2>Génération d&#39;une paire de clés SSH</h2>\n<p>Avant de créer notre AMI personnalisée, nous devons générer une paire de clés SSH qui sera utilisée pour l&#39;accès à nos serveurs. Cette clé sera intégrée dans l&#39;AMI et permettra une connexion sécurisée sans mot de passe.</p>\n<h3>Création de la clé SSH</h3>\n<p>Générez une nouvelle paire de clés SSH sans phrase de passe avec la commande suivante :</p>\n<pre><code class=\"language-bash\">ssh-keygen -t ed25519 -f ~/.ssh/id_terraform -N &quot;&quot;\n</code></pre>\n<p>Cette commande crée deux fichiers :</p>\n<ul>\n<li><code>~/.ssh/id_terraform</code> : la clé privée (à garder secrète)</li>\n<li><code>~/.ssh/id_terraform.pub</code> : la clé publique (à intégrer dans l&#39;AMI)</li>\n</ul>\n<p>Affichez le contenu de la clé publique pour l&#39;utiliser dans Packer :</p>\n<pre><code class=\"language-bash\">cat ~/.ssh/id_terraform.pub\n</code></pre>\n<h2>Création de l&#39;AMI avec Packer</h2>\n<p>Packer est un outil open-source qui permet de créer des images de machines automatiquement à partir d&#39;une configuration déclarative. Dans cette partie nous allons créer une AMI Ubuntu 22.04 personnalisée avec notre clé SSH publique pré-installée.</p>\n<h3>Configuration Packer</h3>\n<p>Créez un dossier <code>packer</code> et ajoutez-y un fichier <code>ubuntu-custom.pkr.hcl</code> avec le contenu suivant :</p>\n<pre><code class=\"language-coffee\">packer {\n  required_plugins {\n    amazon = {\n      version = &quot;&gt;= 1.2.8&quot;\n      source  = &quot;github.com/hashicorp/amazon&quot;\n    }\n  }\n}\n\nvariable &quot;region&quot; {\n  type    = string\n  default = &quot;eu-west-3&quot;\n}\n\nvariable &quot;profile&quot; {\n  type    = string\n  default = &quot;&lt;awsprofile-votreprenom&gt;&quot;\n}\n\ndata &quot;amazon-ami&quot; &quot;ubuntu&quot; {\n  filters = {\n    name                = &quot;ubuntu/images/hvm-ssd/ubuntu-*-24.04-amd64-server-*&quot;\n    root-device-type    = &quot;ebs&quot;\n    virtualization-type = &quot;hvm&quot;\n  }\n  most_recent = true\n  owners      = [&quot;099720109477&quot;]\n  region      = var.region\n  profile     = var.profile\n}\n\nsource &quot;amazon-ebs&quot; &quot;ubuntu&quot; {\n  ami_name      = &quot;ubuntu-24.04-custom-{{timestamp}}&quot;\n  instance_type = &quot;t2.micro&quot;\n  region        = var.region\n  profile       = var.profile\n  source_ami    = data.amazon-ami.ubuntu.id\n  ssh_username  = &quot;ubuntu&quot;\n\n  tags = {\n    Name = &quot;Ubuntu 24.04 Custom AMI&quot;\n    OS   = &quot;Ubuntu&quot;\n    Version = &quot;24.04&quot;\n  }\n}\n\nbuild {\n  name = &quot;ubuntu-custom&quot;\n  sources = [&quot;source.amazon-ebs.ubuntu&quot;]\n\n  provisioner &quot;shell&quot; {\n    inline = [\n      &quot;sleep 30&quot;,\n      &quot;sudo apt-get update&quot;,\n      &quot;sudo mkdir -p /root/.ssh&quot;,\n      &quot;echo &#39;&lt;VOTRE_CLÉ_PUBLIQUE&gt;&#39; | sudo tee /root/.ssh/authorized_keys&quot;,\n      &quot;sudo chmod 700 /root/.ssh&quot;,\n      &quot;sudo chmod 600 /root/.ssh/authorized_keys&quot;,\n      &quot;sudo chown -R root:root /root/.ssh&quot;,\n      &quot;echo &#39;Custom AMI build completed&#39;&quot;\n    ]\n  }\n}\n</code></pre>\n<p>Ce fichier définit le plugin Amazon nécessaire, les variables de configuration (région et profil AWS), la recherche de l&#39;AMI Ubuntu 22.04 de base, la configuration de construction et les commandes à exécuter pour personnaliser l&#39;image. Remplacez <code>&lt;VOTRE_CLÉ_PUBLIQUE&gt;</code> par le contenu de votre fichier <code>~/.ssh/id_terraform.pub</code> obtenu précédemment.</p>\n<h3>Construction de l&#39;AMI</h3>\n<p>Une fois le fichier de configuration créé, vous pouvez construire votre AMI personnalisée avec les commandes suivantes :</p>\n<pre><code class=\"language-bash\">cd packer\npacker init ubuntu-custom.pkr.hcl\npacker validate ubuntu-custom.pkr.hcl\npacker build ubuntu-custom.pkr.hcl\n</code></pre>\n<h2>Utilisation de l&#39;AMI avec Terraform</h2>\n<p>Maintenant que nous avons créé notre AMI personnalisée, nous allons l&#39;utiliser dans un projet Terraform. Cette fois-ci au lieu de rechercher une AMI publique Ubuntu, nous allons utiliser notre propre AMI qui contient déjà la clé SSH configurée.</p>\n<h3>Configuration du fichier main.tf</h3>\n<p>Modifiez un fichier <code>main.tf</code> à la racine du projet (pas dans le dossier packer) avec le contenu suivant :</p>\n<pre><code class=\"language-coffee\">terraform {\n  required_providers {\n    aws = {\n      source  = &quot;hashicorp/aws&quot;\n      version = &quot;~&gt; 5.0&quot;\n    }\n  }\n}\n\nprovider &quot;aws&quot; {\n  region  = &quot;eu-west-3&quot;\n  profile = &quot;&lt;awsprofile-votreprenom&gt;&quot;\n}\n\ndata &quot;aws_ami&quot; &quot;custom_ubuntu&quot; {\n  most_recent = true\n  owners      = [&quot;self&quot;]\n\n  filter {\n    name   = &quot;name&quot;\n    values = [&quot;ubuntu-24.04-custom-*&quot;]\n  }\n\n  filter {\n    name   = &quot;state&quot;\n    values = [&quot;available&quot;]\n  }\n}\n\nresource &quot;aws_instance&quot; &quot;web_server&quot; {\n  ami           = data.aws_ami.custom_ubuntu.id\n  instance_type = &quot;t2.micro&quot;\n\n  tags = {\n    Name = &quot;Custom Ubuntu Server&quot;\n    Type = &quot;Packer-built&quot;\n  }\n}\n\noutput &quot;instance_id&quot; {\n  value = aws_instance.web_server.id\n}\n\noutput &quot;instance_public_ip&quot; {\n  value = aws_instance.web_server.public_ip\n}\n\noutput &quot;instance_public_dns&quot; {\n  value = aws_instance.web_server.public_dns\n}\n\noutput &quot;ami_id&quot; {\n  value = data.aws_ami.custom_ubuntu.id\n}\n\noutput &quot;ami_name&quot; {\n  value = data.aws_ami.custom_ubuntu.name\n}\n</code></pre>\n<p>La différence principale avec la partie 1 est que le bloc <code>data &quot;aws_ami&quot;</code> utilise <code>owners = [&quot;self&quot;]</code> pour rechercher uniquement dans vos AMI personnelles au lieu des AMI publiques d&#39;Ubuntu. Le filtre sur le nom correspond au pattern défini dans la configuration Packer.</p>\n<h3>Se connecter en SSH à notre serveur</h3>\n<p>Pour vous connecter à l&#39;instance, utilisez la clé privée que nous avons générée :</p>\n<pre><code class=\"language-sh\">ssh -i ~/.ssh/id_terraform root@&lt;ip publique de la sortie terraform&gt;\n</code></pre>\n<p>Nous n&#39;arrivons pas à nous connecter parce que l&#39;instance EC2 n&#39;a pas de règles de sécurité réseau autorisant l&#39;accès SSH. Un Security Group AWS est un pare-feu virtuel qui contrôle le trafic réseau entrant et sortant des instances EC2.</p>\n<h2>Configuration du Security Group</h2>\n<p>Pour autoriser l&#39;accès SSH à notre instance, nous devons créer un Security Group avec les règles appropriées et l&#39;associer à notre instance EC2.</p>\n<h3>Ajout du Security Group au fichier main.tf</h3>\n<p>Modifiez votre fichier <code>main.tf</code> pour ajouter le Security Group avant la ressource instance :</p>\n<pre><code class=\"language-hcl\">resource &quot;aws_security_group&quot; &quot;web_ssh_access&quot; {\n  name        = &quot;web-ssh-access&quot;\n  description = &quot;Allow SSH and HTTP access&quot;\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = &quot;-1&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  tags = {\n    Name = &quot;Web and SSH Access&quot;\n  }\n}\n\nresource &quot;aws_instance&quot; &quot;web_server&quot; {\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = &quot;t2.micro&quot;\n  vpc_security_group_ids = [aws_security_group.web_ssh_access.id]\n\n  tags = {\n    Name = &quot;Custom Ubuntu Server&quot;\n    Type = &quot;Packer-built&quot;\n  }\n}\n</code></pre>\n<p>Ce Security Group définit deux règles d&#39;entrée (<code>ingress</code>) : une qui autorise le trafic TCP sur le port 22 (SSH) et une autre sur le port 80 (HTTP) depuis n&#39;importe quelle adresse IP (<code>0.0.0.0/0</code>). La règle de sortie (<code>egress</code>) autorise tout le trafic sortant. L&#39;instance EC2 est maintenant associée à ce Security Group via le paramètre <code>vpc_security_group_ids</code>.</p>\n<h3>Application des modifications</h3>\n<p>Après avoir modifié le fichier, appliquez les changements avec :</p>\n<pre><code class=\"language-bash\">terraform plan\nterraform apply\n</code></pre>\n<p>Terraform va créer le Security Group et modifier l&#39;instance pour l&#39;y associer. Une fois l&#39;application terminée, vous devriez pouvoir vous connecter en SSH à votre instance.</p>\n"
    },
    {
      "slug": "part3_terraform_provisioner",
      "title": "TP partie 3 - Provisionnement SSH avec Terraform",
      "weight": 6,
      "metadata": {
        "title": "TP partie 3 - Provisionnement SSH avec Terraform",
        "weight": 6
      },
      "content": "\nDans cette troisième partie, nous allons découvrir comment utiliser les provisioners Terraform pour configurer automatiquement notre serveur après sa création. Nous allons installer et configurer Nginx en utilisant le provisioner SSH remote-exec.\n\n## Structure du projet\n\n```\npart3_terraform_provisioner/\n├── main.tf\n└── part3.md\n```\n\n## Les provisioners Terraform\n\nLes provisioners Terraform permettent d'exécuter des scripts ou des commandes sur une ressource locale ou distante après sa création. Ils sont utiles pour installer des logiciels, configurer des services ou effectuer toute autre tâche de configuration initiale. Cependant, il est important de noter que les provisioners sont considérés comme un dernier recours - il est généralement préférable d'utiliser des images pré-configurées (comme nous l'avons fait avec Packer) ou des outils de gestion de configuration dédiés.\n\n### Configuration du fichier main.tf\n\nCréez un fichier `main.tf` avec le contenu suivant qui reprend la base du projet part2 et ajoute un provisioner SSH :\n\n```coffee\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region  = \"eu-west-3\"\n  profile = \"<awsprofile-votreprenom>\"\n}\n\ndata \"aws_ami\" \"custom_ubuntu\" {\n  most_recent = true\n  owners      = [\"self\"]\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu-22.04-custom-*\"]\n  }\n\n  filter {\n    name   = \"state\"\n    values = [\"available\"]\n  }\n}\n\nresource \"aws_security_group\" \"web_ssh_access\" {\n  name        = \"web-ssh-access\"\n  description = \"Allow SSH and HTTP access\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"Web and SSH Access\"\n  }\n}\n\nresource \"aws_instance\" \"web_server\" {\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = \"t2.micro\"\n  vpc_security_group_ids = [aws_security_group.web_ssh_access.id]\n\n  connection {\n    type        = \"ssh\"\n    user        = \"root\"\n    private_key = file(\"~/.ssh/id_terraform\")\n    host        = self.public_ip\n  }\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"apt-get update\",\n      \"apt-get install -y nginx\",\n      \"systemctl start nginx\",\n      \"systemctl enable nginx\",\n      \"echo '<h1>Hello from Terraform Provisioner!</h1>' > /var/www/html/index.html\",\n      \"echo 'Nginx installed and configured successfully'\"\n    ]\n  }\n\n  tags = {\n    Name = \"Nginx Web Server\"\n    Type = \"Provisioner-configured\"\n  }\n}\n\noutput \"instance_id\" {\n  value = aws_instance.web_server.id\n}\n\noutput \"instance_public_ip\" {\n  value = aws_instance.web_server.public_ip\n}\n\noutput \"instance_public_dns\" {\n  value = aws_instance.web_server.public_dns\n}\n\noutput \"web_url\" {\n  value = \"http://${aws_instance.web_server.public_ip}\"\n}\n```\n\n## Nouveaux éléments dans ce projet\n\n### Réutilisation du Security Group\n\nNous réutilisons le même Security Group que dans la partie 2, qui autorise déjà le SSH (port 22) et le trafic HTTP (port 80). Cette configuration avait été mise en place en prévision de l'installation d'un serveur web.\n\n### Bloc connection\n\nLe bloc `connection` définit comment Terraform doit se connecter à l'instance pour exécuter les commandes. Il spécifie le type de connexion (SSH), l'utilisateur (root grâce à notre AMI personnalisée), la clé privée à utiliser (`~/.ssh/id_terraform` générée dans la partie 2) et l'adresse IP de l'hôte. L'utilisation de `self.public_ip` permet de référencer dynamiquement l'IP publique de l'instance en cours de création.\n\n### Provisioner remote-exec\n\nLe provisioner `remote-exec` exécute une série de commandes sur l'instance distante via SSH. Dans notre cas, il met à jour les paquets système, installe Nginx, démarre le service, l'active au démarrage et crée une page d'accueil personnalisée. Les commandes sont exécutées dans l'ordre et si l'une échoue, le provisionnement s'arrête.\n\n### Output web_url\n\nUn nouvel output a été ajouté pour afficher directement l'URL du serveur web, facilitant ainsi l'accès au site après le déploiement.\n\n## Déploiement et vérification\n\nExécutez les commandes suivantes pour déployer l'infrastructure :\n\n```bash\nterraform init\nterraform plan\nterraform apply\n```\n\nUne fois le déploiement terminé, Terraform affichera l'URL du serveur web. Vous pouvez vérifier que Nginx fonctionne en ouvrant cette URL dans votre navigateur. Vous devriez voir le message \"Hello from Terraform Provisioner!\".\n\n## Points importants sur les provisioners\n\nLes provisioners présentent certaines limitations qu'il est important de connaître. Ils ne s'exécutent qu'une seule fois lors de la création de la ressource et ne peuvent pas être réexécutés facilement. Si le provisionnement échoue, Terraform marque la ressource comme \"tainted\" et elle devra être recréée lors du prochain apply. De plus, les provisioners rendent l'infrastructure moins reproductible car leur succès peut dépendre de facteurs externes comme la disponibilité des paquets ou la connectivité réseau.\n\nPour ces raisons, il est généralement recommandé d'utiliser des images pré-configurées avec Packer (comme dans la partie 2) ou des outils de gestion de configuration comme Ansible pour des configurations plus complexes. Les provisioners restent utiles pour des configurations simples ou des prototypes rapides.",
      "html": "<p>Dans cette troisième partie, nous allons découvrir comment utiliser les provisioners Terraform pour configurer automatiquement notre serveur après sa création. Nous allons installer et configurer Nginx en utilisant le provisioner SSH remote-exec.</p>\n<h2>Structure du projet</h2>\n<pre><code>part3_terraform_provisioner/\n├── main.tf\n└── part3.md\n</code></pre>\n<h2>Les provisioners Terraform</h2>\n<p>Les provisioners Terraform permettent d&#39;exécuter des scripts ou des commandes sur une ressource locale ou distante après sa création. Ils sont utiles pour installer des logiciels, configurer des services ou effectuer toute autre tâche de configuration initiale. Cependant, il est important de noter que les provisioners sont considérés comme un dernier recours - il est généralement préférable d&#39;utiliser des images pré-configurées (comme nous l&#39;avons fait avec Packer) ou des outils de gestion de configuration dédiés.</p>\n<h3>Configuration du fichier main.tf</h3>\n<p>Créez un fichier <code>main.tf</code> avec le contenu suivant qui reprend la base du projet part2 et ajoute un provisioner SSH :</p>\n<pre><code class=\"language-coffee\">terraform {\n  required_providers {\n    aws = {\n      source  = &quot;hashicorp/aws&quot;\n      version = &quot;~&gt; 5.0&quot;\n    }\n  }\n}\n\nprovider &quot;aws&quot; {\n  region  = &quot;eu-west-3&quot;\n  profile = &quot;&lt;awsprofile-votreprenom&gt;&quot;\n}\n\ndata &quot;aws_ami&quot; &quot;custom_ubuntu&quot; {\n  most_recent = true\n  owners      = [&quot;self&quot;]\n\n  filter {\n    name   = &quot;name&quot;\n    values = [&quot;ubuntu-22.04-custom-*&quot;]\n  }\n\n  filter {\n    name   = &quot;state&quot;\n    values = [&quot;available&quot;]\n  }\n}\n\nresource &quot;aws_security_group&quot; &quot;web_ssh_access&quot; {\n  name        = &quot;web-ssh-access&quot;\n  description = &quot;Allow SSH and HTTP access&quot;\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = &quot;-1&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  tags = {\n    Name = &quot;Web and SSH Access&quot;\n  }\n}\n\nresource &quot;aws_instance&quot; &quot;web_server&quot; {\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = &quot;t2.micro&quot;\n  vpc_security_group_ids = [aws_security_group.web_ssh_access.id]\n\n  connection {\n    type        = &quot;ssh&quot;\n    user        = &quot;root&quot;\n    private_key = file(&quot;~/.ssh/id_terraform&quot;)\n    host        = self.public_ip\n  }\n\n  provisioner &quot;remote-exec&quot; {\n    inline = [\n      &quot;apt-get update&quot;,\n      &quot;apt-get install -y nginx&quot;,\n      &quot;systemctl start nginx&quot;,\n      &quot;systemctl enable nginx&quot;,\n      &quot;echo &#39;&lt;h1&gt;Hello from Terraform Provisioner!&lt;/h1&gt;&#39; &gt; /var/www/html/index.html&quot;,\n      &quot;echo &#39;Nginx installed and configured successfully&#39;&quot;\n    ]\n  }\n\n  tags = {\n    Name = &quot;Nginx Web Server&quot;\n    Type = &quot;Provisioner-configured&quot;\n  }\n}\n\noutput &quot;instance_id&quot; {\n  value = aws_instance.web_server.id\n}\n\noutput &quot;instance_public_ip&quot; {\n  value = aws_instance.web_server.public_ip\n}\n\noutput &quot;instance_public_dns&quot; {\n  value = aws_instance.web_server.public_dns\n}\n\noutput &quot;web_url&quot; {\n  value = &quot;http://${aws_instance.web_server.public_ip}&quot;\n}\n</code></pre>\n<h2>Nouveaux éléments dans ce projet</h2>\n<h3>Réutilisation du Security Group</h3>\n<p>Nous réutilisons le même Security Group que dans la partie 2, qui autorise déjà le SSH (port 22) et le trafic HTTP (port 80). Cette configuration avait été mise en place en prévision de l&#39;installation d&#39;un serveur web.</p>\n<h3>Bloc connection</h3>\n<p>Le bloc <code>connection</code> définit comment Terraform doit se connecter à l&#39;instance pour exécuter les commandes. Il spécifie le type de connexion (SSH), l&#39;utilisateur (root grâce à notre AMI personnalisée), la clé privée à utiliser (<code>~/.ssh/id_terraform</code> générée dans la partie 2) et l&#39;adresse IP de l&#39;hôte. L&#39;utilisation de <code>self.public_ip</code> permet de référencer dynamiquement l&#39;IP publique de l&#39;instance en cours de création.</p>\n<h3>Provisioner remote-exec</h3>\n<p>Le provisioner <code>remote-exec</code> exécute une série de commandes sur l&#39;instance distante via SSH. Dans notre cas, il met à jour les paquets système, installe Nginx, démarre le service, l&#39;active au démarrage et crée une page d&#39;accueil personnalisée. Les commandes sont exécutées dans l&#39;ordre et si l&#39;une échoue, le provisionnement s&#39;arrête.</p>\n<h3>Output web_url</h3>\n<p>Un nouvel output a été ajouté pour afficher directement l&#39;URL du serveur web, facilitant ainsi l&#39;accès au site après le déploiement.</p>\n<h2>Déploiement et vérification</h2>\n<p>Exécutez les commandes suivantes pour déployer l&#39;infrastructure :</p>\n<pre><code class=\"language-bash\">terraform init\nterraform plan\nterraform apply\n</code></pre>\n<p>Une fois le déploiement terminé, Terraform affichera l&#39;URL du serveur web. Vous pouvez vérifier que Nginx fonctionne en ouvrant cette URL dans votre navigateur. Vous devriez voir le message &quot;Hello from Terraform Provisioner!&quot;.</p>\n<h2>Points importants sur les provisioners</h2>\n<p>Les provisioners présentent certaines limitations qu&#39;il est important de connaître. Ils ne s&#39;exécutent qu&#39;une seule fois lors de la création de la ressource et ne peuvent pas être réexécutés facilement. Si le provisionnement échoue, Terraform marque la ressource comme &quot;tainted&quot; et elle devra être recréée lors du prochain apply. De plus, les provisioners rendent l&#39;infrastructure moins reproductible car leur succès peut dépendre de facteurs externes comme la disponibilité des paquets ou la connectivité réseau.</p>\n<p>Pour ces raisons, il est généralement recommandé d&#39;utiliser des images pré-configurées avec Packer (comme dans la partie 2) ou des outils de gestion de configuration comme Ansible pour des configurations plus complexes. Les provisioners restent utiles pour des configurations simples ou des prototypes rapides.</p>\n"
    },
    {
      "slug": "part4_simple_vpc",
      "title": "TP partie 4 - Introduction aux VPC avec un exemple simple",
      "weight": 7,
      "metadata": {
        "title": "TP partie 4 - Introduction aux VPC avec un exemple simple",
        "weight": 7
      },
      "content": "\nDans cette quatrième partie, nous allons découvrir les concepts fondamentaux des VPC (Virtual Private Cloud) AWS en créant une architecture réseau simple. Nous reprendrons notre serveur web de la partie 3 mais cette fois-ci dans un VPC personnalisé.\n\n## Qu'est-ce qu'un VPC AWS ?\n\nUn VPC (Virtual Private Cloud) est un réseau virtuel dédié à votre compte AWS. C'est un environnement isolé logiquement du reste du cloud AWS où vous pouvez lancer vos ressources dans un réseau virtuel que vous définissez. Contrairement aux parties précédentes où nous utilisions le VPC par défaut d'AWS, nous allons maintenant créer et configurer notre propre VPC.\n\n### Avantages d'un VPC personnalisé\n\nCréer son propre VPC offre plusieurs avantages par rapport à l'utilisation du VPC par défaut. Vous avez un contrôle total sur l'architecture réseau, vous pouvez définir vos propres plages d'adresses IP et configurer les tables de routage selon vos besoins. Cela permet également une meilleure sécurité grâce à l'isolation réseau et la possibilité de créer des environnements multi-tiers.\n\n## Architecture de notre VPC simple\n\n![Architecture VPC Simple](/part4_simple_vpc/vpc-simple-diagram.png)\n\nNotre architecture comprend les éléments essentiels d'un VPC fonctionnel. Nous créons un VPC avec un seul subnet public contenant notre serveur web Nginx, une Internet Gateway pour l'accès Internet, une table de routage pour diriger le trafic et un Security Group pour contrôler l'accès.\n\n## Construction progressive du VPC\n\nNous allons construire notre infrastructure bloc par bloc pour bien comprendre chaque composant et son rôle dans l'architecture réseau.\n\n### Étape 1 : Création du VPC de base\n\nPartez du code de la partie 3 (copiez le dossier part3 vers part4 ou commitez les changements de part3 et créez une nouvelle branche). Ajoutez le bloc VPC au début de votre fichier `main.tf` :\n\n```coffee\n# VPC\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = \"10.0.0.0/16\"\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = \"main-vpc\"\n  }\n}\n```\n\nLe bloc VPC définit notre réseau virtuel privé avec une plage d'adresses IP de 10.0.0.0/16, ce qui nous donne 65 536 adresses IP disponibles. Les paramètres `enable_dns_hostnames` et `enable_dns_support` activent la résolution DNS dans le VPC, permettant aux instances d'avoir des noms DNS.\n\n### Internet Gateway\n\nAjoutez l'Internet Gateway pour permettre l'accès Internet :\n\n```coffee\n# Internet Gateway\nresource \"aws_internet_gateway\" \"main\" {\n  vpc_id = aws_vpc.main.id\n\n  tags = {\n    Name = \"main-igw\"\n  }\n}\n```\n\nL'Internet Gateway (IGW) est le composant qui permet aux ressources du VPC de communiquer avec Internet. Elle est attachée au VPC via `vpc_id` et constitue le point d'entrée et de sortie pour tout le trafic Internet.\n\n### Subnet public\n\nCréez maintenant un subnet public pour héberger notre serveur web :\n\n```coffee\n# Subnet public\nresource \"aws_subnet\" \"public\" {\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = \"10.0.1.0/24\"\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = \"public-subnet\"\n  }\n}\n```\n\nLe subnet utilise une partie de la plage IP du VPC (10.0.1.0/24 = 256 adresses). Le paramètre `map_public_ip_on_launch` fait que les instances lancées dans ce subnet reçoivent automatiquement une adresse IP publique.\n\n### Table de routage\n\nAjoutez la table de routage qui définit comment le trafic est acheminé/routé :\n\n```coffee\n# Route table\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.main.id\n\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.main.id\n  }\n\n  tags = {\n    Name = \"public-route-table\"\n  }\n}\n```\n\nCette table de routage contient une route par défaut (0.0.0.0/0) qui dirige tout le trafic Internet vers l'Internet Gateway. AWS ajoute automatiquement une route locale pour le trafic interne au VPC.\n\n### Association subnet-route table\n\nAssociez le subnet à la table de routage :\n\n```coffee\n# Association subnet avec route table\nresource \"aws_route_table_association\" \"public\" {\n  subnet_id      = aws_subnet.public.id\n  route_table_id = aws_route_table.public.id\n}\n```\n\nCette association indique que le trafic du subnet public doit utiliser la table de routage publique pour déterminer sa destination.\n\n### Modification du Security Group\n\nLe Security Group de la partie 3 doit être légèrement modifié pour fonctionner dans notre VPC personnalisé. Modifiez le bloc Security Group existant en ajoutant simplement le paramètre `vpc_id` :\n\n```coffee\n# Security Group - modification de la partie 3\nresource \"aws_security_group\" \"web_ssh_access\" {\n  name        = \"web-ssh-access\"\n  description = \"Allow SSH and HTTP access\"\n  vpc_id      = aws_vpc.main.id  # <-- Ligne ajoutée pour le VPC personnalisé\n\n  # Le reste reste identique à la partie 3\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"Web and SSH Access\"\n  }\n}\n```\n\nLa seule différence avec la partie 3 est l'ajout de `vpc_id = aws_vpc.main.id` qui indique que ce Security Group appartient à notre VPC personnalisé et non au VPC par défaut d'AWS.\n\n### Modification de l'instance EC2\n\nL'instance EC2 de la partie 3 doit être modifiée pour fonctionner dans notre VPC personnalisé. Modifiez le bloc d'instance existant en ajoutant le paramètre `subnet_id` :\n\n```coffee\n# Instance EC2 - modification de la partie 3\nresource \"aws_instance\" \"web_server\" {\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = \"t2.micro\"\n  subnet_id              = aws_subnet.public.id  # <-- Ligne ajoutée pour placer l'instance dans notre subnet\n  vpc_security_group_ids = [aws_security_group.web_ssh_access.id]\n\n  # Le reste reste identique à la partie 3\n  connection {\n    type        = \"ssh\"\n    user        = \"root\"\n    private_key = file(\"~/.ssh/id_terraform\")\n    host        = self.public_ip\n  }\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"apt-get update\",\n      \"apt-get install -y nginx\",\n      \"systemctl start nginx\",\n      \"systemctl enable nginx\",\n      \"echo '<h1>Hello from VPC!</h1>' > /var/www/html/index.html\",\n      \"echo 'Nginx installed in VPC'\"\n    ]\n  }\n\n  tags = {\n    Name = \"nginx-web-server-vpc\"\n  }\n}\n```\n\nLa principale différence avec la partie 3 est l'ajout de `subnet_id = aws_subnet.public.id` qui place explicitement l'instance dans notre subnet public personnalisé au lieu du subnet par défaut d'AWS.\n\n### Outputs\n\nAjoutez les outputs pour afficher les informations importantes :\n\n```coffee\n# Outputs\noutput \"vpc_id\" {\n  value = aws_vpc.main.id\n}\n\noutput \"subnet_id\" {\n  value = aws_subnet.public.id\n}\n\noutput \"instance_id\" {\n  value = aws_instance.web_server.id\n}\n\noutput \"instance_public_ip\" {\n  value = aws_instance.web_server.public_ip\n}\n\noutput \"web_url\" {\n  value = \"http://${aws_instance.web_server.public_ip}\"\n}\n```\n\nCes outputs vous permettent de voir les IDs des ressources créées et l'URL pour accéder au serveur web.\n\n### Différences avec le VPC par défaut\n\nNotre VPC personnalisé diffère du VPC par défaut sur plusieurs points importants. Nous contrôlons entièrement la configuration réseau, nous définissons nos propres plages IP et nous créons explicitement tous les composants (IGW, subnets, route tables). Cela nous donne une meilleure visibilité et un contrôle total sur l'architecture.\n\n\n## Déploiement et vérification\n\nDéployez l'infrastructure en utilisant une approche plus sécurisée avec le paramètre `-out` :\n\n```bash\nterraform init\nterraform plan -out=tfplan\n```\n\n**Pourquoi utiliser `-out` avec terraform plan ?**\n\nLe paramètre `-out` de `terraform plan` est une bonne pratique importante pour plusieurs raisons :\n\n**Sécurité et cohérence** : Il garantit que les modifications appliquées avec `terraform apply` correspondent exactement à ce qui a été planifié et validé. Sans ce paramètre, il pourrait y avoir des différences entre ce que vous avez vu dans le plan et ce qui est réellement appliqué si des changements ont eu lieu entre les deux commandes.\n\n**Environnements de production** : Dans un environnement de production ou dans un pipeline CI/CD, cette approche est essentielle. Elle permet de valider le plan dans une étape séparée (review, approbation) avant l'application effective des changements.\n\n**Audit et traçabilité** : Le fichier de plan peut être conservé comme trace de ce qui a été appliqué à un moment donné, facilitant les audits et le debugging.\n\n\n\nMaintenant nous pouvons réafficher le plan avec `terraform show tfplan`\n\n- Etudiez le diff : `Plan: 7 to add, 0 to change, 2 to destroy.`\n\nEst-ce que notre mise à jour implique de la haute disponibilité ? Pourquoi ?\n\n> La resource aws_instance i.e. notre serveur doit être remplacé par AWS pour pouvoir être connecté au nouveau subnet. Terraform implique de façon générale une vision immutable des serveurs (ils sont jetés et recréés comme des conteneurs). Pour éviter un coupure de service il va nous falloir une architecture HA permettant le remplacement progressif de plusieurs instances.\n\n\nAppliquez enfin les modification avec la commande :\n\n```sh\nterraform apply tfplan\n```\n\nUne fois le déploiement terminé, vous pouvez accéder au serveur web via l'URL affichée dans les outputs. La page affichera \"Hello from VPC!\" confirmant que le serveur fonctionne dans votre VPC personnalisé.",
      "html": "<p>Dans cette quatrième partie, nous allons découvrir les concepts fondamentaux des VPC (Virtual Private Cloud) AWS en créant une architecture réseau simple. Nous reprendrons notre serveur web de la partie 3 mais cette fois-ci dans un VPC personnalisé.</p>\n<h2>Qu&#39;est-ce qu&#39;un VPC AWS ?</h2>\n<p>Un VPC (Virtual Private Cloud) est un réseau virtuel dédié à votre compte AWS. C&#39;est un environnement isolé logiquement du reste du cloud AWS où vous pouvez lancer vos ressources dans un réseau virtuel que vous définissez. Contrairement aux parties précédentes où nous utilisions le VPC par défaut d&#39;AWS, nous allons maintenant créer et configurer notre propre VPC.</p>\n<h3>Avantages d&#39;un VPC personnalisé</h3>\n<p>Créer son propre VPC offre plusieurs avantages par rapport à l&#39;utilisation du VPC par défaut. Vous avez un contrôle total sur l&#39;architecture réseau, vous pouvez définir vos propres plages d&#39;adresses IP et configurer les tables de routage selon vos besoins. Cela permet également une meilleure sécurité grâce à l&#39;isolation réseau et la possibilité de créer des environnements multi-tiers.</p>\n<h2>Architecture de notre VPC simple</h2>\n<p><img src=\"/part4_simple_vpc/vpc-simple-diagram.png\" alt=\"Architecture VPC Simple\"></p>\n<p>Notre architecture comprend les éléments essentiels d&#39;un VPC fonctionnel. Nous créons un VPC avec un seul subnet public contenant notre serveur web Nginx, une Internet Gateway pour l&#39;accès Internet, une table de routage pour diriger le trafic et un Security Group pour contrôler l&#39;accès.</p>\n<h2>Construction progressive du VPC</h2>\n<p>Nous allons construire notre infrastructure bloc par bloc pour bien comprendre chaque composant et son rôle dans l&#39;architecture réseau.</p>\n<h3>Étape 1 : Création du VPC de base</h3>\n<p>Partez du code de la partie 3 (copiez le dossier part3 vers part4 ou commitez les changements de part3 et créez une nouvelle branche). Ajoutez le bloc VPC au début de votre fichier <code>main.tf</code> :</p>\n<pre><code class=\"language-coffee\"># VPC\nresource &quot;aws_vpc&quot; &quot;main&quot; {\n  cidr_block           = &quot;10.0.0.0/16&quot;\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = &quot;main-vpc&quot;\n  }\n}\n</code></pre>\n<p>Le bloc VPC définit notre réseau virtuel privé avec une plage d&#39;adresses IP de 10.0.0.0/16, ce qui nous donne 65 536 adresses IP disponibles. Les paramètres <code>enable_dns_hostnames</code> et <code>enable_dns_support</code> activent la résolution DNS dans le VPC, permettant aux instances d&#39;avoir des noms DNS.</p>\n<h3>Internet Gateway</h3>\n<p>Ajoutez l&#39;Internet Gateway pour permettre l&#39;accès Internet :</p>\n<pre><code class=\"language-coffee\"># Internet Gateway\nresource &quot;aws_internet_gateway&quot; &quot;main&quot; {\n  vpc_id = aws_vpc.main.id\n\n  tags = {\n    Name = &quot;main-igw&quot;\n  }\n}\n</code></pre>\n<p>L&#39;Internet Gateway (IGW) est le composant qui permet aux ressources du VPC de communiquer avec Internet. Elle est attachée au VPC via <code>vpc_id</code> et constitue le point d&#39;entrée et de sortie pour tout le trafic Internet.</p>\n<h3>Subnet public</h3>\n<p>Créez maintenant un subnet public pour héberger notre serveur web :</p>\n<pre><code class=\"language-coffee\"># Subnet public\nresource &quot;aws_subnet&quot; &quot;public&quot; {\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = &quot;10.0.1.0/24&quot;\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = &quot;public-subnet&quot;\n  }\n}\n</code></pre>\n<p>Le subnet utilise une partie de la plage IP du VPC (10.0.1.0/24 = 256 adresses). Le paramètre <code>map_public_ip_on_launch</code> fait que les instances lancées dans ce subnet reçoivent automatiquement une adresse IP publique.</p>\n<h3>Table de routage</h3>\n<p>Ajoutez la table de routage qui définit comment le trafic est acheminé/routé :</p>\n<pre><code class=\"language-coffee\"># Route table\nresource &quot;aws_route_table&quot; &quot;public&quot; {\n  vpc_id = aws_vpc.main.id\n\n  route {\n    cidr_block = &quot;0.0.0.0/0&quot;\n    gateway_id = aws_internet_gateway.main.id\n  }\n\n  tags = {\n    Name = &quot;public-route-table&quot;\n  }\n}\n</code></pre>\n<p>Cette table de routage contient une route par défaut (0.0.0.0/0) qui dirige tout le trafic Internet vers l&#39;Internet Gateway. AWS ajoute automatiquement une route locale pour le trafic interne au VPC.</p>\n<h3>Association subnet-route table</h3>\n<p>Associez le subnet à la table de routage :</p>\n<pre><code class=\"language-coffee\"># Association subnet avec route table\nresource &quot;aws_route_table_association&quot; &quot;public&quot; {\n  subnet_id      = aws_subnet.public.id\n  route_table_id = aws_route_table.public.id\n}\n</code></pre>\n<p>Cette association indique que le trafic du subnet public doit utiliser la table de routage publique pour déterminer sa destination.</p>\n<h3>Modification du Security Group</h3>\n<p>Le Security Group de la partie 3 doit être légèrement modifié pour fonctionner dans notre VPC personnalisé. Modifiez le bloc Security Group existant en ajoutant simplement le paramètre <code>vpc_id</code> :</p>\n<pre><code class=\"language-coffee\"># Security Group - modification de la partie 3\nresource &quot;aws_security_group&quot; &quot;web_ssh_access&quot; {\n  name        = &quot;web-ssh-access&quot;\n  description = &quot;Allow SSH and HTTP access&quot;\n  vpc_id      = aws_vpc.main.id  # &lt;-- Ligne ajoutée pour le VPC personnalisé\n\n  # Le reste reste identique à la partie 3\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = &quot;-1&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  tags = {\n    Name = &quot;Web and SSH Access&quot;\n  }\n}\n</code></pre>\n<p>La seule différence avec la partie 3 est l&#39;ajout de <code>vpc_id = aws_vpc.main.id</code> qui indique que ce Security Group appartient à notre VPC personnalisé et non au VPC par défaut d&#39;AWS.</p>\n<h3>Modification de l&#39;instance EC2</h3>\n<p>L&#39;instance EC2 de la partie 3 doit être modifiée pour fonctionner dans notre VPC personnalisé. Modifiez le bloc d&#39;instance existant en ajoutant le paramètre <code>subnet_id</code> :</p>\n<pre><code class=\"language-coffee\"># Instance EC2 - modification de la partie 3\nresource &quot;aws_instance&quot; &quot;web_server&quot; {\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = &quot;t2.micro&quot;\n  subnet_id              = aws_subnet.public.id  # &lt;-- Ligne ajoutée pour placer l&#39;instance dans notre subnet\n  vpc_security_group_ids = [aws_security_group.web_ssh_access.id]\n\n  # Le reste reste identique à la partie 3\n  connection {\n    type        = &quot;ssh&quot;\n    user        = &quot;root&quot;\n    private_key = file(&quot;~/.ssh/id_terraform&quot;)\n    host        = self.public_ip\n  }\n\n  provisioner &quot;remote-exec&quot; {\n    inline = [\n      &quot;apt-get update&quot;,\n      &quot;apt-get install -y nginx&quot;,\n      &quot;systemctl start nginx&quot;,\n      &quot;systemctl enable nginx&quot;,\n      &quot;echo &#39;&lt;h1&gt;Hello from VPC!&lt;/h1&gt;&#39; &gt; /var/www/html/index.html&quot;,\n      &quot;echo &#39;Nginx installed in VPC&#39;&quot;\n    ]\n  }\n\n  tags = {\n    Name = &quot;nginx-web-server-vpc&quot;\n  }\n}\n</code></pre>\n<p>La principale différence avec la partie 3 est l&#39;ajout de <code>subnet_id = aws_subnet.public.id</code> qui place explicitement l&#39;instance dans notre subnet public personnalisé au lieu du subnet par défaut d&#39;AWS.</p>\n<h3>Outputs</h3>\n<p>Ajoutez les outputs pour afficher les informations importantes :</p>\n<pre><code class=\"language-coffee\"># Outputs\noutput &quot;vpc_id&quot; {\n  value = aws_vpc.main.id\n}\n\noutput &quot;subnet_id&quot; {\n  value = aws_subnet.public.id\n}\n\noutput &quot;instance_id&quot; {\n  value = aws_instance.web_server.id\n}\n\noutput &quot;instance_public_ip&quot; {\n  value = aws_instance.web_server.public_ip\n}\n\noutput &quot;web_url&quot; {\n  value = &quot;http://${aws_instance.web_server.public_ip}&quot;\n}\n</code></pre>\n<p>Ces outputs vous permettent de voir les IDs des ressources créées et l&#39;URL pour accéder au serveur web.</p>\n<h3>Différences avec le VPC par défaut</h3>\n<p>Notre VPC personnalisé diffère du VPC par défaut sur plusieurs points importants. Nous contrôlons entièrement la configuration réseau, nous définissons nos propres plages IP et nous créons explicitement tous les composants (IGW, subnets, route tables). Cela nous donne une meilleure visibilité et un contrôle total sur l&#39;architecture.</p>\n<h2>Déploiement et vérification</h2>\n<p>Déployez l&#39;infrastructure en utilisant une approche plus sécurisée avec le paramètre <code>-out</code> :</p>\n<pre><code class=\"language-bash\">terraform init\nterraform plan -out=tfplan\n</code></pre>\n<p><strong>Pourquoi utiliser <code>-out</code> avec terraform plan ?</strong></p>\n<p>Le paramètre <code>-out</code> de <code>terraform plan</code> est une bonne pratique importante pour plusieurs raisons :</p>\n<p><strong>Sécurité et cohérence</strong> : Il garantit que les modifications appliquées avec <code>terraform apply</code> correspondent exactement à ce qui a été planifié et validé. Sans ce paramètre, il pourrait y avoir des différences entre ce que vous avez vu dans le plan et ce qui est réellement appliqué si des changements ont eu lieu entre les deux commandes.</p>\n<p><strong>Environnements de production</strong> : Dans un environnement de production ou dans un pipeline CI/CD, cette approche est essentielle. Elle permet de valider le plan dans une étape séparée (review, approbation) avant l&#39;application effective des changements.</p>\n<p><strong>Audit et traçabilité</strong> : Le fichier de plan peut être conservé comme trace de ce qui a été appliqué à un moment donné, facilitant les audits et le debugging.</p>\n<p>Maintenant nous pouvons réafficher le plan avec <code>terraform show tfplan</code></p>\n<ul>\n<li>Etudiez le diff : <code>Plan: 7 to add, 0 to change, 2 to destroy.</code></li>\n</ul>\n<p>Est-ce que notre mise à jour implique de la haute disponibilité ? Pourquoi ?</p>\n<blockquote>\n<p>La resource aws_instance i.e. notre serveur doit être remplacé par AWS pour pouvoir être connecté au nouveau subnet. Terraform implique de façon générale une vision immutable des serveurs (ils sont jetés et recréés comme des conteneurs). Pour éviter un coupure de service il va nous falloir une architecture HA permettant le remplacement progressif de plusieurs instances.</p>\n</blockquote>\n<p>Appliquez enfin les modification avec la commande :</p>\n<pre><code class=\"language-sh\">terraform apply tfplan\n</code></pre>\n<p>Une fois le déploiement terminé, vous pouvez accéder au serveur web via l&#39;URL affichée dans les outputs. La page affichera &quot;Hello from VPC!&quot; confirmant que le serveur fonctionne dans votre VPC personnalisé.</p>\n"
    },
    {
      "slug": "part5_terraform_organization",
      "title": "TP partie 5 - Organisation et structure des projets Terraform",
      "weight": 8,
      "metadata": {
        "title": "TP partie 5 - Organisation et structure des projets Terraform",
        "weight": 8
      },
      "content": "\nDans cette cinquième partie, nous allons apprendre à mieux organiser nos projets Terraform en étudiant les dépendances entre ressources et en restructurant notre code. Cette partie se concentre sur les bonnes pratiques d'organisation plutôt que sur de nouvelles fonctionnalités AWS.\n\n## Étude des dépendances avec terraform graph\n\nCommençons par analyser les dépendances entre nos ressources en utilisant la commande `terraform graph`.\n\n### Génération du graphe de dépendances\n\nPartez du code de la partie 4 (copiez le dossier ou commitez les changements). Dans le dossier part5, exécutez :\n\n```bash\nterraform init\nterraform graph > dependencies.dot\n```\n\nCette commande génère un fichier DOT (format GraphViz) qui décrit les relations entre toutes les ressources Terraform.\n\n### Visualisation du graphe\n\nPour visualiser le graphe, vous pouvez utiliser GraphViz (si installé) ou des outils en ligne :\n\n```bash\n# Si GraphViz est installé\ndot -Tpng dependencies.dot -o dependencies.png\n\n# Ou copiez le contenu de dependencies.dot dans un visualiseur en ligne\n# comme http://magjac.com/graphviz-visual-editor/\n```\n\n### Analyse des dépendances\n\nExaminez le graphe généré. Vous devriez voir les dépendances suivantes :\n\n- **aws_instance.web_server** dépend de **aws_subnet.public** et **aws_security_group.web_ssh_access**\n- **aws_route_table_association.public** dépend de **aws_subnet.public** et **aws_route_table.public**\n- **aws_route_table.public** dépend de **aws_vpc.main** et **aws_internet_gateway.main**\n- **aws_internet_gateway.main** dépend de **aws_vpc.main**\n- **aws_subnet.public** dépend de **aws_vpc.main**\n- **aws_security_group.web_ssh_access** dépend de **aws_vpc.main**\n\nCes dépendances garantissent que Terraform crée les ressources dans le bon ordre et les détruit dans l'ordre inverse.\n\n## Refactorisation : séparation des fichiers\n\nMaintenant, nous allons refactoriser notre code monolithique en plusieurs fichiers thématiques pour améliorer la lisibilité et la maintenance.\n\n\n- Créez un fichier `vpc.tf` et déplacez toutes les ressources réseau à l'intérieur :\n\n```coffee\n# VPC\n...\n# Internet Gateway\n...\n# Subnet public\n...\n# Route table\n...\n# Association subnet avec route table\n...\n# Security Group\nr..\n# Outputs réseau\noutput \"vpc_id\" {\n  value = aws_vpc.main.id\n}\noutput \"subnet_id\" {\n  value = aws_subnet.public.id\n}\n```\n\n### Serveur web\n\nDe même créez un fichier `webserver.tf` contenant l'instance EC2 et les ressources associées :\n\n```coffee\n# Data source pour l'AMI personnalisée\n...\n# Instance EC2\n...\n# Outputs webserver\noutput \"instance_id\" {\n  value = aws_instance.web_server.id\n}\n\noutput \"instance_public_ip\" {\n  value = aws_instance.web_server.public_ip\n}\n\noutput \"web_url\" {\n  value = \"http://${aws_instance.web_server.public_ip}\"\n}\n```\n\n### Main simplifié\n\nLe fichier `main.tf` ne contient plus que les providers :\n\n```coffee\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region  = \"eu-west-3\"\n  profile = \"<awsprofile-votreprenom>\"\n}\n```\n\nIl devient le point d'entrée principal du projet, contenant les informations de configuration Terraform et le provider AWS.\n\n### Vérification de la refactorisation\n\nTestez la nouvelle structure :\n\n```bash\n# Vérifiez que la configuration est identique\nterraform plan\n```\n\n**Résultat attendu** : `No changes. Your infrastructure matches the configuration.`\n\nCette vérification confirme que la refactorisation n'a introduit aucun changement fonctionnel - le code fait exactement la même chose, mais il est mieux organisé.\n\n## Fichiers de structure projet\n\n### versions.tf - Gestion des versions\n\nCréez un fichier `versions.tf` pour centraliser les contraintes de versions :\n\n```coffee\nterraform {\n  required_version = \">= 1.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n```\n\n**Pourquoi un fichier versions.tf ?**\n\nCe fichier sépare les contraintes de versions du code fonctionnel, facilitant la maintenance et les mises à jour. Il garantit que tous les membres de l'équipe utilisent des versions compatibles de Terraform et des providers.\n\n### variables.tf - Paramétrage\n\nCréez un fichier `variables.tf` pour définir les variables d'entrée :\n\n```coffee\nvariable \"aws_region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"eu-west-3\"\n}\n\nvariable \"aws_profile\" {\n  description = \"AWS CLI profile to use\"\n  type        = string\n  default     = \"<awsprofile-votreprenom>\"\n}\n\nvariable \"vpc_cidr\" {\n  description = \"CIDR block for VPC\"\n  type        = string\n  default     = \"10.0.0.0/16\"\n}\n\nvariable \"public_subnet_cidr\" {\n  description = \"CIDR block for public subnet\"\n  type        = string\n  default     = \"10.0.1.0/24\"\n}\n\nvariable \"instance_type\" {\n  description = \"EC2 instance type\"\n  type        = string\n  default     = \"t2.micro\"\n}\n\nvariable \"ssh_key_path\" {\n  description = \"Path to SSH private key\"\n  type        = string\n  default     = \"~/.ssh/id_terraform\"\n}\n```\n\n**Avantages des variables :**\n\n- **Réutilisabilité** : Le même code peut être utilisé pour différents environnements\n- **Flexibilité** : Les valeurs peuvent être modifiées sans changer le code\n- **Documentation** : Les descriptions expliquent l'usage de chaque paramètre\n- **Validation** : Terraform peut valider les types et contraintes\n\nAvec des variables d'entrées, notre projet devient une sorte de module fonctionnel un peu plus réutilisable plutôt qu'un simple description.\n\n### Utilisation des variables\n\nModifiez vos fichiers pour utiliser les variables. Par exemple, dans `main.tf` :\n\n```coffee\nprovider \"aws\" {\n  region  = var.aws_region\n  profile = var.aws_profile\n}\n```\n\nDans `vpc.tf` :\n\n```coffee\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = var.vpc_cidr\n  # ... reste identique\n}\n\nresource \"aws_subnet\" \"public\" {\n  cidr_block = var.public_subnet_cidr\n  # ... reste identique\n}\n```\n\nDans `webserver.tf` :\n\n```coffee\nresource \"aws_instance\" \"web_server\" {\n  instance_type = var.instance_type\n  # ...\n  \n  connection {\n    private_key = file(var.ssh_key_path)\n    # ... reste identique\n  }\n}\n```\n\n## Conclusion : quelques bonnes pratiques d'organisation\n\n### Convention de nommage des fichiers\n\n- **main.tf** : Providers et configuration Terraform principale\n- **variables.tf** : Toutes les variables d'entrée\n- **versions.tf** : Contraintes de versions\n- **outputs.tf** : Outputs globaux (optionnel si distribués dans les autres fichiers)\n- **[service].tf** : Regroupement logique par service (vpc.tf, webserver.tf, database.tf, etc.)\n\n### Séparation des responsabilités\n\nChaque fichier doit avoir une responsabilité claire :\n- Infrastructure réseau séparée des applications\n- Variables centralisées et documentées\n- Outputs regroupés par domaine fonctionnel\n\n### Commentaires et documentation\n\nAjoutez des commentaires pour expliquer les choix techniques non évidents et documentez les variables avec des descriptions claires.\n\n## Déploiement et vérification\n\nTestez votre nouvelle structure organisée :\n\n```bash\nterraform init\nterraform plan -out=tfplan\nterraform apply tfplan\n```\n\nLa nouvelle structure doit produire exactement la même infrastructure que la partie 4, démontrant que l'organisation du code n'affecte pas le fonctionnement.\n\nDans la partie suivante, nous utiliserons cette structure organisée pour créer une architecture VPC multi-AZ plus complexe.",
      "html": "<p>Dans cette cinquième partie, nous allons apprendre à mieux organiser nos projets Terraform en étudiant les dépendances entre ressources et en restructurant notre code. Cette partie se concentre sur les bonnes pratiques d&#39;organisation plutôt que sur de nouvelles fonctionnalités AWS.</p>\n<h2>Étude des dépendances avec terraform graph</h2>\n<p>Commençons par analyser les dépendances entre nos ressources en utilisant la commande <code>terraform graph</code>.</p>\n<h3>Génération du graphe de dépendances</h3>\n<p>Partez du code de la partie 4 (copiez le dossier ou commitez les changements). Dans le dossier part5, exécutez :</p>\n<pre><code class=\"language-bash\">terraform init\nterraform graph &gt; dependencies.dot\n</code></pre>\n<p>Cette commande génère un fichier DOT (format GraphViz) qui décrit les relations entre toutes les ressources Terraform.</p>\n<h3>Visualisation du graphe</h3>\n<p>Pour visualiser le graphe, vous pouvez utiliser GraphViz (si installé) ou des outils en ligne :</p>\n<pre><code class=\"language-bash\"># Si GraphViz est installé\ndot -Tpng dependencies.dot -o dependencies.png\n\n# Ou copiez le contenu de dependencies.dot dans un visualiseur en ligne\n# comme http://magjac.com/graphviz-visual-editor/\n</code></pre>\n<h3>Analyse des dépendances</h3>\n<p>Examinez le graphe généré. Vous devriez voir les dépendances suivantes :</p>\n<ul>\n<li><strong>aws_instance.web_server</strong> dépend de <strong>aws_subnet.public</strong> et <strong>aws_security_group.web_ssh_access</strong></li>\n<li><strong>aws_route_table_association.public</strong> dépend de <strong>aws_subnet.public</strong> et <strong>aws_route_table.public</strong></li>\n<li><strong>aws_route_table.public</strong> dépend de <strong>aws_vpc.main</strong> et <strong>aws_internet_gateway.main</strong></li>\n<li><strong>aws_internet_gateway.main</strong> dépend de <strong>aws_vpc.main</strong></li>\n<li><strong>aws_subnet.public</strong> dépend de <strong>aws_vpc.main</strong></li>\n<li><strong>aws_security_group.web_ssh_access</strong> dépend de <strong>aws_vpc.main</strong></li>\n</ul>\n<p>Ces dépendances garantissent que Terraform crée les ressources dans le bon ordre et les détruit dans l&#39;ordre inverse.</p>\n<h2>Refactorisation : séparation des fichiers</h2>\n<p>Maintenant, nous allons refactoriser notre code monolithique en plusieurs fichiers thématiques pour améliorer la lisibilité et la maintenance.</p>\n<ul>\n<li>Créez un fichier <code>vpc.tf</code> et déplacez toutes les ressources réseau à l&#39;intérieur :</li>\n</ul>\n<pre><code class=\"language-coffee\"># VPC\n...\n# Internet Gateway\n...\n# Subnet public\n...\n# Route table\n...\n# Association subnet avec route table\n...\n# Security Group\nr..\n# Outputs réseau\noutput &quot;vpc_id&quot; {\n  value = aws_vpc.main.id\n}\noutput &quot;subnet_id&quot; {\n  value = aws_subnet.public.id\n}\n</code></pre>\n<h3>Serveur web</h3>\n<p>De même créez un fichier <code>webserver.tf</code> contenant l&#39;instance EC2 et les ressources associées :</p>\n<pre><code class=\"language-coffee\"># Data source pour l&#39;AMI personnalisée\n...\n# Instance EC2\n...\n# Outputs webserver\noutput &quot;instance_id&quot; {\n  value = aws_instance.web_server.id\n}\n\noutput &quot;instance_public_ip&quot; {\n  value = aws_instance.web_server.public_ip\n}\n\noutput &quot;web_url&quot; {\n  value = &quot;http://${aws_instance.web_server.public_ip}&quot;\n}\n</code></pre>\n<h3>Main simplifié</h3>\n<p>Le fichier <code>main.tf</code> ne contient plus que les providers :</p>\n<pre><code class=\"language-coffee\">terraform {\n  required_providers {\n    aws = {\n      source  = &quot;hashicorp/aws&quot;\n      version = &quot;~&gt; 5.0&quot;\n    }\n  }\n}\n\nprovider &quot;aws&quot; {\n  region  = &quot;eu-west-3&quot;\n  profile = &quot;&lt;awsprofile-votreprenom&gt;&quot;\n}\n</code></pre>\n<p>Il devient le point d&#39;entrée principal du projet, contenant les informations de configuration Terraform et le provider AWS.</p>\n<h3>Vérification de la refactorisation</h3>\n<p>Testez la nouvelle structure :</p>\n<pre><code class=\"language-bash\"># Vérifiez que la configuration est identique\nterraform plan\n</code></pre>\n<p><strong>Résultat attendu</strong> : <code>No changes. Your infrastructure matches the configuration.</code></p>\n<p>Cette vérification confirme que la refactorisation n&#39;a introduit aucun changement fonctionnel - le code fait exactement la même chose, mais il est mieux organisé.</p>\n<h2>Fichiers de structure projet</h2>\n<h3>versions.tf - Gestion des versions</h3>\n<p>Créez un fichier <code>versions.tf</code> pour centraliser les contraintes de versions :</p>\n<pre><code class=\"language-coffee\">terraform {\n  required_version = &quot;&gt;= 1.0&quot;\n\n  required_providers {\n    aws = {\n      source  = &quot;hashicorp/aws&quot;\n      version = &quot;~&gt; 5.0&quot;\n    }\n  }\n}\n</code></pre>\n<p><strong>Pourquoi un fichier versions.tf ?</strong></p>\n<p>Ce fichier sépare les contraintes de versions du code fonctionnel, facilitant la maintenance et les mises à jour. Il garantit que tous les membres de l&#39;équipe utilisent des versions compatibles de Terraform et des providers.</p>\n<h3>variables.tf - Paramétrage</h3>\n<p>Créez un fichier <code>variables.tf</code> pour définir les variables d&#39;entrée :</p>\n<pre><code class=\"language-coffee\">variable &quot;aws_region&quot; {\n  description = &quot;AWS region&quot;\n  type        = string\n  default     = &quot;eu-west-3&quot;\n}\n\nvariable &quot;aws_profile&quot; {\n  description = &quot;AWS CLI profile to use&quot;\n  type        = string\n  default     = &quot;&lt;awsprofile-votreprenom&gt;&quot;\n}\n\nvariable &quot;vpc_cidr&quot; {\n  description = &quot;CIDR block for VPC&quot;\n  type        = string\n  default     = &quot;10.0.0.0/16&quot;\n}\n\nvariable &quot;public_subnet_cidr&quot; {\n  description = &quot;CIDR block for public subnet&quot;\n  type        = string\n  default     = &quot;10.0.1.0/24&quot;\n}\n\nvariable &quot;instance_type&quot; {\n  description = &quot;EC2 instance type&quot;\n  type        = string\n  default     = &quot;t2.micro&quot;\n}\n\nvariable &quot;ssh_key_path&quot; {\n  description = &quot;Path to SSH private key&quot;\n  type        = string\n  default     = &quot;~/.ssh/id_terraform&quot;\n}\n</code></pre>\n<p><strong>Avantages des variables :</strong></p>\n<ul>\n<li><strong>Réutilisabilité</strong> : Le même code peut être utilisé pour différents environnements</li>\n<li><strong>Flexibilité</strong> : Les valeurs peuvent être modifiées sans changer le code</li>\n<li><strong>Documentation</strong> : Les descriptions expliquent l&#39;usage de chaque paramètre</li>\n<li><strong>Validation</strong> : Terraform peut valider les types et contraintes</li>\n</ul>\n<p>Avec des variables d&#39;entrées, notre projet devient une sorte de module fonctionnel un peu plus réutilisable plutôt qu&#39;un simple description.</p>\n<h3>Utilisation des variables</h3>\n<p>Modifiez vos fichiers pour utiliser les variables. Par exemple, dans <code>main.tf</code> :</p>\n<pre><code class=\"language-coffee\">provider &quot;aws&quot; {\n  region  = var.aws_region\n  profile = var.aws_profile\n}\n</code></pre>\n<p>Dans <code>vpc.tf</code> :</p>\n<pre><code class=\"language-coffee\">resource &quot;aws_vpc&quot; &quot;main&quot; {\n  cidr_block           = var.vpc_cidr\n  # ... reste identique\n}\n\nresource &quot;aws_subnet&quot; &quot;public&quot; {\n  cidr_block = var.public_subnet_cidr\n  # ... reste identique\n}\n</code></pre>\n<p>Dans <code>webserver.tf</code> :</p>\n<pre><code class=\"language-coffee\">resource &quot;aws_instance&quot; &quot;web_server&quot; {\n  instance_type = var.instance_type\n  # ...\n  \n  connection {\n    private_key = file(var.ssh_key_path)\n    # ... reste identique\n  }\n}\n</code></pre>\n<h2>Conclusion : quelques bonnes pratiques d&#39;organisation</h2>\n<h3>Convention de nommage des fichiers</h3>\n<ul>\n<li><strong>main.tf</strong> : Providers et configuration Terraform principale</li>\n<li><strong>variables.tf</strong> : Toutes les variables d&#39;entrée</li>\n<li><strong>versions.tf</strong> : Contraintes de versions</li>\n<li><strong>outputs.tf</strong> : Outputs globaux (optionnel si distribués dans les autres fichiers)</li>\n<li><strong>[service].tf</strong> : Regroupement logique par service (vpc.tf, webserver.tf, database.tf, etc.)</li>\n</ul>\n<h3>Séparation des responsabilités</h3>\n<p>Chaque fichier doit avoir une responsabilité claire :</p>\n<ul>\n<li>Infrastructure réseau séparée des applications</li>\n<li>Variables centralisées et documentées</li>\n<li>Outputs regroupés par domaine fonctionnel</li>\n</ul>\n<h3>Commentaires et documentation</h3>\n<p>Ajoutez des commentaires pour expliquer les choix techniques non évidents et documentez les variables avec des descriptions claires.</p>\n<h2>Déploiement et vérification</h2>\n<p>Testez votre nouvelle structure organisée :</p>\n<pre><code class=\"language-bash\">terraform init\nterraform plan -out=tfplan\nterraform apply tfplan\n</code></pre>\n<p>La nouvelle structure doit produire exactement la même infrastructure que la partie 4, démontrant que l&#39;organisation du code n&#39;affecte pas le fonctionnement.</p>\n<p>Dans la partie suivante, nous utiliserons cette structure organisée pour créer une architecture VPC multi-AZ plus complexe.</p>\n"
    },
    {
      "slug": "partmaybe_vpc_networking",
      "title": "TP partie 5 - Architecture VPC avancée multi-AZ",
      "weight": 8,
      "metadata": {
        "title": "TP partie 5 - Architecture VPC avancée multi-AZ",
        "weight": 8
      },
      "content": "\nDans cette cinquième partie, nous allons construire une architecture VPC avancée multi-zones de disponibilité (multi-AZ) avec subnets publics et privés. Cette architecture reprend les concepts de la partie 4 et les étend pour créer une infrastructure hautement disponible et sécurisée.\n\n## Qu'est-ce qu'un VPC AWS ?\n\nUn VPC (Virtual Private Cloud) est un réseau virtuel dédié à votre application cloud. Il s'agit d'un environnement isolé logiquement du reste du cloud AWS où vous pouvez lancer vos ressources AWS dans un réseau virtuel que vous définissez. Vous avez un contrôle très large sur cet environnement réseau virtuel, y compris la sélection de votre propre plage d'adresses IP, la création de sous-réseaux et la configuration des tables de routage et des passerelles réseau.\n\n### Concepts fondamentaux du VPC\n\n#### CIDR (Classless Inter-Domain Routing)\n\nLe CIDR est une méthode pour allouer des adresses IP et router des paquets IP. Dans AWS VPC, vous définissez votre espace d'adressage IP en utilisant la notation CIDR. Par exemple, `10.0.0.0/16` signifie que les 16 premiers bits sont fixes (10.0) et que vous avez 16 bits pour vos hôtes, ce qui vous donne 65 536 adresses IP possibles (de 10.0.0.0 à 10.0.255.255).\n\n#### Subnets (Sous-réseaux)\n\nLes subnets sont des segments de votre VPC où vous placez vos ressources. Chaque subnet est associé à une zone de disponibilité (AZ) spécifique. On distingue deux types de subnets :\n\n- **Subnets publics** : Ont une route vers Internet via une Internet Gateway. Les ressources dans ces subnets peuvent avoir des adresses IP publiques.\n- **Subnets privés** : N'ont pas de route directe vers Internet. Les ressources dans ces subnets ne sont accessibles que depuis l'intérieur du VPC.\n\n#### Internet Gateway (IGW)\n\nUne Internet Gateway est un composant VPC qui permet la communication entre les instances de votre VPC et Internet. C'est un service hautement disponible et redondant qui sert de point de sortie pour le trafic Internet de votre VPC.\n\n#### NAT Gateway\n\nUn NAT (Network Address Translation) Gateway permet aux instances dans un subnet privé d'initier des connexions sortantes vers Internet (pour les mises à jour, par exemple) tout en empêchant Internet d'initier des connexions entrantes vers ces instances. C'est essentiel pour la sécurité des ressources privées.\n\n#### Route Tables\n\nLes tables de routage contiennent des règles (routes) qui déterminent où le trafic réseau est dirigé. Chaque subnet doit être associé à une table de routage. La table de routage spécifie les chemins possibles pour le trafic sortant du subnet.\n\n#### Security Groups\n\nLes Security Groups agissent comme des pare-feu virtuels pour contrôler le trafic entrant et sortant au niveau de l'instance. Ils fonctionnent au niveau de l'instance et sont stateful (ils se souviennent des connexions établies).\n\n## Architecture du VPC\n\n![Architecture VPC](/partmaybe_vpc_networking/vpc-diagram.png)\n\nNotre architecture VPC comprend les éléments suivants :\n\n### Structure réseau\n\n- **VPC principal** : Plage CIDR 10.0.0.0/16 (65 536 adresses IP)\n- **2 zones de disponibilité** : Pour la haute disponibilité\n- **4 subnets** :\n  - 2 subnets publics (10.0.1.0/24 et 10.0.2.0/24)\n  - 2 subnets privés (10.0.11.0/24 et 10.0.12.0/24)\n\n### Composants de connectivité\n\n- **1 Internet Gateway** : Pour l'accès Internet des subnets publics\n- **2 NAT Gateways** : Un dans chaque subnet public pour la redondance\n- **2 Elastic IPs** : Pour les NAT Gateways\n\n### Tables de routage\n\n- **1 table de routage publique** : Route par défaut vers l'Internet Gateway\n- **2 tables de routage privées** : Une par AZ, route par défaut vers le NAT Gateway local\n\n### Sécurité\n\n- **Security Group Web** : Autorise SSH (22) et HTTP (80) depuis Internet\n- **Security Group Privé** : Autorise SSH depuis le Security Group Web et MySQL (3306) depuis le VPC\n\n## Avantages de cette architecture\n\n### Isolation et sécurité\n\nLe VPC crée un environnement réseau isolé où vous contrôlez totalement l'accès. Les ressources privées sont protégées derrière les NAT Gateways et ne sont pas directement accessibles depuis Internet.\n\n### Haute disponibilité\n\nEn déployant des ressources dans plusieurs zones de disponibilité, l'architecture résiste aux pannes d'une zone complète. Si une AZ devient indisponible, l'application continue de fonctionner dans l'autre AZ.\n\n### Évolutivité\n\nLa structure permet d'ajouter facilement de nouvelles ressources. Les plages CIDR choisies laissent de la place pour l'expansion future avec de nouveaux subnets si nécessaire.\n\n### Séparation des responsabilités\n\nLa séparation entre subnets publics et privés permet de suivre les bonnes pratiques de sécurité en plaçant uniquement les ressources nécessitant un accès Internet direct dans les subnets publics.\n\n## Construction progressive de l'architecture\n\nContrairement à la partie 4 où nous avons créé un VPC simple, nous allons maintenant construire une architecture complexe étape par étape pour bien comprendre chaque composant.\n\n### Étape 1 : Base du projet\n\nCommencez par reprendre la base de la partie 4 avec les providers et data sources :\n\n```hcl\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region  = \"eu-west-3\"\n  profile = \"<awsprofile-votreprenom>\"\n}\n\ndata \"aws_ami\" \"custom_ubuntu\" {\n  most_recent = true\n  owners      = [\"self\"]\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu-22.04-custom-*\"]\n  }\n\n  filter {\n    name   = \"state\"\n    values = [\"available\"]\n  }\n}\n\ndata \"aws_availability_zones\" \"available\" {\n  state = \"available\"\n}\n```\n\n### Étape 2 : VPC et Internet Gateway\n\nCréez le VPC principal et son Internet Gateway :\n\n```hcl\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = \"10.0.0.0/16\"\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = \"main-vpc\"\n  }\n}\n\nresource \"aws_internet_gateway\" \"main\" {\n  vpc_id = aws_vpc.main.id\n\n  tags = {\n    Name = \"main-igw\"\n  }\n}\n```\n\n### Étape 3 : Elastic IPs pour les NAT Gateways\n\nCréez les IPs publiques fixes pour les NAT Gateways :\n\n```hcl\nresource \"aws_eip\" \"nat\" {\n  count  = 2\n  domain = \"vpc\"\n\n  tags = {\n    Name = \"nat-eip-${count.index + 1}\"\n  }\n}\n```\n\n### Étape 4 : Subnets publics et privés\n\nCréez les subnets dans différentes zones de disponibilité :\n\n```hcl\nresource \"aws_subnet\" \"public\" {\n  count                   = 2\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = \"10.0.${count.index + 1}.0/24\"\n  availability_zone       = data.aws_availability_zones.available.names[count.index]\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = \"public-subnet-${count.index + 1}\"\n    Type = \"public\"\n  }\n}\n\nresource \"aws_subnet\" \"private\" {\n  count             = 2\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = \"10.0.${count.index + 11}.0/24\"\n  availability_zone = data.aws_availability_zones.available.names[count.index]\n\n  tags = {\n    Name = \"private-subnet-${count.index + 1}\"\n    Type = \"private\"\n  }\n}\n```\n\n### Étape 5 : NAT Gateways\n\nCréez les NAT Gateways pour l'accès Internet sortant des subnets privés :\n\n```hcl\nresource \"aws_nat_gateway\" \"main\" {\n  count         = 2\n  allocation_id = aws_eip.nat[count.index].id\n  subnet_id     = aws_subnet.public[count.index].id\n\n  tags = {\n    Name = \"nat-gateway-${count.index + 1}\"\n  }\n\n  depends_on = [aws_internet_gateway.main]\n}\n```\n\n### Étape 6 : Tables de routage\n\nCréez les tables de routage pour diriger le trafic correctement :\n\n```hcl\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.main.id\n\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.main.id\n  }\n\n  tags = {\n    Name = \"public-route-table\"\n  }\n}\n\nresource \"aws_route_table\" \"private\" {\n  count  = 2\n  vpc_id = aws_vpc.main.id\n\n  route {\n    cidr_block     = \"0.0.0.0/0\"\n    nat_gateway_id = aws_nat_gateway.main[count.index].id\n  }\n\n  tags = {\n    Name = \"private-route-table-${count.index + 1}\"\n  }\n}\n```\n\n### Étape 7 : Associations des tables de routage\n\nAssociez les subnets aux bonnes tables de routage :\n\n```hcl\nresource \"aws_route_table_association\" \"public\" {\n  count          = 2\n  subnet_id      = aws_subnet.public[count.index].id\n  route_table_id = aws_route_table.public.id\n}\n\nresource \"aws_route_table_association\" \"private\" {\n  count          = 2\n  subnet_id      = aws_subnet.private[count.index].id\n  route_table_id = aws_route_table.private[count.index].id\n}\n```\n\n### Étape 8 : Security Groups\n\nCréez les groupes de sécurité pour les différents tiers :\n\n```hcl\nresource \"aws_security_group\" \"web\" {\n  name        = \"web-security-group\"\n  description = \"Security group for web servers\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"web-security-group\"\n  }\n}\n\nresource \"aws_security_group\" \"private\" {\n  name        = \"private-security-group\"\n  description = \"Security group for private instances\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    from_port       = 22\n    to_port         = 22\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.web.id]\n  }\n\n  ingress {\n    from_port   = 3306\n    to_port     = 3306\n    protocol    = \"tcp\"\n    cidr_blocks = [aws_vpc.main.cidr_block]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"private-security-group\"\n  }\n}\n```\n\n### Étape 9 : Instances EC2\n\nCréez les instances dans les différents subnets :\n\n```hcl\nresource \"aws_instance\" \"web\" {\n  count                  = 2\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = \"t2.micro\"\n  subnet_id              = aws_subnet.public[count.index].id\n  vpc_security_group_ids = [aws_security_group.web.id]\n\n  user_data = <<-EOF\n    #!/bin/bash\n    apt-get update\n    apt-get install -y nginx\n    echo \"<h1>Web Server ${count.index + 1} - AZ: ${data.aws_availability_zones.available.names[count.index]}</h1>\" > /var/www/html/index.html\n    systemctl start nginx\n    systemctl enable nginx\n  EOF\n\n  tags = {\n    Name = \"web-server-${count.index + 1}\"\n    Type = \"public\"\n  }\n}\n\nresource \"aws_instance\" \"private\" {\n  count                  = 2\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = \"t2.micro\"\n  subnet_id              = aws_subnet.private[count.index].id\n  vpc_security_group_ids = [aws_security_group.private.id]\n\n  tags = {\n    Name = \"private-instance-${count.index + 1}\"\n    Type = \"private\"\n  }\n}\n```\n\n### Étape 10 : Outputs\n\nAjoutez les outputs pour voir les résultats :\n\n```hcl\noutput \"vpc_id\" {\n  value = aws_vpc.main.id\n}\n\noutput \"public_subnet_ids\" {\n  value = aws_subnet.public[*].id\n}\n\noutput \"private_subnet_ids\" {\n  value = aws_subnet.private[*].id\n}\n\noutput \"web_server_public_ips\" {\n  value = aws_instance.web[*].public_ip\n}\n\noutput \"web_urls\" {\n  value = [for ip in aws_instance.web[*].public_ip : \"http://${ip}\"]\n}\n```\n\n## Déploiement\n\nPour déployer cette infrastructure :\n\n```bash\nterraform init\nterraform plan\nterraform apply\n```\n\nUne fois le déploiement terminé, vous pouvez accéder aux serveurs web via leurs IPs publiques affichées dans les outputs. Les instances privées ne sont accessibles qu'en passant par les instances publiques (architecture bastion).\n\n## Coûts et optimisations\n\nCette architecture inclut des composants payants :\n- **NAT Gateways** : Environ 0.045$/heure chacune\n- **Elastic IPs** : Gratuite si associée, payante si non utilisée\n- **Transfert de données** : Facturé pour le trafic sortant\n\nPour un environnement de développement, vous pourriez :\n- Utiliser une seule NAT Gateway au lieu de deux\n- Remplacer les NAT Gateways par des NAT Instances (moins chères mais nécessitent plus de maintenance)\n- Arrêter les ressources quand elles ne sont pas utilisées\n\n## Conclusion\n\nCette architecture VPC fournit une base solide pour déployer des applications sécurisées et hautement disponibles sur AWS. Elle suit les bonnes pratiques AWS en matière de sécurité, de disponibilité et d'évolutivité. Dans les prochaines parties, nous construirons sur cette base pour déployer des applications plus complexes.",
      "html": "<p>Dans cette cinquième partie, nous allons construire une architecture VPC avancée multi-zones de disponibilité (multi-AZ) avec subnets publics et privés. Cette architecture reprend les concepts de la partie 4 et les étend pour créer une infrastructure hautement disponible et sécurisée.</p>\n<h2>Qu&#39;est-ce qu&#39;un VPC AWS ?</h2>\n<p>Un VPC (Virtual Private Cloud) est un réseau virtuel dédié à votre application cloud. Il s&#39;agit d&#39;un environnement isolé logiquement du reste du cloud AWS où vous pouvez lancer vos ressources AWS dans un réseau virtuel que vous définissez. Vous avez un contrôle très large sur cet environnement réseau virtuel, y compris la sélection de votre propre plage d&#39;adresses IP, la création de sous-réseaux et la configuration des tables de routage et des passerelles réseau.</p>\n<h3>Concepts fondamentaux du VPC</h3>\n<h4>CIDR (Classless Inter-Domain Routing)</h4>\n<p>Le CIDR est une méthode pour allouer des adresses IP et router des paquets IP. Dans AWS VPC, vous définissez votre espace d&#39;adressage IP en utilisant la notation CIDR. Par exemple, <code>10.0.0.0/16</code> signifie que les 16 premiers bits sont fixes (10.0) et que vous avez 16 bits pour vos hôtes, ce qui vous donne 65 536 adresses IP possibles (de 10.0.0.0 à 10.0.255.255).</p>\n<h4>Subnets (Sous-réseaux)</h4>\n<p>Les subnets sont des segments de votre VPC où vous placez vos ressources. Chaque subnet est associé à une zone de disponibilité (AZ) spécifique. On distingue deux types de subnets :</p>\n<ul>\n<li><strong>Subnets publics</strong> : Ont une route vers Internet via une Internet Gateway. Les ressources dans ces subnets peuvent avoir des adresses IP publiques.</li>\n<li><strong>Subnets privés</strong> : N&#39;ont pas de route directe vers Internet. Les ressources dans ces subnets ne sont accessibles que depuis l&#39;intérieur du VPC.</li>\n</ul>\n<h4>Internet Gateway (IGW)</h4>\n<p>Une Internet Gateway est un composant VPC qui permet la communication entre les instances de votre VPC et Internet. C&#39;est un service hautement disponible et redondant qui sert de point de sortie pour le trafic Internet de votre VPC.</p>\n<h4>NAT Gateway</h4>\n<p>Un NAT (Network Address Translation) Gateway permet aux instances dans un subnet privé d&#39;initier des connexions sortantes vers Internet (pour les mises à jour, par exemple) tout en empêchant Internet d&#39;initier des connexions entrantes vers ces instances. C&#39;est essentiel pour la sécurité des ressources privées.</p>\n<h4>Route Tables</h4>\n<p>Les tables de routage contiennent des règles (routes) qui déterminent où le trafic réseau est dirigé. Chaque subnet doit être associé à une table de routage. La table de routage spécifie les chemins possibles pour le trafic sortant du subnet.</p>\n<h4>Security Groups</h4>\n<p>Les Security Groups agissent comme des pare-feu virtuels pour contrôler le trafic entrant et sortant au niveau de l&#39;instance. Ils fonctionnent au niveau de l&#39;instance et sont stateful (ils se souviennent des connexions établies).</p>\n<h2>Architecture du VPC</h2>\n<p><img src=\"/partmaybe_vpc_networking/vpc-diagram.png\" alt=\"Architecture VPC\"></p>\n<p>Notre architecture VPC comprend les éléments suivants :</p>\n<h3>Structure réseau</h3>\n<ul>\n<li><strong>VPC principal</strong> : Plage CIDR 10.0.0.0/16 (65 536 adresses IP)</li>\n<li><strong>2 zones de disponibilité</strong> : Pour la haute disponibilité</li>\n<li><strong>4 subnets</strong> :<ul>\n<li>2 subnets publics (10.0.1.0/24 et 10.0.2.0/24)</li>\n<li>2 subnets privés (10.0.11.0/24 et 10.0.12.0/24)</li>\n</ul>\n</li>\n</ul>\n<h3>Composants de connectivité</h3>\n<ul>\n<li><strong>1 Internet Gateway</strong> : Pour l&#39;accès Internet des subnets publics</li>\n<li><strong>2 NAT Gateways</strong> : Un dans chaque subnet public pour la redondance</li>\n<li><strong>2 Elastic IPs</strong> : Pour les NAT Gateways</li>\n</ul>\n<h3>Tables de routage</h3>\n<ul>\n<li><strong>1 table de routage publique</strong> : Route par défaut vers l&#39;Internet Gateway</li>\n<li><strong>2 tables de routage privées</strong> : Une par AZ, route par défaut vers le NAT Gateway local</li>\n</ul>\n<h3>Sécurité</h3>\n<ul>\n<li><strong>Security Group Web</strong> : Autorise SSH (22) et HTTP (80) depuis Internet</li>\n<li><strong>Security Group Privé</strong> : Autorise SSH depuis le Security Group Web et MySQL (3306) depuis le VPC</li>\n</ul>\n<h2>Avantages de cette architecture</h2>\n<h3>Isolation et sécurité</h3>\n<p>Le VPC crée un environnement réseau isolé où vous contrôlez totalement l&#39;accès. Les ressources privées sont protégées derrière les NAT Gateways et ne sont pas directement accessibles depuis Internet.</p>\n<h3>Haute disponibilité</h3>\n<p>En déployant des ressources dans plusieurs zones de disponibilité, l&#39;architecture résiste aux pannes d&#39;une zone complète. Si une AZ devient indisponible, l&#39;application continue de fonctionner dans l&#39;autre AZ.</p>\n<h3>Évolutivité</h3>\n<p>La structure permet d&#39;ajouter facilement de nouvelles ressources. Les plages CIDR choisies laissent de la place pour l&#39;expansion future avec de nouveaux subnets si nécessaire.</p>\n<h3>Séparation des responsabilités</h3>\n<p>La séparation entre subnets publics et privés permet de suivre les bonnes pratiques de sécurité en plaçant uniquement les ressources nécessitant un accès Internet direct dans les subnets publics.</p>\n<h2>Construction progressive de l&#39;architecture</h2>\n<p>Contrairement à la partie 4 où nous avons créé un VPC simple, nous allons maintenant construire une architecture complexe étape par étape pour bien comprendre chaque composant.</p>\n<h3>Étape 1 : Base du projet</h3>\n<p>Commencez par reprendre la base de la partie 4 avec les providers et data sources :</p>\n<pre><code class=\"language-hcl\">terraform {\n  required_providers {\n    aws = {\n      source  = &quot;hashicorp/aws&quot;\n      version = &quot;~&gt; 5.0&quot;\n    }\n  }\n}\n\nprovider &quot;aws&quot; {\n  region  = &quot;eu-west-3&quot;\n  profile = &quot;&lt;awsprofile-votreprenom&gt;&quot;\n}\n\ndata &quot;aws_ami&quot; &quot;custom_ubuntu&quot; {\n  most_recent = true\n  owners      = [&quot;self&quot;]\n\n  filter {\n    name   = &quot;name&quot;\n    values = [&quot;ubuntu-22.04-custom-*&quot;]\n  }\n\n  filter {\n    name   = &quot;state&quot;\n    values = [&quot;available&quot;]\n  }\n}\n\ndata &quot;aws_availability_zones&quot; &quot;available&quot; {\n  state = &quot;available&quot;\n}\n</code></pre>\n<h3>Étape 2 : VPC et Internet Gateway</h3>\n<p>Créez le VPC principal et son Internet Gateway :</p>\n<pre><code class=\"language-hcl\">resource &quot;aws_vpc&quot; &quot;main&quot; {\n  cidr_block           = &quot;10.0.0.0/16&quot;\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = &quot;main-vpc&quot;\n  }\n}\n\nresource &quot;aws_internet_gateway&quot; &quot;main&quot; {\n  vpc_id = aws_vpc.main.id\n\n  tags = {\n    Name = &quot;main-igw&quot;\n  }\n}\n</code></pre>\n<h3>Étape 3 : Elastic IPs pour les NAT Gateways</h3>\n<p>Créez les IPs publiques fixes pour les NAT Gateways :</p>\n<pre><code class=\"language-hcl\">resource &quot;aws_eip&quot; &quot;nat&quot; {\n  count  = 2\n  domain = &quot;vpc&quot;\n\n  tags = {\n    Name = &quot;nat-eip-${count.index + 1}&quot;\n  }\n}\n</code></pre>\n<h3>Étape 4 : Subnets publics et privés</h3>\n<p>Créez les subnets dans différentes zones de disponibilité :</p>\n<pre><code class=\"language-hcl\">resource &quot;aws_subnet&quot; &quot;public&quot; {\n  count                   = 2\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = &quot;10.0.${count.index + 1}.0/24&quot;\n  availability_zone       = data.aws_availability_zones.available.names[count.index]\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = &quot;public-subnet-${count.index + 1}&quot;\n    Type = &quot;public&quot;\n  }\n}\n\nresource &quot;aws_subnet&quot; &quot;private&quot; {\n  count             = 2\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = &quot;10.0.${count.index + 11}.0/24&quot;\n  availability_zone = data.aws_availability_zones.available.names[count.index]\n\n  tags = {\n    Name = &quot;private-subnet-${count.index + 1}&quot;\n    Type = &quot;private&quot;\n  }\n}\n</code></pre>\n<h3>Étape 5 : NAT Gateways</h3>\n<p>Créez les NAT Gateways pour l&#39;accès Internet sortant des subnets privés :</p>\n<pre><code class=\"language-hcl\">resource &quot;aws_nat_gateway&quot; &quot;main&quot; {\n  count         = 2\n  allocation_id = aws_eip.nat[count.index].id\n  subnet_id     = aws_subnet.public[count.index].id\n\n  tags = {\n    Name = &quot;nat-gateway-${count.index + 1}&quot;\n  }\n\n  depends_on = [aws_internet_gateway.main]\n}\n</code></pre>\n<h3>Étape 6 : Tables de routage</h3>\n<p>Créez les tables de routage pour diriger le trafic correctement :</p>\n<pre><code class=\"language-hcl\">resource &quot;aws_route_table&quot; &quot;public&quot; {\n  vpc_id = aws_vpc.main.id\n\n  route {\n    cidr_block = &quot;0.0.0.0/0&quot;\n    gateway_id = aws_internet_gateway.main.id\n  }\n\n  tags = {\n    Name = &quot;public-route-table&quot;\n  }\n}\n\nresource &quot;aws_route_table&quot; &quot;private&quot; {\n  count  = 2\n  vpc_id = aws_vpc.main.id\n\n  route {\n    cidr_block     = &quot;0.0.0.0/0&quot;\n    nat_gateway_id = aws_nat_gateway.main[count.index].id\n  }\n\n  tags = {\n    Name = &quot;private-route-table-${count.index + 1}&quot;\n  }\n}\n</code></pre>\n<h3>Étape 7 : Associations des tables de routage</h3>\n<p>Associez les subnets aux bonnes tables de routage :</p>\n<pre><code class=\"language-hcl\">resource &quot;aws_route_table_association&quot; &quot;public&quot; {\n  count          = 2\n  subnet_id      = aws_subnet.public[count.index].id\n  route_table_id = aws_route_table.public.id\n}\n\nresource &quot;aws_route_table_association&quot; &quot;private&quot; {\n  count          = 2\n  subnet_id      = aws_subnet.private[count.index].id\n  route_table_id = aws_route_table.private[count.index].id\n}\n</code></pre>\n<h3>Étape 8 : Security Groups</h3>\n<p>Créez les groupes de sécurité pour les différents tiers :</p>\n<pre><code class=\"language-hcl\">resource &quot;aws_security_group&quot; &quot;web&quot; {\n  name        = &quot;web-security-group&quot;\n  description = &quot;Security group for web servers&quot;\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = &quot;-1&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  tags = {\n    Name = &quot;web-security-group&quot;\n  }\n}\n\nresource &quot;aws_security_group&quot; &quot;private&quot; {\n  name        = &quot;private-security-group&quot;\n  description = &quot;Security group for private instances&quot;\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    from_port       = 22\n    to_port         = 22\n    protocol        = &quot;tcp&quot;\n    security_groups = [aws_security_group.web.id]\n  }\n\n  ingress {\n    from_port   = 3306\n    to_port     = 3306\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [aws_vpc.main.cidr_block]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = &quot;-1&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  tags = {\n    Name = &quot;private-security-group&quot;\n  }\n}\n</code></pre>\n<h3>Étape 9 : Instances EC2</h3>\n<p>Créez les instances dans les différents subnets :</p>\n<pre><code class=\"language-hcl\">resource &quot;aws_instance&quot; &quot;web&quot; {\n  count                  = 2\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = &quot;t2.micro&quot;\n  subnet_id              = aws_subnet.public[count.index].id\n  vpc_security_group_ids = [aws_security_group.web.id]\n\n  user_data = &lt;&lt;-EOF\n    #!/bin/bash\n    apt-get update\n    apt-get install -y nginx\n    echo &quot;&lt;h1&gt;Web Server ${count.index + 1} - AZ: ${data.aws_availability_zones.available.names[count.index]}&lt;/h1&gt;&quot; &gt; /var/www/html/index.html\n    systemctl start nginx\n    systemctl enable nginx\n  EOF\n\n  tags = {\n    Name = &quot;web-server-${count.index + 1}&quot;\n    Type = &quot;public&quot;\n  }\n}\n\nresource &quot;aws_instance&quot; &quot;private&quot; {\n  count                  = 2\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = &quot;t2.micro&quot;\n  subnet_id              = aws_subnet.private[count.index].id\n  vpc_security_group_ids = [aws_security_group.private.id]\n\n  tags = {\n    Name = &quot;private-instance-${count.index + 1}&quot;\n    Type = &quot;private&quot;\n  }\n}\n</code></pre>\n<h3>Étape 10 : Outputs</h3>\n<p>Ajoutez les outputs pour voir les résultats :</p>\n<pre><code class=\"language-hcl\">output &quot;vpc_id&quot; {\n  value = aws_vpc.main.id\n}\n\noutput &quot;public_subnet_ids&quot; {\n  value = aws_subnet.public[*].id\n}\n\noutput &quot;private_subnet_ids&quot; {\n  value = aws_subnet.private[*].id\n}\n\noutput &quot;web_server_public_ips&quot; {\n  value = aws_instance.web[*].public_ip\n}\n\noutput &quot;web_urls&quot; {\n  value = [for ip in aws_instance.web[*].public_ip : &quot;http://${ip}&quot;]\n}\n</code></pre>\n<h2>Déploiement</h2>\n<p>Pour déployer cette infrastructure :</p>\n<pre><code class=\"language-bash\">terraform init\nterraform plan\nterraform apply\n</code></pre>\n<p>Une fois le déploiement terminé, vous pouvez accéder aux serveurs web via leurs IPs publiques affichées dans les outputs. Les instances privées ne sont accessibles qu&#39;en passant par les instances publiques (architecture bastion).</p>\n<h2>Coûts et optimisations</h2>\n<p>Cette architecture inclut des composants payants :</p>\n<ul>\n<li><strong>NAT Gateways</strong> : Environ 0.045$/heure chacune</li>\n<li><strong>Elastic IPs</strong> : Gratuite si associée, payante si non utilisée</li>\n<li><strong>Transfert de données</strong> : Facturé pour le trafic sortant</li>\n</ul>\n<p>Pour un environnement de développement, vous pourriez :</p>\n<ul>\n<li>Utiliser une seule NAT Gateway au lieu de deux</li>\n<li>Remplacer les NAT Gateways par des NAT Instances (moins chères mais nécessitent plus de maintenance)</li>\n<li>Arrêter les ressources quand elles ne sont pas utilisées</li>\n</ul>\n<h2>Conclusion</h2>\n<p>Cette architecture VPC fournit une base solide pour déployer des applications sécurisées et hautement disponibles sur AWS. Elle suit les bonnes pratiques AWS en matière de sécurité, de disponibilité et d&#39;évolutivité. Dans les prochaines parties, nous construirons sur cette base pour déployer des applications plus complexes.</p>\n"
    },
    {
      "slug": "part6_terraform_state_workspaces",
      "title": "TP partie 6 - État Terraform et workspaces",
      "weight": 9,
      "metadata": {
        "title": "TP partie 6 - État Terraform et workspaces",
        "weight": 9
      },
      "content": "\nDans cette sixième partie, nous allons explorer en profondeur la gestion de l'état (state) dans Terraform et l'utilisation des workspaces pour gérer des environnements temporaires et des tests de fonctionnalités.\n\n## Étude de l'état Terraform\n\nL'état Terraform est un fichier JSON qui maintient la correspondance entre votre configuration et les ressources réelles dans le cloud. Nous allons explorer cet élément fondamental de Terraform.\n\n### Analyse du fichier d'état local\n\nPartez du code de la partie 5 (copiez le dossier ou commitez les changements). Dans le dossier part6, assurez vous du bon déploiement l'infrastructure (avoir un état) :\n\n```bash\nterraform init\nterraform plan -var-file=\"feature-a.tfvars\" -out=tfplan\nterraform apply tfplan\n```\n\n### Exploration via la CLI Terraform\n\nUtilisez les commandes Terraform pour inspecter l'état :\n\n```bash\n# Lister toutes les ressources dans l'état\nterraform state list\n\n# Afficher les détails d'une ressource\nterraform state show aws_vpc.main\n\n# Afficher l'état complet en format lisible\nterraform show\n\n# Afficher l'état en format JSON\nterraform show -json > state.json\n```\n\nCes commandes permettent d'explorer l'état sans le modifier. Vous y trouverez les identifiants des ressources (IDs AWS), les métadonnées de création, les dépendances entre ressources et les attributs sensibles (marqués comme tels).\n\n### Structure du fichier terraform.tfstate\n\nOuvrez le fichier `terraform.tfstate` dans votre éditeur pour examiner sa structure. C'est un fichier JSON lisible qui contient toute l'information sur votre infrastructure :\n\n```json\n{\n  \"version\": 4,\n  \"terraform_version\": \"1.5.7\",\n  \"serial\": 42,\n  \"lineage\": \"8a8b6c91-f6f7-c289-66f7-1b4e5dca8d50\",\n  \"outputs\": {\n    \"web_url\": {\n      \"value\": \"http://13.37.42.10\",\n      \"type\": \"string\"\n    }\n  },\n  \"resources\": [\n    {\n      \"mode\": \"managed\",\n      \"type\": \"aws_vpc\",\n      \"name\": \"main\",\n      \"provider\": \"provider[\\\"registry.terraform.io/hashicorp/aws\\\"]\",\n      \"instances\": [\n        {\n          \"schema_version\": 1,\n          \"attributes\": {\n            \"id\": \"vpc-0123456789abcdef0\",\n            \"cidr_block\": \"10.0.0.0/16\",\n            \"tags\": {\n              \"Name\": \"main-vpc\"\n            }\n          }\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Éléments importants de la structure :**\n\n- **version** : Version du format de fichier d'état (actuellement 4)\n- **terraform_version** : Version de Terraform qui a créé cet état\n- **serial** : Compteur incrémenté à chaque modification pour éviter les conflits\n- **lineage** : UUID unique généré à la création de l'état, reste constant pendant toute sa vie\n- **outputs** : Valeurs de sortie de votre configuration\n- **resources** : Liste complète des ressources avec leurs attributs AWS réels\n\nChaque ressource contient :\n- **mode** : \"managed\" (créée par Terraform) ou \"data\" (source de données)\n- **type** et **name** : Correspondent à votre configuration (ex: `aws_vpc.main`)\n- **provider** : Provider utilisé pour gérer cette ressource\n- **instances** : Détails de chaque instance de la ressource avec tous ses attributs\n\n### Le fichier terraform.tfstate.backup\n\nTerraform crée automatiquement un fichier `terraform.tfstate.backup` qui contient la version précédente de l'état avant la dernière modification. Ce fichier de sauvegarde est crucial pour la récupération en cas de problème :\n\n- Créé automatiquement à chaque `terraform apply` ou modification d'état\n- Contient l'état exact d'avant la dernière opération\n- Permet de revenir en arrière en cas de corruption ou d'erreur\n- Ne doit pas être commité dans Git mais peut être sauvegardé séparément\n\nEn cas de problème grave avec l'état, vous pouvez restaurer manuellement :\n```bash\n# En dernier recours uniquement !\ncp terraform.tfstate.backup terraform.tfstate\n```\n\n**⚠️ Important :** Ne modifiez jamais directement les fichiers d'état. Utilisez toujours les commandes Terraform pour toute manipulation.\n\n### Commandes avancées d'état\n\n```bash\n# Importer une ressource existante dans l'état\n# terraform import aws_vpc.main vpc-1234567890abcdef0\n\n# Retirer une ressource de l'état sans la détruire\n# terraform state rm aws_instance.web_server\n\n# Déplacer une ressource dans l'état\n# terraform state mv aws_instance.web_server aws_instance.web_server_renamed\n\n# Remplacer une ressource\n# terraform state replace-provider hashicorp/aws registry.terraform.io/hashicorp/aws\n```\n\n## Workspaces : cas d'usage et limitations\n\nLes workspaces Terraform permettent de créer plusieurs instances isolées d'une même configuration, chacune avec son propre fichier d'état. Contrairement à une idée répandue, ils ne sont pas la solution idéale pour séparer des environnements critiques comme production et développement. Il est facile de faire des erreurs avec en tout cas manuellement. Pour séparer dev et prod il on utilise plus classiquement deux backend séparés (avec un authentification et un code distinct qu'on peut factoriser avec des modules)\n\n### Comprendre les workspaces\n\nPar défaut, Terraform utilise un workspace nommé \"default\" :\n\n```bash\n# Voir le workspace actuel\nterraform workspace show\n\n# Créer et sélectionner un nouveau workspace\nterraform workspace new feature-test\nterraform workspace select feature-test\n\n# Lister tous les workspaces\nterraform workspace list\n```\n\nChaque workspace possède son propre fichier d'état, stocké dans le même backend mais dans des chemins séparés (ici avec un backend local `terraform.state.d`).\n\n### Cas d'usage appropriés pour les workspaces\n\nLes workspaces sont particulièrement adaptés pour :\n\n**Tests de branches de fonctionnalités** : Déployer temporairement une branche pour tests sans impacter l'environnement principal de développement.\n\n**Déploiements multi-régions** : Déployer la même application dans plusieurs régions AWS avec des variations mineures.\n\n**Environnements temporaires** : Créer des environnements éphémères pour des démonstrations ou des tests de charge.\n\n**Variantes d'une même application** : Déployer plusieurs versions d'une application dans le même environnement (par exemple, différentes configurations pour différents clients).\n\n### Adaptation du code pour les workspaces\n\nPour utiliser les workspaces efficacement, nous devons adapter notre code de la partie 5. Voici les modifications nécessaires :\n\n**Ajout d'une nouvelle variable dans `variables.tf`** :\n\n```coffee\nvariable \"feature_name\" {\n  description = \"Name of the feature being tested\"\n  type        = string\n  default     = \"main\"\n}\n```\n\n**Utilisation de `terraform.workspace` dans `vpc.tf`** :\n\nTerraform expose le nom du workspace actuel via `terraform.workspace`. Modifiez toutes les ressources VPC pour utiliser cette valeur :\n\n```coffee\n# VPC avec nom dynamique par workspace\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = var.vpc_cidr\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name      = \"${terraform.workspace}-vpc\"    # Changé de \"main-vpc\"\n    Workspace = terraform.workspace             # Nouveau tag\n    Feature   = var.feature_name               # Nouveau tag\n  }\n}\n\n# Internet Gateway\nresource \"aws_internet_gateway\" \"main\" {\n  vpc_id = aws_vpc.main.id\n\n  tags = {\n    Name      = \"${terraform.workspace}-igw\"    # Changé de \"main-igw\"\n    Workspace = terraform.workspace             # Nouveau tag\n    Feature   = var.feature_name               # Nouveau tag\n  }\n}\n\n# Subnet public\nresource \"aws_subnet\" \"public\" {\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = var.public_subnet_cidr\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name      = \"${terraform.workspace}-public-subnet\"  # Changé de \"public-subnet\"\n    Workspace = terraform.workspace                     # Nouveau tag\n    Feature   = var.feature_name                       # Nouveau tag\n  }\n}\n\n# Route table\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.main.id\n\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.main.id\n  }\n\n  tags = {\n    Name      = \"${terraform.workspace}-public-route-table\"  # Changé de \"public-route-table\"\n    Workspace = terraform.workspace                          # Nouveau tag\n    Feature   = var.feature_name                            # Nouveau tag\n  }\n}\n\n# Security Group avec nom unique par workspace\nresource \"aws_security_group\" \"web_ssh_access\" {\n  name        = \"${terraform.workspace}-web-ssh-access\"      # Changé de \"web-ssh-access\"\n  description = \"Allow SSH and HTTP access for ${terraform.workspace}\"\n  vpc_id      = aws_vpc.main.id\n\n  # ... règles inchangées ...\n\n  tags = {\n    Name      = \"${terraform.workspace}-web-ssh-access\"  # Changé de \"Web and SSH Access\"\n    Workspace = terraform.workspace                      # Nouveau tag\n    Feature   = var.feature_name                        # Nouveau tag\n  }\n}\n```\n\n**Modification de `webserver.tf`** :\n\n```coffee\nresource \"aws_instance\" \"web_server\" {\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = var.instance_type\n  subnet_id              = aws_subnet.public.id\n  vpc_security_group_ids = [aws_security_group.web_ssh_access.id]\n\n  connection {\n    type        = \"ssh\"\n    user        = \"root\"\n    private_key = file(var.ssh_key_path)\n    host        = self.public_ip\n  }\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"apt-get update\",\n      \"apt-get install -y nginx\",\n      \"systemctl start nginx\",\n      \"systemctl enable nginx\",\n      # Modification importante : affichage du workspace et de la fonctionnalité\n      \"echo '<h1>Feature: ${var.feature_name} (${terraform.workspace})</h1>' > /var/www/html/index.html\"\n    ]\n  }\n\n  tags = {\n    Name      = \"${terraform.workspace}-web-server\"  # Changé de \"nginx-web-server-vpc\"\n    Workspace = terraform.workspace                  # Nouveau tag\n    Feature   = var.feature_name                    # Nouveau tag\n  }\n}\n```\n\n**Création du fichier `feature-a.tfvars`** pour tester une fonctionnalité spécifique :\n\n```coffee\naws_region           = \"eu-west-3\"\naws_profile          = \"<awsprofile-votreprenom>\"\nvpc_cidr             = \"10.100.0.0/16\"\npublic_subnet_cidr   = \"10.100.1.0/24\"\ninstance_type        = \"t2.micro\"\nssh_key_path         = \"~/.ssh/id_terraform\"\nfeature_name         = \"feature-payment-api\"\n```\n\nCes modifications permettent à chaque workspace d'avoir des noms de ressources uniques et d'afficher clairement dans quel contexte il fonctionne.\n\n### Déploiement de branches de fonctionnalités\n\n```bash\n# Déploiement de la branche feature-payment\nterraform workspace new feature-payment\nterraform plan -var-file=\"feature-a.tfvars\" -out=feature.tfplan\nterraform apply feature.tfplan\n\n# Retour au workspace principal\nterraform workspace select default\n\n# Nettoyage après les tests\nterraform workspace select feature-payment\nterraform destroy -var-file=\"feature-a.tfvars\"\nterraform workspace select default\nterraform workspace delete feature-payment\n```\n\n### Limitations des workspaces pour les environnements\n\nLes workspaces présentent des limitations importantes pour la séparation d'environnements critiques :\n\n**Même backend partagé** : Tous les workspaces utilisent le même backend S3, donc les mêmes permissions d'accès. Impossible d'isoler réellement production et développement.\n\n**Manque de visibilité et Risque d'erreurs humaines** : Le workspace actuel n'est pas visible dans le code. Un `terraform destroy` ou mauvaise modification dans le mauvais workspace peut avoir des conséquences catastrophiques.\n\n**Pas de séparation des pipelines** : Plus difficile d'avoir des processus CI/CD différents par workspace.\n\n### Alternative recommandée pour les environnements\n\nPour une vraie séparation dev/staging/prod, privilégiez :\n\n```\n# Structure de répertoires séparés\nterraform-infra/\n├── environments/\n│   ├── dev/\n│   │   ├── main.tf\n│   │   ├── backend.tf  # Backend S3 différent\n│   │   └── terraform.tfvars\n│   ├── staging/\n│   │   ├── main.tf\n│   │   ├── backend.tf  # Backend S3 différent\n│   │   └── terraform.tfvars\n│   └── prod/\n│       ├── main.tf\n│       ├── backend.tf  # Backend S3 différent\n│       └── terraform.tfvars\n└── modules/\n    ├── vpc/\n    └── webserver/\n```\n\nCette approche offre une vraie isolation avec des backends séparés, des permissions différentes et des pipelines CI/CD distincts. nous verrons cela dans un TP suivant.\n\n### Gestion des états par workspace\n\nChaque workspace maintient son propre fichier d'état. Pour un backend local :\n\n```bash\n# Les états sont stockés dans\n# terraform.tfstate.d/feature-payment/terraform.tfstate\n# terraform.tfstate.d/feature-auth/terraform.tfstate\n\nls -la terraform.tfstate.d/\n```\n\nPour un backend S3, les états sont organisés avec le préfixe `env:` :\n```\ns3://bucket-name/env:/feature-payment/path/to/state\ns3://bucket-name/env:/feature-auth/path/to/state\n```\n\n\nPoints clés à retenir :\n- L'état Terraform est le cœur de votre infrastructure et doit être protégé\n- Les workspaces sont utiles pour des variations temporaires, pas pour isoler prod/dev\n- Pour de vrais environnements séparés, utilisez des répertoires et backends distincts\n- Les workspaces permettent de tester des branches de fonctionnalités de manière isolée\n\nDans la partie suivante, nous configurerons un backend S3 distant avec verrouillage DynamoDB pour permettre la collaboration sécurisée entre développeurs.",
      "html": "<p>Dans cette sixième partie, nous allons explorer en profondeur la gestion de l&#39;état (state) dans Terraform et l&#39;utilisation des workspaces pour gérer des environnements temporaires et des tests de fonctionnalités.</p>\n<h2>Étude de l&#39;état Terraform</h2>\n<p>L&#39;état Terraform est un fichier JSON qui maintient la correspondance entre votre configuration et les ressources réelles dans le cloud. Nous allons explorer cet élément fondamental de Terraform.</p>\n<h3>Analyse du fichier d&#39;état local</h3>\n<p>Partez du code de la partie 5 (copiez le dossier ou commitez les changements). Dans le dossier part6, assurez vous du bon déploiement l&#39;infrastructure (avoir un état) :</p>\n<pre><code class=\"language-bash\">terraform init\nterraform plan -var-file=&quot;feature-a.tfvars&quot; -out=tfplan\nterraform apply tfplan\n</code></pre>\n<h3>Exploration via la CLI Terraform</h3>\n<p>Utilisez les commandes Terraform pour inspecter l&#39;état :</p>\n<pre><code class=\"language-bash\"># Lister toutes les ressources dans l&#39;état\nterraform state list\n\n# Afficher les détails d&#39;une ressource\nterraform state show aws_vpc.main\n\n# Afficher l&#39;état complet en format lisible\nterraform show\n\n# Afficher l&#39;état en format JSON\nterraform show -json &gt; state.json\n</code></pre>\n<p>Ces commandes permettent d&#39;explorer l&#39;état sans le modifier. Vous y trouverez les identifiants des ressources (IDs AWS), les métadonnées de création, les dépendances entre ressources et les attributs sensibles (marqués comme tels).</p>\n<h3>Structure du fichier terraform.tfstate</h3>\n<p>Ouvrez le fichier <code>terraform.tfstate</code> dans votre éditeur pour examiner sa structure. C&#39;est un fichier JSON lisible qui contient toute l&#39;information sur votre infrastructure :</p>\n<pre><code class=\"language-json\">{\n  &quot;version&quot;: 4,\n  &quot;terraform_version&quot;: &quot;1.5.7&quot;,\n  &quot;serial&quot;: 42,\n  &quot;lineage&quot;: &quot;8a8b6c91-f6f7-c289-66f7-1b4e5dca8d50&quot;,\n  &quot;outputs&quot;: {\n    &quot;web_url&quot;: {\n      &quot;value&quot;: &quot;http://13.37.42.10&quot;,\n      &quot;type&quot;: &quot;string&quot;\n    }\n  },\n  &quot;resources&quot;: [\n    {\n      &quot;mode&quot;: &quot;managed&quot;,\n      &quot;type&quot;: &quot;aws_vpc&quot;,\n      &quot;name&quot;: &quot;main&quot;,\n      &quot;provider&quot;: &quot;provider[\\&quot;registry.terraform.io/hashicorp/aws\\&quot;]&quot;,\n      &quot;instances&quot;: [\n        {\n          &quot;schema_version&quot;: 1,\n          &quot;attributes&quot;: {\n            &quot;id&quot;: &quot;vpc-0123456789abcdef0&quot;,\n            &quot;cidr_block&quot;: &quot;10.0.0.0/16&quot;,\n            &quot;tags&quot;: {\n              &quot;Name&quot;: &quot;main-vpc&quot;\n            }\n          }\n        }\n      ]\n    }\n  ]\n}\n</code></pre>\n<p><strong>Éléments importants de la structure :</strong></p>\n<ul>\n<li><strong>version</strong> : Version du format de fichier d&#39;état (actuellement 4)</li>\n<li><strong>terraform_version</strong> : Version de Terraform qui a créé cet état</li>\n<li><strong>serial</strong> : Compteur incrémenté à chaque modification pour éviter les conflits</li>\n<li><strong>lineage</strong> : UUID unique généré à la création de l&#39;état, reste constant pendant toute sa vie</li>\n<li><strong>outputs</strong> : Valeurs de sortie de votre configuration</li>\n<li><strong>resources</strong> : Liste complète des ressources avec leurs attributs AWS réels</li>\n</ul>\n<p>Chaque ressource contient :</p>\n<ul>\n<li><strong>mode</strong> : &quot;managed&quot; (créée par Terraform) ou &quot;data&quot; (source de données)</li>\n<li><strong>type</strong> et <strong>name</strong> : Correspondent à votre configuration (ex: <code>aws_vpc.main</code>)</li>\n<li><strong>provider</strong> : Provider utilisé pour gérer cette ressource</li>\n<li><strong>instances</strong> : Détails de chaque instance de la ressource avec tous ses attributs</li>\n</ul>\n<h3>Le fichier terraform.tfstate.backup</h3>\n<p>Terraform crée automatiquement un fichier <code>terraform.tfstate.backup</code> qui contient la version précédente de l&#39;état avant la dernière modification. Ce fichier de sauvegarde est crucial pour la récupération en cas de problème :</p>\n<ul>\n<li>Créé automatiquement à chaque <code>terraform apply</code> ou modification d&#39;état</li>\n<li>Contient l&#39;état exact d&#39;avant la dernière opération</li>\n<li>Permet de revenir en arrière en cas de corruption ou d&#39;erreur</li>\n<li>Ne doit pas être commité dans Git mais peut être sauvegardé séparément</li>\n</ul>\n<p>En cas de problème grave avec l&#39;état, vous pouvez restaurer manuellement :</p>\n<pre><code class=\"language-bash\"># En dernier recours uniquement !\ncp terraform.tfstate.backup terraform.tfstate\n</code></pre>\n<p><strong>⚠️ Important :</strong> Ne modifiez jamais directement les fichiers d&#39;état. Utilisez toujours les commandes Terraform pour toute manipulation.</p>\n<h3>Commandes avancées d&#39;état</h3>\n<pre><code class=\"language-bash\"># Importer une ressource existante dans l&#39;état\n# terraform import aws_vpc.main vpc-1234567890abcdef0\n\n# Retirer une ressource de l&#39;état sans la détruire\n# terraform state rm aws_instance.web_server\n\n# Déplacer une ressource dans l&#39;état\n# terraform state mv aws_instance.web_server aws_instance.web_server_renamed\n\n# Remplacer une ressource\n# terraform state replace-provider hashicorp/aws registry.terraform.io/hashicorp/aws\n</code></pre>\n<h2>Workspaces : cas d&#39;usage et limitations</h2>\n<p>Les workspaces Terraform permettent de créer plusieurs instances isolées d&#39;une même configuration, chacune avec son propre fichier d&#39;état. Contrairement à une idée répandue, ils ne sont pas la solution idéale pour séparer des environnements critiques comme production et développement. Il est facile de faire des erreurs avec en tout cas manuellement. Pour séparer dev et prod il on utilise plus classiquement deux backend séparés (avec un authentification et un code distinct qu&#39;on peut factoriser avec des modules)</p>\n<h3>Comprendre les workspaces</h3>\n<p>Par défaut, Terraform utilise un workspace nommé &quot;default&quot; :</p>\n<pre><code class=\"language-bash\"># Voir le workspace actuel\nterraform workspace show\n\n# Créer et sélectionner un nouveau workspace\nterraform workspace new feature-test\nterraform workspace select feature-test\n\n# Lister tous les workspaces\nterraform workspace list\n</code></pre>\n<p>Chaque workspace possède son propre fichier d&#39;état, stocké dans le même backend mais dans des chemins séparés (ici avec un backend local <code>terraform.state.d</code>).</p>\n<h3>Cas d&#39;usage appropriés pour les workspaces</h3>\n<p>Les workspaces sont particulièrement adaptés pour :</p>\n<p><strong>Tests de branches de fonctionnalités</strong> : Déployer temporairement une branche pour tests sans impacter l&#39;environnement principal de développement.</p>\n<p><strong>Déploiements multi-régions</strong> : Déployer la même application dans plusieurs régions AWS avec des variations mineures.</p>\n<p><strong>Environnements temporaires</strong> : Créer des environnements éphémères pour des démonstrations ou des tests de charge.</p>\n<p><strong>Variantes d&#39;une même application</strong> : Déployer plusieurs versions d&#39;une application dans le même environnement (par exemple, différentes configurations pour différents clients).</p>\n<h3>Adaptation du code pour les workspaces</h3>\n<p>Pour utiliser les workspaces efficacement, nous devons adapter notre code de la partie 5. Voici les modifications nécessaires :</p>\n<p><strong>Ajout d&#39;une nouvelle variable dans <code>variables.tf</code></strong> :</p>\n<pre><code class=\"language-coffee\">variable &quot;feature_name&quot; {\n  description = &quot;Name of the feature being tested&quot;\n  type        = string\n  default     = &quot;main&quot;\n}\n</code></pre>\n<p><strong>Utilisation de <code>terraform.workspace</code> dans <code>vpc.tf</code></strong> :</p>\n<p>Terraform expose le nom du workspace actuel via <code>terraform.workspace</code>. Modifiez toutes les ressources VPC pour utiliser cette valeur :</p>\n<pre><code class=\"language-coffee\"># VPC avec nom dynamique par workspace\nresource &quot;aws_vpc&quot; &quot;main&quot; {\n  cidr_block           = var.vpc_cidr\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-vpc&quot;    # Changé de &quot;main-vpc&quot;\n    Workspace = terraform.workspace             # Nouveau tag\n    Feature   = var.feature_name               # Nouveau tag\n  }\n}\n\n# Internet Gateway\nresource &quot;aws_internet_gateway&quot; &quot;main&quot; {\n  vpc_id = aws_vpc.main.id\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-igw&quot;    # Changé de &quot;main-igw&quot;\n    Workspace = terraform.workspace             # Nouveau tag\n    Feature   = var.feature_name               # Nouveau tag\n  }\n}\n\n# Subnet public\nresource &quot;aws_subnet&quot; &quot;public&quot; {\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = var.public_subnet_cidr\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-public-subnet&quot;  # Changé de &quot;public-subnet&quot;\n    Workspace = terraform.workspace                     # Nouveau tag\n    Feature   = var.feature_name                       # Nouveau tag\n  }\n}\n\n# Route table\nresource &quot;aws_route_table&quot; &quot;public&quot; {\n  vpc_id = aws_vpc.main.id\n\n  route {\n    cidr_block = &quot;0.0.0.0/0&quot;\n    gateway_id = aws_internet_gateway.main.id\n  }\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-public-route-table&quot;  # Changé de &quot;public-route-table&quot;\n    Workspace = terraform.workspace                          # Nouveau tag\n    Feature   = var.feature_name                            # Nouveau tag\n  }\n}\n\n# Security Group avec nom unique par workspace\nresource &quot;aws_security_group&quot; &quot;web_ssh_access&quot; {\n  name        = &quot;${terraform.workspace}-web-ssh-access&quot;      # Changé de &quot;web-ssh-access&quot;\n  description = &quot;Allow SSH and HTTP access for ${terraform.workspace}&quot;\n  vpc_id      = aws_vpc.main.id\n\n  # ... règles inchangées ...\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-web-ssh-access&quot;  # Changé de &quot;Web and SSH Access&quot;\n    Workspace = terraform.workspace                      # Nouveau tag\n    Feature   = var.feature_name                        # Nouveau tag\n  }\n}\n</code></pre>\n<p><strong>Modification de <code>webserver.tf</code></strong> :</p>\n<pre><code class=\"language-coffee\">resource &quot;aws_instance&quot; &quot;web_server&quot; {\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = var.instance_type\n  subnet_id              = aws_subnet.public.id\n  vpc_security_group_ids = [aws_security_group.web_ssh_access.id]\n\n  connection {\n    type        = &quot;ssh&quot;\n    user        = &quot;root&quot;\n    private_key = file(var.ssh_key_path)\n    host        = self.public_ip\n  }\n\n  provisioner &quot;remote-exec&quot; {\n    inline = [\n      &quot;apt-get update&quot;,\n      &quot;apt-get install -y nginx&quot;,\n      &quot;systemctl start nginx&quot;,\n      &quot;systemctl enable nginx&quot;,\n      # Modification importante : affichage du workspace et de la fonctionnalité\n      &quot;echo &#39;&lt;h1&gt;Feature: ${var.feature_name} (${terraform.workspace})&lt;/h1&gt;&#39; &gt; /var/www/html/index.html&quot;\n    ]\n  }\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-web-server&quot;  # Changé de &quot;nginx-web-server-vpc&quot;\n    Workspace = terraform.workspace                  # Nouveau tag\n    Feature   = var.feature_name                    # Nouveau tag\n  }\n}\n</code></pre>\n<p><strong>Création du fichier <code>feature-a.tfvars</code></strong> pour tester une fonctionnalité spécifique :</p>\n<pre><code class=\"language-coffee\">aws_region           = &quot;eu-west-3&quot;\naws_profile          = &quot;&lt;awsprofile-votreprenom&gt;&quot;\nvpc_cidr             = &quot;10.100.0.0/16&quot;\npublic_subnet_cidr   = &quot;10.100.1.0/24&quot;\ninstance_type        = &quot;t2.micro&quot;\nssh_key_path         = &quot;~/.ssh/id_terraform&quot;\nfeature_name         = &quot;feature-payment-api&quot;\n</code></pre>\n<p>Ces modifications permettent à chaque workspace d&#39;avoir des noms de ressources uniques et d&#39;afficher clairement dans quel contexte il fonctionne.</p>\n<h3>Déploiement de branches de fonctionnalités</h3>\n<pre><code class=\"language-bash\"># Déploiement de la branche feature-payment\nterraform workspace new feature-payment\nterraform plan -var-file=&quot;feature-a.tfvars&quot; -out=feature.tfplan\nterraform apply feature.tfplan\n\n# Retour au workspace principal\nterraform workspace select default\n\n# Nettoyage après les tests\nterraform workspace select feature-payment\nterraform destroy -var-file=&quot;feature-a.tfvars&quot;\nterraform workspace select default\nterraform workspace delete feature-payment\n</code></pre>\n<h3>Limitations des workspaces pour les environnements</h3>\n<p>Les workspaces présentent des limitations importantes pour la séparation d&#39;environnements critiques :</p>\n<p><strong>Même backend partagé</strong> : Tous les workspaces utilisent le même backend S3, donc les mêmes permissions d&#39;accès. Impossible d&#39;isoler réellement production et développement.</p>\n<p><strong>Manque de visibilité et Risque d&#39;erreurs humaines</strong> : Le workspace actuel n&#39;est pas visible dans le code. Un <code>terraform destroy</code> ou mauvaise modification dans le mauvais workspace peut avoir des conséquences catastrophiques.</p>\n<p><strong>Pas de séparation des pipelines</strong> : Plus difficile d&#39;avoir des processus CI/CD différents par workspace.</p>\n<h3>Alternative recommandée pour les environnements</h3>\n<p>Pour une vraie séparation dev/staging/prod, privilégiez :</p>\n<pre><code># Structure de répertoires séparés\nterraform-infra/\n├── environments/\n│   ├── dev/\n│   │   ├── main.tf\n│   │   ├── backend.tf  # Backend S3 différent\n│   │   └── terraform.tfvars\n│   ├── staging/\n│   │   ├── main.tf\n│   │   ├── backend.tf  # Backend S3 différent\n│   │   └── terraform.tfvars\n│   └── prod/\n│       ├── main.tf\n│       ├── backend.tf  # Backend S3 différent\n│       └── terraform.tfvars\n└── modules/\n    ├── vpc/\n    └── webserver/\n</code></pre>\n<p>Cette approche offre une vraie isolation avec des backends séparés, des permissions différentes et des pipelines CI/CD distincts. nous verrons cela dans un TP suivant.</p>\n<h3>Gestion des états par workspace</h3>\n<p>Chaque workspace maintient son propre fichier d&#39;état. Pour un backend local :</p>\n<pre><code class=\"language-bash\"># Les états sont stockés dans\n# terraform.tfstate.d/feature-payment/terraform.tfstate\n# terraform.tfstate.d/feature-auth/terraform.tfstate\n\nls -la terraform.tfstate.d/\n</code></pre>\n<p>Pour un backend S3, les états sont organisés avec le préfixe <code>env:</code> :</p>\n<pre><code>s3://bucket-name/env:/feature-payment/path/to/state\ns3://bucket-name/env:/feature-auth/path/to/state\n</code></pre>\n<p>Points clés à retenir :</p>\n<ul>\n<li>L&#39;état Terraform est le cœur de votre infrastructure et doit être protégé</li>\n<li>Les workspaces sont utiles pour des variations temporaires, pas pour isoler prod/dev</li>\n<li>Pour de vrais environnements séparés, utilisez des répertoires et backends distincts</li>\n<li>Les workspaces permettent de tester des branches de fonctionnalités de manière isolée</li>\n</ul>\n<p>Dans la partie suivante, nous configurerons un backend S3 distant avec verrouillage DynamoDB pour permettre la collaboration sécurisée entre développeurs.</p>\n"
    },
    {
      "slug": "part7_backend_s3",
      "title": "TP partie 7 - Backend S3 et collaboration",
      "weight": 10,
      "metadata": {
        "title": "TP partie 7 - Backend S3 et collaboration",
        "weight": 10
      },
      "content": "\nDans cette septième partie, nous allons configurer un backend S3 distant avec verrouillage DynamoDB pour permettre la collaboration sécurisée entre développeurs et une gestion centralisée de l'état Terraform.\n\n## Backend distant avec S3\n\nUn backend distant permet le partage sécurisé de l'état entre plusieurs utilisateurs et systèmes CI/CD. Nous allons configurer un backend S3 avec verrouillage DynamoDB.\n\n### Création de l'infrastructure backend\n\n**⚠️ Important :** Le backend S3 doit être créé séparément avant d'être utilisé. Nous allons utiliser un projet distinct au niveau racine pour créer cette infrastructure.\n\nL'infrastructure backend comprend :\n- Un bucket S3 chiffré avec versioning pour stocker les états\n- Une table DynamoDB pour le verrouillage d'état\n- Des politiques de sécurité appropriées\n\n### Configuration du backend S3\n\nDans notre projet principal, copiez le contenu de part6 vers part7 :\n\n```bash\n# Copiez la structure depuis part6\ncp -r part6_terraform_state/* part7_backend_s3/\n```\n\nCréez un fichier `backend.tf` (et supprimez `versions.tf` qui est ici remplacé) avec la configuration du backend distant :\n\n```coffee\nterraform {\n  required_version = \">= 1.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n\n  backend \"s3\" {\n    bucket         = \"terraform-state-<YOUR-BUCKET-NAME>\"\n    key            = \"tp-fil-rouge-dev/terraform.tfstate\"\n    region         = \"eu-west-3\"\n    profile        = \"<awsprofile-votreprenom>\"\n    encrypt        = true\n    use_lockfile   = true\n    dynamodb_table = \"terraform-state-lock\"\n  }\n}\n```\n\n**Important :** Remplacez `<YOUR-BUCKET-NAME>` par le nom réel du bucket créé par le projet `s3_backend`.\n\n### Migration vers le backend distant\n\nAvant de migrer, assurez-vous que votre infrastructure backend S3 est déployée :\n\n```bash\n# Vérifiez que le backend S3 existe\ncd ../part7_annexe_s3_bucket_creation\nterraform output\n\n# Revenez au dossier part7\ncd ../part7_backend_s3\n```\n\nProcédez à la migration :\n\n```bash\n# Initialisez avec le nouveau backend\nterraform init\n\n# Terraform vous demandera si vous voulez migrer l'état\n# Répondez \"yes\" pour copier l'état local vers S3\n```\n\n### Vérification du backend distant\n\n```bash\n# Vérifiez que l'état est maintenant sur S3\naws s3 ls s3://terraform-state-<YOUR-BUCKET-NAME>/ --recursive --profile=<votreprenom>\n\n# Vérifiez les workspaces\nterraform workspace list\n\n# Changez de workspace et observez les différents fichiers d'état\nterraform workspace new test-s3-lock\naws s3 ls s3://terraform-state-<YOUR-BUCKET-NAME>/ --recursive --profile=<votreprenom>\n```\n\n### Déploiement avec backend distant\n\nDéployez votre infrastructure avec le backend S3 :\n\n```bash\n# Planifiez et appliquez\nterraform plan -out=tfplan\nterraform apply tfplan\n\n# Vérifiez l'état dans S3\naws s3 ls s3://terraform-state-<YOUR-BUCKET-NAME>/ --recursive --profile=<votreprenom>\n```\n\n### Avantages du backend distant et collaboration en équipe\n\nLe backend distant offre plusieurs avantages essentiels :\n\n- **Collaboration** : Plusieurs développeurs peuvent travailler sur la même infrastructure en partageant l'état, évitant les conflits et les incohérences\n- **Verrouillage** : Via DynamoDB, le système empêche les modifications simultanées qui pourraient corrompre l'état. Chaque opération terraform lock l'état pendant son exécution  \n- **Sécurité** : Le chiffrement au repos et en transit protège les données sensibles. Les versions précédentes de l'état sont conservées pour la récupération\n- **Partage d'état** : Tous les membres de l'équipe accèdent au même état centralisé, éliminant les divergences locales\n- **Audit et traçabilité** : S3 conserve l'historique des modifications avec le versioning, permettant de tracer qui a modifié quoi et quand\n- **Sécurité d'accès** : Les permissions IAM contrôlent l'accès au bucket et à la table DynamoDB, permettant une gestion fine des autorisations\n- **Intégration CI/CD** : Les pipelines d'intégration continue peuvent accéder à l'état partagé pour les déploiements automatisés\n\n### Le mécanisme de lock file\n\nLe paramètre `use_lockfile = true` active explicitement le système de verrouillage d'état de Terraform, tandis que `dynamodb_table` spécifie où stocker les verrous. Voici comment cela fonctionne :\n\n**Lock file** : Terraform crée un enregistrement de verrouillage temporaire dans la table DynamoDB spécifiée qui empêche les modifications simultanées de l'état. Ce verrou contient :\n- L'ID de l'opération en cours\n- L'horodatage du début de l'opération  \n- L'utilisateur qui a acquis le verrou\n- Les informations sur l'environnement d'exécution\n\n**Cycle de vie du verrou** :\n1. **Acquisition** : Terraform tente d'acquérir le verrou au début de chaque opération (`plan`, `apply`, `destroy`)\n2. **Maintien** : Le verrou est maintenu pendant toute la durée de l'opération\n3. **Libération** : Le verrou est automatiquement libéré à la fin de l'opération (succès ou échec)\n\n**Gestion des conflits** : Si une seconde opération tente de modifier le même état, elle attendra que le premier verrou soit libéré ou affichera une erreur après un timeout.\n\n### Test du verrouillage\n\nPour tester le mécanisme de verrouillage, ouvrez deux terminaux :\n\n```bash\n# Terminal 1 - Lancez une opération longue\nterraform worspace list (assurez vous d'etre dans test-s3-lock)\nterraform destroy -auto-approve && terraform apply -auto-approve\n\n# Terminal 2 - Tentez une opération simultanée\nterraform plan\n# Vous devriez voir un message de verrouillage\n```\n\nLe second terminal affichera un message similaire à :\n```\nAcquiring state lock. This may take a few moments...\nError: Error acquiring the state lock\n```\n\n\n### Exemple de workflow collaboratif\n\n```bash\n# Développeur A - Crée une branche et un workspace\ngit checkout -b feature-api-v2\nterraform workspace new feature-api-v2\nterraform plan -var-file=\"feature-api.tfvars\" -out=feature.tfplan\nterraform apply feature.tfplan\n\n# Développeur B - Travaille sur une autre fonctionnalité (pas de verrouillage meme si meme etat car workspaces differents)\ngit checkout -b feature-ui-update\nterraform workspace new feature-ui-update\nterraform plan -var-file=\"feature-ui.tfvars\" -out=feature.tfplan\nterraform apply feature.tfplan\n\n# Après validation, nettoyage\nterraform workspace select feature-api-v2\nterraform destroy -var-file=\"feature-api.tfvars\"\nterraform workspace select default\nterraform workspace delete feature-api-v2\n```\n\nCette partie vous a montré comment configurer et utiliser un backend S3 distant avec verrouillage DynamoDB pour la collaboration sécurisée. Le backend distant est essentiel pour le travail en équipe et la gestion sécurisée de l'infrastructure.\n\n## Alternatives pour le backend\n\n",
      "html": "<p>Dans cette septième partie, nous allons configurer un backend S3 distant avec verrouillage DynamoDB pour permettre la collaboration sécurisée entre développeurs et une gestion centralisée de l&#39;état Terraform.</p>\n<h2>Backend distant avec S3</h2>\n<p>Un backend distant permet le partage sécurisé de l&#39;état entre plusieurs utilisateurs et systèmes CI/CD. Nous allons configurer un backend S3 avec verrouillage DynamoDB.</p>\n<h3>Création de l&#39;infrastructure backend</h3>\n<p><strong>⚠️ Important :</strong> Le backend S3 doit être créé séparément avant d&#39;être utilisé. Nous allons utiliser un projet distinct au niveau racine pour créer cette infrastructure.</p>\n<p>L&#39;infrastructure backend comprend :</p>\n<ul>\n<li>Un bucket S3 chiffré avec versioning pour stocker les états</li>\n<li>Une table DynamoDB pour le verrouillage d&#39;état</li>\n<li>Des politiques de sécurité appropriées</li>\n</ul>\n<h3>Configuration du backend S3</h3>\n<p>Dans notre projet principal, copiez le contenu de part6 vers part7 :</p>\n<pre><code class=\"language-bash\"># Copiez la structure depuis part6\ncp -r part6_terraform_state/* part7_backend_s3/\n</code></pre>\n<p>Créez un fichier <code>backend.tf</code> (et supprimez <code>versions.tf</code> qui est ici remplacé) avec la configuration du backend distant :</p>\n<pre><code class=\"language-coffee\">terraform {\n  required_version = &quot;&gt;= 1.0&quot;\n\n  required_providers {\n    aws = {\n      source  = &quot;hashicorp/aws&quot;\n      version = &quot;~&gt; 5.0&quot;\n    }\n  }\n\n  backend &quot;s3&quot; {\n    bucket         = &quot;terraform-state-&lt;YOUR-BUCKET-NAME&gt;&quot;\n    key            = &quot;tp-fil-rouge-dev/terraform.tfstate&quot;\n    region         = &quot;eu-west-3&quot;\n    profile        = &quot;&lt;awsprofile-votreprenom&gt;&quot;\n    encrypt        = true\n    use_lockfile   = true\n    dynamodb_table = &quot;terraform-state-lock&quot;\n  }\n}\n</code></pre>\n<p><strong>Important :</strong> Remplacez <code>&lt;YOUR-BUCKET-NAME&gt;</code> par le nom réel du bucket créé par le projet <code>s3_backend</code>.</p>\n<h3>Migration vers le backend distant</h3>\n<p>Avant de migrer, assurez-vous que votre infrastructure backend S3 est déployée :</p>\n<pre><code class=\"language-bash\"># Vérifiez que le backend S3 existe\ncd ../part7_annexe_s3_bucket_creation\nterraform output\n\n# Revenez au dossier part7\ncd ../part7_backend_s3\n</code></pre>\n<p>Procédez à la migration :</p>\n<pre><code class=\"language-bash\"># Initialisez avec le nouveau backend\nterraform init\n\n# Terraform vous demandera si vous voulez migrer l&#39;état\n# Répondez &quot;yes&quot; pour copier l&#39;état local vers S3\n</code></pre>\n<h3>Vérification du backend distant</h3>\n<pre><code class=\"language-bash\"># Vérifiez que l&#39;état est maintenant sur S3\naws s3 ls s3://terraform-state-&lt;YOUR-BUCKET-NAME&gt;/ --recursive --profile=&lt;votreprenom&gt;\n\n# Vérifiez les workspaces\nterraform workspace list\n\n# Changez de workspace et observez les différents fichiers d&#39;état\nterraform workspace new test-s3-lock\naws s3 ls s3://terraform-state-&lt;YOUR-BUCKET-NAME&gt;/ --recursive --profile=&lt;votreprenom&gt;\n</code></pre>\n<h3>Déploiement avec backend distant</h3>\n<p>Déployez votre infrastructure avec le backend S3 :</p>\n<pre><code class=\"language-bash\"># Planifiez et appliquez\nterraform plan -out=tfplan\nterraform apply tfplan\n\n# Vérifiez l&#39;état dans S3\naws s3 ls s3://terraform-state-&lt;YOUR-BUCKET-NAME&gt;/ --recursive --profile=&lt;votreprenom&gt;\n</code></pre>\n<h3>Avantages du backend distant et collaboration en équipe</h3>\n<p>Le backend distant offre plusieurs avantages essentiels :</p>\n<ul>\n<li><strong>Collaboration</strong> : Plusieurs développeurs peuvent travailler sur la même infrastructure en partageant l&#39;état, évitant les conflits et les incohérences</li>\n<li><strong>Verrouillage</strong> : Via DynamoDB, le système empêche les modifications simultanées qui pourraient corrompre l&#39;état. Chaque opération terraform lock l&#39;état pendant son exécution  </li>\n<li><strong>Sécurité</strong> : Le chiffrement au repos et en transit protège les données sensibles. Les versions précédentes de l&#39;état sont conservées pour la récupération</li>\n<li><strong>Partage d&#39;état</strong> : Tous les membres de l&#39;équipe accèdent au même état centralisé, éliminant les divergences locales</li>\n<li><strong>Audit et traçabilité</strong> : S3 conserve l&#39;historique des modifications avec le versioning, permettant de tracer qui a modifié quoi et quand</li>\n<li><strong>Sécurité d&#39;accès</strong> : Les permissions IAM contrôlent l&#39;accès au bucket et à la table DynamoDB, permettant une gestion fine des autorisations</li>\n<li><strong>Intégration CI/CD</strong> : Les pipelines d&#39;intégration continue peuvent accéder à l&#39;état partagé pour les déploiements automatisés</li>\n</ul>\n<h3>Le mécanisme de lock file</h3>\n<p>Le paramètre <code>use_lockfile = true</code> active explicitement le système de verrouillage d&#39;état de Terraform, tandis que <code>dynamodb_table</code> spécifie où stocker les verrous. Voici comment cela fonctionne :</p>\n<p><strong>Lock file</strong> : Terraform crée un enregistrement de verrouillage temporaire dans la table DynamoDB spécifiée qui empêche les modifications simultanées de l&#39;état. Ce verrou contient :</p>\n<ul>\n<li>L&#39;ID de l&#39;opération en cours</li>\n<li>L&#39;horodatage du début de l&#39;opération  </li>\n<li>L&#39;utilisateur qui a acquis le verrou</li>\n<li>Les informations sur l&#39;environnement d&#39;exécution</li>\n</ul>\n<p><strong>Cycle de vie du verrou</strong> :</p>\n<ol>\n<li><strong>Acquisition</strong> : Terraform tente d&#39;acquérir le verrou au début de chaque opération (<code>plan</code>, <code>apply</code>, <code>destroy</code>)</li>\n<li><strong>Maintien</strong> : Le verrou est maintenu pendant toute la durée de l&#39;opération</li>\n<li><strong>Libération</strong> : Le verrou est automatiquement libéré à la fin de l&#39;opération (succès ou échec)</li>\n</ol>\n<p><strong>Gestion des conflits</strong> : Si une seconde opération tente de modifier le même état, elle attendra que le premier verrou soit libéré ou affichera une erreur après un timeout.</p>\n<h3>Test du verrouillage</h3>\n<p>Pour tester le mécanisme de verrouillage, ouvrez deux terminaux :</p>\n<pre><code class=\"language-bash\"># Terminal 1 - Lancez une opération longue\nterraform worspace list (assurez vous d&#39;etre dans test-s3-lock)\nterraform destroy -auto-approve &amp;&amp; terraform apply -auto-approve\n\n# Terminal 2 - Tentez une opération simultanée\nterraform plan\n# Vous devriez voir un message de verrouillage\n</code></pre>\n<p>Le second terminal affichera un message similaire à :</p>\n<pre><code>Acquiring state lock. This may take a few moments...\nError: Error acquiring the state lock\n</code></pre>\n<h3>Exemple de workflow collaboratif</h3>\n<pre><code class=\"language-bash\"># Développeur A - Crée une branche et un workspace\ngit checkout -b feature-api-v2\nterraform workspace new feature-api-v2\nterraform plan -var-file=&quot;feature-api.tfvars&quot; -out=feature.tfplan\nterraform apply feature.tfplan\n\n# Développeur B - Travaille sur une autre fonctionnalité (pas de verrouillage meme si meme etat car workspaces differents)\ngit checkout -b feature-ui-update\nterraform workspace new feature-ui-update\nterraform plan -var-file=&quot;feature-ui.tfvars&quot; -out=feature.tfplan\nterraform apply feature.tfplan\n\n# Après validation, nettoyage\nterraform workspace select feature-api-v2\nterraform destroy -var-file=&quot;feature-api.tfvars&quot;\nterraform workspace select default\nterraform workspace delete feature-api-v2\n</code></pre>\n<p>Cette partie vous a montré comment configurer et utiliser un backend S3 distant avec verrouillage DynamoDB pour la collaboration sécurisée. Le backend distant est essentiel pour le travail en équipe et la gestion sécurisée de l&#39;infrastructure.</p>\n<h2>Alternatives pour le backend</h2>\n"
    },
    {
      "slug": "part8_count_loadbalancer",
      "title": "TP partie 8 - Count, for_each et haute disponibilité",
      "weight": 11,
      "metadata": {
        "title": "TP partie 8 - Count, for_each et haute disponibilité",
        "weight": 11
      },
      "content": "\nDans cette huitième partie, nous allons explorer les concepts Terraform de `count` et `for_each` pour multiplier les ressources, et implémenter un load balancer pour assurer la haute disponibilité de notre application web.\n\n## Concepts de multiplication des ressources\n\nTerraform offre plusieurs mécanismes pour créer plusieurs instances d'une même ressource. Cette capacité est essentielle pour construire des infrastructures redondantes et hautement disponibles.\n\n### Count vs for_each\n\nTerraform propose deux approches principales pour créer plusieurs ressources :\n\n**Count** : Crée un nombre défini d'instances indexées numériquement (0, 1, 2...). Idéal quand vous voulez simplement multiplier une ressource par un nombre.\n\n**For_each** : Crée des instances basées sur un set ou une map, avec des clés nommées. Plus flexible pour des configurations variées et évite les problèmes de réindexation.\n\n### Haute disponibilité et load balancing\n\nLa haute disponibilité consiste à distribuer la charge sur plusieurs serveurs pour éviter les points de défaillance unique. Si un serveur tombe en panne, les autres continuent de servir le trafic.\n\nUn **load balancer** (répartiteur de charge) distribue intelligemment le trafic entrant entre plusieurs serveurs backend selon différents algorithmes (round-robin, least connections, etc.).\n\n## Architecture cible\n\nNotre objectif est de créer une infrastructure avec :\n- **3 serveurs web** dans la même région\n- Un **Application Load Balancer (ALB)** automatiquement créé si le nombre de serveurs > 1\n- **Target Group** pour gérer la santé des serveurs\n- **Health checks** automatiques\n- Configuration dynamique basée sur le nombre d'instances\n\n![Architecture Part 8 - Load Balancer et haute disponibilité](/part8_count_loadbalancer/architecture_part8.png)\n\n## Implémentation avec count\n\nCommençons par modifier notre configuration pour utiliser `count` et créer plusieurs serveurs web.\n\n### Ajout des variables de configuration\n\nModifiez le fichier `variables.tf` pour ajouter les nouvelles variables :\n\n```coffee\nvariable \"instance_count\" {\n  description = \"Number of web server instances\"\n  type        = number\n  default     = 3\n}\n```\n\n### Modification de webserver.tf avec count\n\nTransformons notre instance unique en plusieurs instances avec `count` :\n\n```coffee\n...\n# Instances EC2 avec count\nresource \"aws_instance\" \"web_server\" {\n  count                  = var.instance_count\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = var.instance_type\n  subnet_id              = aws_subnet.public.id\n  vpc_security_group_ids = [aws_security_group.web_servers.id]\n\n  connection {\n    type        = \"ssh\"\n    user        = \"root\"\n    private_key = file(var.ssh_key_path)\n    host        = self.public_ip\n  }\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"apt-get update\",\n      \"apt-get install -y nginx\",\n      \"systemctl start nginx\",\n      \"systemctl enable nginx\",\n      \"echo '<h1>Server ${count.index + 1} - Feature: ${var.feature_name} (${terraform.workspace})</h1>' > /var/www/html/index.html\",\n      \"echo '<p>Instance ID: ${count.index}</p>' >> /var/www/html/index.html\"\n    ]\n  }\n\n  tags = {\n    Name      = \"${terraform.workspace}-web-server-${count.index + 1}\"\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n    Server    = \"web-${count.index + 1}\"\n  }\n}\n```\n\n### Gestion des security groups\n\nNous devons séparer les security groups pour l'ALB et les serveurs web. Modifiez `vpc.tf` :\n\n```coffee\n# Security Group pour les serveurs web\nresource \"aws_security_group\" \"web_servers\" {\n  name        = \"${terraform.workspace}-web-servers\"\n  description = \"Security group for web servers\"\n  vpc_id      = aws_vpc.main.id\n\n  # SSH depuis l'extérieur (pour administration)\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  # HTTP depuis l'ALB seulement (si plusieurs instances)\n  dynamic \"ingress\" {\n    for_each = var.instance_count > 1 ? [1] : []\n    content {\n      from_port       = 80\n      to_port         = 80\n      protocol        = \"tcp\"\n      security_groups = [aws_security_group.alb[0].id]\n    }\n  }\n\n  # HTTP direct si une seule instance\n  dynamic \"ingress\" {\n    for_each = var.instance_count == 1 ? [1] : []\n    content {\n      from_port   = 80\n      to_port     = 80\n      protocol    = \"tcp\"\n      cidr_blocks = [\"0.0.0.0/0\"]\n    }\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name      = \"${terraform.workspace}-web-servers-sg\"\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n  }\n}\n\n# Security Group pour l'ALB (conditionnel)\nresource \"aws_security_group\" \"alb\" {\n  count       = var.instance_count > 1 ? 1 : 0\n  name        = \"${terraform.workspace}-alb-sg\"\n  description = \"Security group for Application Load Balancer\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name      = \"${terraform.workspace}-alb-sg\"\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n  }\n}\n```\n\n## Configuration du Load Balancer\n\nAllons lire la documentation des resources autour de Application Load Balancer (Amazon LoadBalancer) sur le registry terraform : \n\nhttps://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lb\n\n### Composants d'un Application Load Balancer\n\nUn ALB est composé de plusieurs ressources Terraform interconnectées :\n\n![Composants ALB - Relations entre ressources Terraform](/part8_count_loadbalancer/alb-components.png)\n\n**Ressources principales :**\n- **aws_lb** : Le load balancer principal qui reçoit le trafic\n- **aws_lb_listener** : Écoute sur un port spécifique (80, 443) et définit les règles de routage\n- **aws_lb_target_group** : Groupe logique qui contient les instances backend\n- **aws_lb_target_group_attachment** : Lie chaque instance EC2 au target group\n- **Health Checks** : Vérifications automatiques de santé des instances\n\n**Flux de trafic :**\n1. Client → ALB (port 80)\n2. ALB → Listener (règles de routage)\n3. Listener → Target Group (forward)\n4. Target Group → Instances saines (round-robin)\n\nCréons un nouveau fichier `loadbalancer.tf` pour gérer l'ALB :\n\n```coffee\n# Application Load Balancer (conditionnel)\nresource \"aws_lb\" \"main\" {\n  count              = var.instance_count > 1 ? 1 : 0\n  name               = \"${terraform.workspace}-alb\"\n  internal           = false\n  load_balancer_type = \"application\"\n  security_groups    = [aws_security_group.alb[0].id]\n  subnets           = [aws_subnet.public.id]\n\n  enable_deletion_protection = false\n\n  tags = {\n    Name        = \"${terraform.workspace}-alb\"\n    Workspace   = terraform.workspace\n    Feature     = var.feature_name\n    Environment = \"load-balancer\"\n  }\n}\n\n# Target Group pour les serveurs web\nresource \"aws_lb_target_group\" \"web_servers\" {\n  count    = var.instance_count > 1 ? 1 : 0\n  name     = \"${terraform.workspace}-web-tg\"\n  port     = 80\n  protocol = \"HTTP\"\n  vpc_id   = aws_vpc.main.id\n\n  health_check {\n    enabled             = true\n    healthy_threshold   = 2\n    interval            = 30\n    matcher             = \"200\"\n    path                = \"/\"\n    port                = \"traffic-port\"\n    protocol            = \"HTTP\"\n    timeout             = 5\n    unhealthy_threshold = 2\n  }\n\n  tags = {\n    Name      = \"${terraform.workspace}-web-tg\"\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n  }\n}\n\n# Attachement des instances au Target Group\nresource \"aws_lb_target_group_attachment\" \"web_servers\" {\n  count            = var.instance_count > 1 ? var.instance_count : 0\n  target_group_arn = aws_lb_target_group.web_servers[0].arn\n  target_id        = aws_instance.web_server[count.index].id\n  port             = 80\n}\n\n# Listener pour l'ALB\nresource \"aws_lb_listener\" \"web\" {\n  count             = var.instance_count > 1 ? 1 : 0\n  load_balancer_arn = aws_lb.main[0].arn\n  port              = \"80\"\n  protocol          = \"HTTP\"\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.web_servers[0].arn\n  }\n\n  tags = {\n    Name      = \"${terraform.workspace}-web-listener\"\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n  }\n}\n```\n\n## Sorties adaptatives\n\nPour une meilleure organisation, créons un fichier `outputs.tf` dédié qui rassemble toutes les sorties du projet :\n\n```coffee\n# Outputs réseau\noutput \"vpc_id\" {\n  description = \"ID of the VPC\"\n  value       = aws_vpc.main.id\n}\n\noutput \"subnet_id\" {\n  description = \"ID of the public subnet\"\n  value       = aws_subnet.public.id\n}\n\n# Outputs pour les instances\noutput \"instance_ids\" {\n  description = \"IDs of the web server instances\"\n  value       = aws_instance.web_server[*].id\n}\n\noutput \"instance_public_ips\" {\n  description = \"Public IP addresses of the web server instances\"\n  value       = aws_instance.web_server[*].public_ip\n}\n\n# Output conditionnel pour l'ALB\noutput \"load_balancer_dns\" {\n  description = \"DNS name of the load balancer (if enabled)\"\n  value       = var.instance_count > 1 ? aws_lb.main[0].dns_name : null\n}\n\n# URL de l'application (ALB ou première instance)\noutput \"web_url\" {\n  description = \"URL to access the web application\"\n  value = var.instance_count > 1 ? \"http://${aws_lb.main[0].dns_name}\" : \"http://${aws_instance.web_server[0].public_ip}\"\n}\n\n# URLs de toutes les instances (pour debug)\noutput \"individual_server_urls\" {\n  description = \"URLs of individual servers\"\n  value = [\n    for instance in aws_instance.web_server : \n    \"http://${instance.public_ip}\"\n  ]\n}\n```\n\nN'oubliez pas de supprimer les outputs de `webserver.tf` et `vpc.tf`.\n\n## Concepts avancés : for_each\n\nBien que nous utilisions `count` dans cet exemple, `for_each` offre plus de flexibilité. \n\nVoici en particulier un exemple alternatif ou les serveurs ne sont pas des instances indistinctes mais des entités nomméés et stables. Cet aspect est important les instances sont stateful et pas nécessairement identiques (ce qui n'est pas notre cas ici).\n\n```coffee\n# Configuration avec for_each (exemple alternatif)\nvariable \"web_servers\" {\n  description = \"Configuration for web servers\"\n  type = map(object({\n    instance_type = string\n    name_suffix   = string\n  }))\n  default = {\n    \"web1\" = {\n      instance_type = \"t2.micro\"\n      name_suffix   = \"primary\"\n    }\n    \"web2\" = {\n      instance_type = \"t2.micro\"\n      name_suffix   = \"secondary\"\n    }\n    \"web3\" = {\n      instance_type = \"t2.micro\"\n      name_suffix   = \"tertiary\"\n    }\n  }\n}\n\n# Instances avec for_each (commenté pour cet exemple)\n/*\nresource \"aws_instance\" \"web_server_foreach\" {\n  for_each               = var.web_servers\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = each.value.instance_type\n  subnet_id              = aws_subnet.public.id\n  vpc_security_group_ids = [aws_security_group.web_servers.id]\n\n  tags = {\n    Name      = \"${terraform.workspace}-${each.key}-${each.value.name_suffix}\"\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n    ServerKey = each.key\n  }\n}\n*/\n```\n\n## Déploiement et test\n\n### Fichiers de variables pour test\n\nCréez différents fichiers de variables pour tester les configurations :\n\n**single-server.tfvars** (un seul serveur) :\n```coffee\naws_region           = \"eu-west-3\"\naws_profile          = \"<awsprofile-votreprenom>\"\nvpc_cidr             = \"10.0.0.0/16\"\npublic_subnet_cidr   = \"10.0.1.0/24\"\ninstance_type        = \"t2.micro\"\nssh_key_path         = \"~/.ssh/id_terraform\"\nfeature_name         = \"single-server-test\"\ninstance_count       = 1\n```\n\n**multi-server.tfvars** (serveurs multiples avec ALB) :\n```coffee\naws_region           = \"eu-west-3\"\naws_profile          = \"<awsprofile-votreprenom>\"\nvpc_cidr             = \"10.0.0.0/16\"\npublic_subnet_cidr   = \"10.0.1.0/24\"\ninstance_type        = \"t2.micro\"\nssh_key_path         = \"~/.ssh/id_terraform\"\nfeature_name         = \"load-balanced-cluster\"\ninstance_count       = 3\n```\n\n### Commandes de déploiement\n\n```bash\n# Déploiement avec un seul serveur\nterraform workspace new single-server\nterraform plan -var-file=\"single-server.tfvars\" -out=single.tfplan\nterraform apply single.tfplan\n\n# Test de l'application\ncurl $(terraform output -raw web_url)\n\n# Passage à plusieurs serveurs\nterraform workspace new multi-server\nterraform plan -var-file=\"multi-server.tfvars\" -out=multi.tfplan\nterraform apply multi.tfplan\n\n# Test du load balancer\nfor i in {1..10}; do curl $(terraform output -raw web_url); sleep 1; done\n```\n\n### Vérification de la haute disponibilité\n\n```bash\n# Vérifier les instances\nterraform output instance_ids\nterraform output instance_public_ips\n\n# Tester chaque serveur individuellement\nterraform output -json individual_server_urls | jq -r '.[]' | while read url; do\n  echo \"Testing $url\"\n  curl \"$url\"\ndone\n\n# Vérifier le health check du load balancer\naws elbv2 describe-target-health \\\n  --target-group-arn $(aws elbv2 describe-target-groups \\\n    --names \"${terraform.workspace}-web-tg\" \\\n    --query 'TargetGroups[0].TargetGroupArn' \\\n    --output text) \\\n  --profile <awsprofile-votreprenom>\n```\n\n## Avantages et bonnes pratiques\n\n### Avantages de count\n\n**Simplicité** : Facile à comprendre et implémenter pour des ressources identiques.\n\n**Performance** : Création parallèle des ressources pour un déploiement rapide.\n\n**Flexibilité numérique** : Ajustement facile du nombre d'instances via une variable.\n\n### Avantages de for_each\n\n**Clés stables** : Évite les problèmes de réindexation lors des modifications.\n\n**Configuration flexible** : Chaque instance peut avoir une configuration différente.\n\n**Lisibilité** : Les clés nommées rendent la configuration plus claire.\n\n### Haute disponibilité\n\n**Redondance** : Plusieurs serveurs éliminent les points de défaillance unique.\n\n**Répartition de charge** : Distribution intelligente du trafic entre les serveurs.\n\n**Health checks** : Détection automatique des serveurs défaillants.\n\n**Scalabilité** : Ajout facile de nouvelles instances selon la charge.\n\n## Conclusion\n\nCette partie vous a montré comment utiliser `count` pour multiplier les ressources et implémenter un load balancer pour la haute disponibilité. Nous avons créé une infrastructure qui s'adapte automatiquement au nombre d'instances : une instance unique sans load balancer, ou plusieurs instances derrière un ALB.\n\nPoints clés à retenir :\n- `count` permet de créer facilement plusieurs instances d'une ressource\n- `for_each` offre plus de flexibilité pour des configurations variées\n- Un load balancer améliore la disponibilité et la performance\n- La configuration conditionnelle permet une infrastructure adaptative\n- Les health checks automatiques garantissent la fiabilité du service\n\nDans la prochaine partie, nous explorerons les modules Terraform pour réutiliser et organiser notre code de manière modulaire.",
      "html": "<p>Dans cette huitième partie, nous allons explorer les concepts Terraform de <code>count</code> et <code>for_each</code> pour multiplier les ressources, et implémenter un load balancer pour assurer la haute disponibilité de notre application web.</p>\n<h2>Concepts de multiplication des ressources</h2>\n<p>Terraform offre plusieurs mécanismes pour créer plusieurs instances d&#39;une même ressource. Cette capacité est essentielle pour construire des infrastructures redondantes et hautement disponibles.</p>\n<h3>Count vs for_each</h3>\n<p>Terraform propose deux approches principales pour créer plusieurs ressources :</p>\n<p><strong>Count</strong> : Crée un nombre défini d&#39;instances indexées numériquement (0, 1, 2...). Idéal quand vous voulez simplement multiplier une ressource par un nombre.</p>\n<p><strong>For_each</strong> : Crée des instances basées sur un set ou une map, avec des clés nommées. Plus flexible pour des configurations variées et évite les problèmes de réindexation.</p>\n<h3>Haute disponibilité et load balancing</h3>\n<p>La haute disponibilité consiste à distribuer la charge sur plusieurs serveurs pour éviter les points de défaillance unique. Si un serveur tombe en panne, les autres continuent de servir le trafic.</p>\n<p>Un <strong>load balancer</strong> (répartiteur de charge) distribue intelligemment le trafic entrant entre plusieurs serveurs backend selon différents algorithmes (round-robin, least connections, etc.).</p>\n<h2>Architecture cible</h2>\n<p>Notre objectif est de créer une infrastructure avec :</p>\n<ul>\n<li><strong>3 serveurs web</strong> dans la même région</li>\n<li>Un <strong>Application Load Balancer (ALB)</strong> automatiquement créé si le nombre de serveurs &gt; 1</li>\n<li><strong>Target Group</strong> pour gérer la santé des serveurs</li>\n<li><strong>Health checks</strong> automatiques</li>\n<li>Configuration dynamique basée sur le nombre d&#39;instances</li>\n</ul>\n<p><img src=\"/part8_count_loadbalancer/architecture_part8.png\" alt=\"Architecture Part 8 - Load Balancer et haute disponibilité\"></p>\n<h2>Implémentation avec count</h2>\n<p>Commençons par modifier notre configuration pour utiliser <code>count</code> et créer plusieurs serveurs web.</p>\n<h3>Ajout des variables de configuration</h3>\n<p>Modifiez le fichier <code>variables.tf</code> pour ajouter les nouvelles variables :</p>\n<pre><code class=\"language-coffee\">variable &quot;instance_count&quot; {\n  description = &quot;Number of web server instances&quot;\n  type        = number\n  default     = 3\n}\n</code></pre>\n<h3>Modification de webserver.tf avec count</h3>\n<p>Transformons notre instance unique en plusieurs instances avec <code>count</code> :</p>\n<pre><code class=\"language-coffee\">...\n# Instances EC2 avec count\nresource &quot;aws_instance&quot; &quot;web_server&quot; {\n  count                  = var.instance_count\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = var.instance_type\n  subnet_id              = aws_subnet.public.id\n  vpc_security_group_ids = [aws_security_group.web_servers.id]\n\n  connection {\n    type        = &quot;ssh&quot;\n    user        = &quot;root&quot;\n    private_key = file(var.ssh_key_path)\n    host        = self.public_ip\n  }\n\n  provisioner &quot;remote-exec&quot; {\n    inline = [\n      &quot;apt-get update&quot;,\n      &quot;apt-get install -y nginx&quot;,\n      &quot;systemctl start nginx&quot;,\n      &quot;systemctl enable nginx&quot;,\n      &quot;echo &#39;&lt;h1&gt;Server ${count.index + 1} - Feature: ${var.feature_name} (${terraform.workspace})&lt;/h1&gt;&#39; &gt; /var/www/html/index.html&quot;,\n      &quot;echo &#39;&lt;p&gt;Instance ID: ${count.index}&lt;/p&gt;&#39; &gt;&gt; /var/www/html/index.html&quot;\n    ]\n  }\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-web-server-${count.index + 1}&quot;\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n    Server    = &quot;web-${count.index + 1}&quot;\n  }\n}\n</code></pre>\n<h3>Gestion des security groups</h3>\n<p>Nous devons séparer les security groups pour l&#39;ALB et les serveurs web. Modifiez <code>vpc.tf</code> :</p>\n<pre><code class=\"language-coffee\"># Security Group pour les serveurs web\nresource &quot;aws_security_group&quot; &quot;web_servers&quot; {\n  name        = &quot;${terraform.workspace}-web-servers&quot;\n  description = &quot;Security group for web servers&quot;\n  vpc_id      = aws_vpc.main.id\n\n  # SSH depuis l&#39;extérieur (pour administration)\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  # HTTP depuis l&#39;ALB seulement (si plusieurs instances)\n  dynamic &quot;ingress&quot; {\n    for_each = var.instance_count &gt; 1 ? [1] : []\n    content {\n      from_port       = 80\n      to_port         = 80\n      protocol        = &quot;tcp&quot;\n      security_groups = [aws_security_group.alb[0].id]\n    }\n  }\n\n  # HTTP direct si une seule instance\n  dynamic &quot;ingress&quot; {\n    for_each = var.instance_count == 1 ? [1] : []\n    content {\n      from_port   = 80\n      to_port     = 80\n      protocol    = &quot;tcp&quot;\n      cidr_blocks = [&quot;0.0.0.0/0&quot;]\n    }\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = &quot;-1&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-web-servers-sg&quot;\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n  }\n}\n\n# Security Group pour l&#39;ALB (conditionnel)\nresource &quot;aws_security_group&quot; &quot;alb&quot; {\n  count       = var.instance_count &gt; 1 ? 1 : 0\n  name        = &quot;${terraform.workspace}-alb-sg&quot;\n  description = &quot;Security group for Application Load Balancer&quot;\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  ingress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = &quot;-1&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-alb-sg&quot;\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n  }\n}\n</code></pre>\n<h2>Configuration du Load Balancer</h2>\n<p>Allons lire la documentation des resources autour de Application Load Balancer (Amazon LoadBalancer) sur le registry terraform : </p>\n<p><a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lb\">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lb</a></p>\n<h3>Composants d&#39;un Application Load Balancer</h3>\n<p>Un ALB est composé de plusieurs ressources Terraform interconnectées :</p>\n<p><img src=\"/part8_count_loadbalancer/alb-components.png\" alt=\"Composants ALB - Relations entre ressources Terraform\"></p>\n<p><strong>Ressources principales :</strong></p>\n<ul>\n<li><strong>aws_lb</strong> : Le load balancer principal qui reçoit le trafic</li>\n<li><strong>aws_lb_listener</strong> : Écoute sur un port spécifique (80, 443) et définit les règles de routage</li>\n<li><strong>aws_lb_target_group</strong> : Groupe logique qui contient les instances backend</li>\n<li><strong>aws_lb_target_group_attachment</strong> : Lie chaque instance EC2 au target group</li>\n<li><strong>Health Checks</strong> : Vérifications automatiques de santé des instances</li>\n</ul>\n<p><strong>Flux de trafic :</strong></p>\n<ol>\n<li>Client → ALB (port 80)</li>\n<li>ALB → Listener (règles de routage)</li>\n<li>Listener → Target Group (forward)</li>\n<li>Target Group → Instances saines (round-robin)</li>\n</ol>\n<p>Créons un nouveau fichier <code>loadbalancer.tf</code> pour gérer l&#39;ALB :</p>\n<pre><code class=\"language-coffee\"># Application Load Balancer (conditionnel)\nresource &quot;aws_lb&quot; &quot;main&quot; {\n  count              = var.instance_count &gt; 1 ? 1 : 0\n  name               = &quot;${terraform.workspace}-alb&quot;\n  internal           = false\n  load_balancer_type = &quot;application&quot;\n  security_groups    = [aws_security_group.alb[0].id]\n  subnets           = [aws_subnet.public.id]\n\n  enable_deletion_protection = false\n\n  tags = {\n    Name        = &quot;${terraform.workspace}-alb&quot;\n    Workspace   = terraform.workspace\n    Feature     = var.feature_name\n    Environment = &quot;load-balancer&quot;\n  }\n}\n\n# Target Group pour les serveurs web\nresource &quot;aws_lb_target_group&quot; &quot;web_servers&quot; {\n  count    = var.instance_count &gt; 1 ? 1 : 0\n  name     = &quot;${terraform.workspace}-web-tg&quot;\n  port     = 80\n  protocol = &quot;HTTP&quot;\n  vpc_id   = aws_vpc.main.id\n\n  health_check {\n    enabled             = true\n    healthy_threshold   = 2\n    interval            = 30\n    matcher             = &quot;200&quot;\n    path                = &quot;/&quot;\n    port                = &quot;traffic-port&quot;\n    protocol            = &quot;HTTP&quot;\n    timeout             = 5\n    unhealthy_threshold = 2\n  }\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-web-tg&quot;\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n  }\n}\n\n# Attachement des instances au Target Group\nresource &quot;aws_lb_target_group_attachment&quot; &quot;web_servers&quot; {\n  count            = var.instance_count &gt; 1 ? var.instance_count : 0\n  target_group_arn = aws_lb_target_group.web_servers[0].arn\n  target_id        = aws_instance.web_server[count.index].id\n  port             = 80\n}\n\n# Listener pour l&#39;ALB\nresource &quot;aws_lb_listener&quot; &quot;web&quot; {\n  count             = var.instance_count &gt; 1 ? 1 : 0\n  load_balancer_arn = aws_lb.main[0].arn\n  port              = &quot;80&quot;\n  protocol          = &quot;HTTP&quot;\n\n  default_action {\n    type             = &quot;forward&quot;\n    target_group_arn = aws_lb_target_group.web_servers[0].arn\n  }\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-web-listener&quot;\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n  }\n}\n</code></pre>\n<h2>Sorties adaptatives</h2>\n<p>Pour une meilleure organisation, créons un fichier <code>outputs.tf</code> dédié qui rassemble toutes les sorties du projet :</p>\n<pre><code class=\"language-coffee\"># Outputs réseau\noutput &quot;vpc_id&quot; {\n  description = &quot;ID of the VPC&quot;\n  value       = aws_vpc.main.id\n}\n\noutput &quot;subnet_id&quot; {\n  description = &quot;ID of the public subnet&quot;\n  value       = aws_subnet.public.id\n}\n\n# Outputs pour les instances\noutput &quot;instance_ids&quot; {\n  description = &quot;IDs of the web server instances&quot;\n  value       = aws_instance.web_server[*].id\n}\n\noutput &quot;instance_public_ips&quot; {\n  description = &quot;Public IP addresses of the web server instances&quot;\n  value       = aws_instance.web_server[*].public_ip\n}\n\n# Output conditionnel pour l&#39;ALB\noutput &quot;load_balancer_dns&quot; {\n  description = &quot;DNS name of the load balancer (if enabled)&quot;\n  value       = var.instance_count &gt; 1 ? aws_lb.main[0].dns_name : null\n}\n\n# URL de l&#39;application (ALB ou première instance)\noutput &quot;web_url&quot; {\n  description = &quot;URL to access the web application&quot;\n  value = var.instance_count &gt; 1 ? &quot;http://${aws_lb.main[0].dns_name}&quot; : &quot;http://${aws_instance.web_server[0].public_ip}&quot;\n}\n\n# URLs de toutes les instances (pour debug)\noutput &quot;individual_server_urls&quot; {\n  description = &quot;URLs of individual servers&quot;\n  value = [\n    for instance in aws_instance.web_server : \n    &quot;http://${instance.public_ip}&quot;\n  ]\n}\n</code></pre>\n<p>N&#39;oubliez pas de supprimer les outputs de <code>webserver.tf</code> et <code>vpc.tf</code>.</p>\n<h2>Concepts avancés : for_each</h2>\n<p>Bien que nous utilisions <code>count</code> dans cet exemple, <code>for_each</code> offre plus de flexibilité. </p>\n<p>Voici en particulier un exemple alternatif ou les serveurs ne sont pas des instances indistinctes mais des entités nomméés et stables. Cet aspect est important les instances sont stateful et pas nécessairement identiques (ce qui n&#39;est pas notre cas ici).</p>\n<pre><code class=\"language-coffee\"># Configuration avec for_each (exemple alternatif)\nvariable &quot;web_servers&quot; {\n  description = &quot;Configuration for web servers&quot;\n  type = map(object({\n    instance_type = string\n    name_suffix   = string\n  }))\n  default = {\n    &quot;web1&quot; = {\n      instance_type = &quot;t2.micro&quot;\n      name_suffix   = &quot;primary&quot;\n    }\n    &quot;web2&quot; = {\n      instance_type = &quot;t2.micro&quot;\n      name_suffix   = &quot;secondary&quot;\n    }\n    &quot;web3&quot; = {\n      instance_type = &quot;t2.micro&quot;\n      name_suffix   = &quot;tertiary&quot;\n    }\n  }\n}\n\n# Instances avec for_each (commenté pour cet exemple)\n/*\nresource &quot;aws_instance&quot; &quot;web_server_foreach&quot; {\n  for_each               = var.web_servers\n  ami                    = data.aws_ami.custom_ubuntu.id\n  instance_type          = each.value.instance_type\n  subnet_id              = aws_subnet.public.id\n  vpc_security_group_ids = [aws_security_group.web_servers.id]\n\n  tags = {\n    Name      = &quot;${terraform.workspace}-${each.key}-${each.value.name_suffix}&quot;\n    Workspace = terraform.workspace\n    Feature   = var.feature_name\n    ServerKey = each.key\n  }\n}\n*/\n</code></pre>\n<h2>Déploiement et test</h2>\n<h3>Fichiers de variables pour test</h3>\n<p>Créez différents fichiers de variables pour tester les configurations :</p>\n<p><strong>single-server.tfvars</strong> (un seul serveur) :</p>\n<pre><code class=\"language-coffee\">aws_region           = &quot;eu-west-3&quot;\naws_profile          = &quot;&lt;awsprofile-votreprenom&gt;&quot;\nvpc_cidr             = &quot;10.0.0.0/16&quot;\npublic_subnet_cidr   = &quot;10.0.1.0/24&quot;\ninstance_type        = &quot;t2.micro&quot;\nssh_key_path         = &quot;~/.ssh/id_terraform&quot;\nfeature_name         = &quot;single-server-test&quot;\ninstance_count       = 1\n</code></pre>\n<p><strong>multi-server.tfvars</strong> (serveurs multiples avec ALB) :</p>\n<pre><code class=\"language-coffee\">aws_region           = &quot;eu-west-3&quot;\naws_profile          = &quot;&lt;awsprofile-votreprenom&gt;&quot;\nvpc_cidr             = &quot;10.0.0.0/16&quot;\npublic_subnet_cidr   = &quot;10.0.1.0/24&quot;\ninstance_type        = &quot;t2.micro&quot;\nssh_key_path         = &quot;~/.ssh/id_terraform&quot;\nfeature_name         = &quot;load-balanced-cluster&quot;\ninstance_count       = 3\n</code></pre>\n<h3>Commandes de déploiement</h3>\n<pre><code class=\"language-bash\"># Déploiement avec un seul serveur\nterraform workspace new single-server\nterraform plan -var-file=&quot;single-server.tfvars&quot; -out=single.tfplan\nterraform apply single.tfplan\n\n# Test de l&#39;application\ncurl $(terraform output -raw web_url)\n\n# Passage à plusieurs serveurs\nterraform workspace new multi-server\nterraform plan -var-file=&quot;multi-server.tfvars&quot; -out=multi.tfplan\nterraform apply multi.tfplan\n\n# Test du load balancer\nfor i in {1..10}; do curl $(terraform output -raw web_url); sleep 1; done\n</code></pre>\n<h3>Vérification de la haute disponibilité</h3>\n<pre><code class=\"language-bash\"># Vérifier les instances\nterraform output instance_ids\nterraform output instance_public_ips\n\n# Tester chaque serveur individuellement\nterraform output -json individual_server_urls | jq -r &#39;.[]&#39; | while read url; do\n  echo &quot;Testing $url&quot;\n  curl &quot;$url&quot;\ndone\n\n# Vérifier le health check du load balancer\naws elbv2 describe-target-health \\\n  --target-group-arn $(aws elbv2 describe-target-groups \\\n    --names &quot;${terraform.workspace}-web-tg&quot; \\\n    --query &#39;TargetGroups[0].TargetGroupArn&#39; \\\n    --output text) \\\n  --profile &lt;awsprofile-votreprenom&gt;\n</code></pre>\n<h2>Avantages et bonnes pratiques</h2>\n<h3>Avantages de count</h3>\n<p><strong>Simplicité</strong> : Facile à comprendre et implémenter pour des ressources identiques.</p>\n<p><strong>Performance</strong> : Création parallèle des ressources pour un déploiement rapide.</p>\n<p><strong>Flexibilité numérique</strong> : Ajustement facile du nombre d&#39;instances via une variable.</p>\n<h3>Avantages de for_each</h3>\n<p><strong>Clés stables</strong> : Évite les problèmes de réindexation lors des modifications.</p>\n<p><strong>Configuration flexible</strong> : Chaque instance peut avoir une configuration différente.</p>\n<p><strong>Lisibilité</strong> : Les clés nommées rendent la configuration plus claire.</p>\n<h3>Haute disponibilité</h3>\n<p><strong>Redondance</strong> : Plusieurs serveurs éliminent les points de défaillance unique.</p>\n<p><strong>Répartition de charge</strong> : Distribution intelligente du trafic entre les serveurs.</p>\n<p><strong>Health checks</strong> : Détection automatique des serveurs défaillants.</p>\n<p><strong>Scalabilité</strong> : Ajout facile de nouvelles instances selon la charge.</p>\n<h2>Conclusion</h2>\n<p>Cette partie vous a montré comment utiliser <code>count</code> pour multiplier les ressources et implémenter un load balancer pour la haute disponibilité. Nous avons créé une infrastructure qui s&#39;adapte automatiquement au nombre d&#39;instances : une instance unique sans load balancer, ou plusieurs instances derrière un ALB.</p>\n<p>Points clés à retenir :</p>\n<ul>\n<li><code>count</code> permet de créer facilement plusieurs instances d&#39;une ressource</li>\n<li><code>for_each</code> offre plus de flexibilité pour des configurations variées</li>\n<li>Un load balancer améliore la disponibilité et la performance</li>\n<li>La configuration conditionnelle permet une infrastructure adaptative</li>\n<li>Les health checks automatiques garantissent la fiabilité du service</li>\n</ul>\n<p>Dans la prochaine partie, nous explorerons les modules Terraform pour réutiliser et organiser notre code de manière modulaire.</p>\n"
    },
    {
      "slug": "part9_refactorisation_modules",
      "title": "TP partie 9 - Modules et gestion d'état avancée",
      "weight": 12,
      "metadata": {
        "title": "TP partie 9 - Modules et gestion d'état avancée",
        "weight": 12
      },
      "content": "\nDans cette neuvième partie, nous allons apprendre à refactoriser notre infrastructure en modules Terraform et maîtriser les techniques avancées de gestion d'état : `terraform state mv` et `terraform import`.\n\n## Problématiques de la refactorisation\n\nQuand on refactorise du code Terraform existant, on se heurte à un problème fondamental : **Terraform suit les ressources par leur adresse dans l'état**. Si vous avez une ressource déclarée directement puis que vous la déplacez dans un module, Terraform ne reconnaît pas que c'est la même ressource ! Il va vouloir détruire l'ancienne et créer une nouvelle, ce qui peut entraîner une interruption de service.\n\nPrenons un exemple concret. Une ressource déclarée directement :\n```coffee\nresource \"aws_instance\" \"web_server\" {\n  # configuration...\n}\n```\n\nDevient, après refactorisation en module :\n```coffee\nmodule \"webserver\" {\n  source = \"./modules/webserver\"\n  # ...\n}\n```\n\nLa ressource prend alors l'adresse `module.webserver.aws_instance.web_server[0]` dans l'état. C'est exactement ce qu'on veut éviter en production : une destruction/recréation non planifiée.\n\n## terraform state mv\n\nLa commande `terraform state mv` permet de déplacer des ressources dans l'état sans les détruire/recréer.\n\n### Préparation de la refactorisation\n\nPour cette démonstration, nous allons partir d'un déploiement existant de la partie 8. Commençons par nous assurer que l'infrastructure est déployée :\n\n```bash\n# Assurez-vous d'avoir un état déployé depuis part8\ncd part8_count_loadbalancer\nterraform workspace select multi-server\nterraform plan -var-file=\"multi-server.tfvars\" -out=tfplan\nterraform apply tfplan\n\n# Listez les ressources actuelles\nterraform state list\n```\n\nCette commande vous montrera toutes les ressources déployées, organisées de manière monolithique. Vous devriez voir des ressources comme `aws_instance.web_server[0]`, `aws_lb.main[0]`, `aws_vpc.main`, etc.\n\n### Préparation de la refactorisation\n\nNous allons effectuer la refactorisation directement dans le projet part8. Cette approche est plus réaliste car elle utilise le backend S3 configuré :\n\n```bash\n# Travaillons directement depuis la part8 (sur le résultat de tp précédent)\n# faites un commit du résultat avant\n\n# Créons d'abord une sauvegarde de sécurité\nterraform state pull > terraform.tfstate.backup-$(date +%Y%m%d-%H%M%S)\n\n# Copions les modules depuis part9\ncp -r ../part9_refactorisation_modules/modules .\n\n# Remplaçons directement les fichiers par leurs versions modulaires\ncp ../part9_refactorisation_modules/main.tf .\ncp ../part9_refactorisation_modules/outputs.tf .\n\n# Supprimons les anciens fichiers maintenant obsolètes\nrm vpc.tf webserver.tf loadbalancer.tf\n```\n\n### Commentons le code modularisé\n\nNotre nouvelle configuration `main.tf` est maintenant beaucoup plus lisible et organisée. Au lieu d'avoir toutes les ressources déclarées au même endroit, nous utilisons trois modules distincts :\n\n```coffee\n# Module VPC\nmodule \"vpc\" {\n  source = \"./modules/vpc\"\n  \n  vpc_cidr             = var.vpc_cidr\n  public_subnet_cidr   = var.public_subnet_cidr\n  public_subnet_cidr_2 = var.public_subnet_cidr_2\n  workspace            = terraform.workspace\n  feature_name         = var.feature_name\n  instance_count       = var.instance_count\n}\n\n# Module Webserver\nmodule \"webserver\" {\n  source = \"./modules/webserver\"\n  \n  instance_count      = var.instance_count\n  instance_type       = var.instance_type\n  subnet_id           = module.vpc.public_subnet_ids[0]\n  security_group_id   = module.vpc.web_servers_security_group_id\n  ssh_key_path        = var.ssh_key_path\n  workspace           = terraform.workspace\n  feature_name        = var.feature_name\n}\n\n# Module Load Balancer\nmodule \"loadbalancer\" {\n  source = \"./modules/loadbalancer\"\n  \n  instance_count       = var.instance_count\n  workspace            = terraform.workspace\n  feature_name         = var.feature_name\n  vpc_id               = module.vpc.vpc_id\n  subnet_ids           = module.vpc.public_subnet_ids\n  security_group_ids   = module.vpc.alb_security_group_ids\n  instance_ids         = module.webserver.instance_ids\n}\n```\n\n**Points clés de cette approche modulaire :**\n\n**Séparation des responsabilités :** Chaque module a une responsabilité claire - le VPC gère le réseau, webserver gère les instances, loadbalancer gère la répartition de charge.\n\n**Communication entre modules :** Notez comment les modules communiquent entre eux via les outputs. Par exemple, `module.vpc.public_subnet_ids[0]` permet au module webserver d'utiliser le subnet créé par le module VPC.\n\n**Réutilisabilité :** Ces modules peuvent maintenant être réutilisés dans d'autres projets en changeant simplement les variables d'entrée.\n\n**Lisibilité :** Le fichier principal montre clairement l'architecture globale sans se perdre dans les détails de chaque ressource.\n\n\n### Test avant migration\n\nNos modules sont déjà créés dans `modules/` et notre configuration utilise déjà ces modules. Voyons ce que Terraform pense de nos changements :\n\n```bash\n# Terraform inir est nécessaire quand on créé de nouveau modules\nterraform init\n\n# Voyons ce que Terraform pense de nos changements\nterraform plan -var-file=\"multi-server.tfvars\"\n```\n\nTerraform va montrer qu'il veut détruire toutes les ressources existantes et en créer de nouvelles. C'est exactement le problème qu'on veut résoudre ! La solution est d'utiliser `terraform state mv` pour déplacer les ressources vers leur nouvelle adresse dans les modules.\n\n## Migration avec terraform state mv\n\nNous allons maintenant migrer chaque ressource de son ancienne adresse vers sa nouvelle adresse dans les modules. Cette opération doit être effectuée avec précaution et il est fortement recommandé de sauvegarder l'état avant de commencer.\n\n### Migration étape par étape\n\nLa migration s'effectue en trois étapes principales : le module VPC, le module webserver, et le module loadbalancer.\n\n**Migration du module VPC :**\n\n```bash\n# VPC principal\nterraform state mv aws_vpc.main module.vpc.aws_vpc.main\n\n# Internet Gateway\nterraform state mv aws_internet_gateway.main module.vpc.aws_internet_gateway.main\n\n# Subnets\nterraform state mv aws_subnet.public module.vpc.aws_subnet.public\nterraform state mv aws_subnet.public_2 module.vpc.aws_subnet.public_2\n\n# Route table et associations\nterraform state mv aws_route_table.public module.vpc.aws_route_table.public\nterraform state mv aws_route_table_association.public module.vpc.aws_route_table_association.public\nterraform state mv aws_route_table_association.public_2 module.vpc.aws_route_table_association.public_2\n\n# Security groups\nterraform state mv aws_security_group.web_servers module.vpc.aws_security_group.web_servers\nterraform state mv 'aws_security_group.alb[0]' 'module.vpc.aws_security_group.alb[0]'\n\n# Data source (sera recréé automatiquement)\nterraform state mv data.aws_availability_zones.available module.vpc.data.aws_availability_zones.available\n```\n\n**Migration du module webserver :**\n\n```bash\n# Instances web\nterraform state mv 'aws_instance.web_server[0]' 'module.webserver.aws_instance.web_server[0]'\nterraform state mv 'aws_instance.web_server[1]' 'module.webserver.aws_instance.web_server[1]'\nterraform state mv 'aws_instance.web_server[2]' 'module.webserver.aws_instance.web_server[2]'\n\n# Data source AMI\nterraform state mv data.aws_ami.custom_ubuntu module.webserver.data.aws_ami.custom_ubuntu\n```\n\n**Migration du module loadbalancer :**\n\n```bash\n# Load balancer\nterraform state mv 'aws_lb.main[0]' 'module.loadbalancer.aws_lb.main[0]'\n\n# Target group\nterraform state mv 'aws_lb_target_group.web_servers[0]' 'module.loadbalancer.aws_lb_target_group.web_servers[0]'\n\n# Target group attachments\nterraform state mv 'aws_lb_target_group_attachment.web_servers[0]' 'module.loadbalancer.aws_lb_target_group_attachment.web_servers[0]'\nterraform state mv 'aws_lb_target_group_attachment.web_servers[1]' 'module.loadbalancer.aws_lb_target_group_attachment.web_servers[1]'\nterraform state mv 'aws_lb_target_group_attachment.web_servers[2]' 'module.loadbalancer.aws_lb_target_group_attachment.web_servers[2]'\n\n# Listener\nterraform state mv 'aws_lb_listener.web[0]' 'module.loadbalancer.aws_lb_listener.web[0]'\n```\n\n### Vérification de la migration\n\nUne fois toutes les ressources migrées, vérifiez que l'opération s'est bien déroulée :\n\n```bash\n# Vérifiez que toutes les ressources ont été déplacées\nterraform state list\n\n# Testez le plan - il ne devrait plus y avoir de destruction/création\nterraform plan -var-file=\"multi-server.tfvars\"\n```\n\nSi tout s'est bien passé, le plan devrait montrer \"No changes\" ou seulement des modifications mineures ! Cela confirme que nos ressources sont maintenant correctement organisées en modules sans risque de destruction.\n\n## terraform import\n\nParfois, vous avez des ressources AWS créées manuellement ou par d'autres outils que vous voulez intégrer à Terraform. C'est là qu'intervient `terraform import`.\n\n### Exemple pratique : Importation d'un bucket S3\n\nSupposons que vous ayez créé manuellement un bucket S3 pour stocker des logs ou des fichiers, et que vous vouliez maintenant le gérer avec Terraform.\n\n**Création manuelle d'un bucket S3 :**\n\n```bash\n# Créons un bucket S3 avec un nom unique\naws s3 mb s3://terraform-demo-logs --region eu-west-3 --profile <awsprofile-votreprenom>\n\n# Vérifions que le bucket existe\naws s3 ls s3://terraform-demo-logs --profile <awsprofile-votreprenom>\n# bucket vide -> par de sortie\n```\n\n**Déclaration dans Terraform :**\n\nAjoutons maintenant la déclaration de ce bucket dans notre configuration Terraform. Ajoutez cette section à votre `main.tf` :\n\n```coffee\n# Bucket S3 pour les logs (créé manuellement, à importer)\nresource \"aws_s3_bucket\" \"logs\" {\n  bucket = \"terraform-demo-logs\"  # Remplacez par votre nom de bucket\n}\n\nresource \"aws_s3_bucket_versioning\" \"logs\" {\n  bucket = aws_s3_bucket.logs.id\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n```\n\n**Importation dans Terraform :**\n\n```bash\n# Importez le bucket existant\nterraform import aws_s3_bucket.logs terraform-demo-logs\n\n# Vérifiez que l'import a fonctionné\nterraform state list\nterraform plan -var-file=\"multi-server.tfvars\" -out=new-bucket.tfplan\n```\n\nLe plan devrait montrer que Terraform veut ajouter le versioning (qui n'existait pas sur le bucket créé manuellement) mais ne veut pas recréer le bucket lui-même.\n\n**Finalisation de la configuration :**\n\n```bash\n# Appliquez les modifications pour ajouter le versioning\nterraform apply new-bucket.tfplan\n\n# Vérifiez que tout fonctionne\nterraform state list | grep aws_s3_bucket.logs\n```\n\nCette opération intègre une ressource existante dans l'état Terraform, permettant de la gérer ensuite via les fichiers de configuration. C'est particulièrement utile pour intégrer des ressources créées avant l'adoption de Terraform ou par d'autres équipes.\n\n## Validation finale\n\nUne fois la migration terminée, il est important de valider que tout fonctionne correctement :\n\n```bash\n# Plan final - devrait montrer \"No changes\"\nterraform plan -var-file=\"multi-server.tfvars\"\n\n# Test de l'application\ncurl $(terraform output -raw web_url)\n\n# Vérification des modules\nterraform validate\n```\n\n## Bonnes pratiques pour la gestion d'état\n\nAvant toute refactorisation ou manipulation d'état, il est essentiel de suivre certaines bonnes pratiques pour éviter toute perte de données ou interruption de service.\n\n### Avant toute refactorisation\n\n**Sauvegardez toujours l'état :**\n\n```bash\n# Sauvegarde manuelle\ncp terraform.tfstate terraform.tfstate.backup-$(date +%Y%m%d-%H%M%S)\n\n# Ou exportez l'état\nterraform show -json > state-backup-$(date +%Y%m%d-%H%M%S).json\n```\n\n**Idéalement testez sur un workspace séparé :**\n\n...donc sur une copie infra de test c'est l'avantage du cloud et de 'IaC de pouvoir faire des copies temporaire d'une infra. Mais ce n'est pas toujours possible selon les dépendances de votre système et les outils de réplication de son état s'il est stateful.\n\n```bash\n# Créez un workspace de test\nterraform workspace new refactoring-test\nterraform apply -var-file=\"multi-server.tfvars\"\n\n# Effectuez la migration sur ce workspace \n# Si ça marche, appliquez sur le workspace principal\n```\n\n**Planifiez la migration :**\n\n```bash\n# Listez toutes les ressources\nterraform state list > resources-before.txt\n\n# Après migration, comparez\nterraform state list > resources-after.txt\ndiff resources-before.txt resources-after.txt\n```\n\n## Conclusion\n\nCette partie vous a montré comment refactoriser une infrastructure monolithique en modules sans interruption de service. Nous avons exploré deux techniques essentielles : `terraform state mv` pour déplacer des ressources dans l'état et `terraform import` pour intégrer des ressources existantes.\n\nLes points clés à retenir sont la nécessité de toujours sauvegarder l'état avant une refactorisation, de tester sur un workspace séparé, et d'utiliser ces commandes pour éviter les destructions/recréations non désirées. La modularisation améliore considérablement la réutilisabilité et la maintenance des infrastructures Terraform.\n\nCes techniques sont essentielles pour maintenir et faire évoluer des infrastructures Terraform en production de manière sûre et contrôlée.",
      "html": "<p>Dans cette neuvième partie, nous allons apprendre à refactoriser notre infrastructure en modules Terraform et maîtriser les techniques avancées de gestion d&#39;état : <code>terraform state mv</code> et <code>terraform import</code>.</p>\n<h2>Problématiques de la refactorisation</h2>\n<p>Quand on refactorise du code Terraform existant, on se heurte à un problème fondamental : <strong>Terraform suit les ressources par leur adresse dans l&#39;état</strong>. Si vous avez une ressource déclarée directement puis que vous la déplacez dans un module, Terraform ne reconnaît pas que c&#39;est la même ressource ! Il va vouloir détruire l&#39;ancienne et créer une nouvelle, ce qui peut entraîner une interruption de service.</p>\n<p>Prenons un exemple concret. Une ressource déclarée directement :</p>\n<pre><code class=\"language-coffee\">resource &quot;aws_instance&quot; &quot;web_server&quot; {\n  # configuration...\n}\n</code></pre>\n<p>Devient, après refactorisation en module :</p>\n<pre><code class=\"language-coffee\">module &quot;webserver&quot; {\n  source = &quot;./modules/webserver&quot;\n  # ...\n}\n</code></pre>\n<p>La ressource prend alors l&#39;adresse <code>module.webserver.aws_instance.web_server[0]</code> dans l&#39;état. C&#39;est exactement ce qu&#39;on veut éviter en production : une destruction/recréation non planifiée.</p>\n<h2>terraform state mv</h2>\n<p>La commande <code>terraform state mv</code> permet de déplacer des ressources dans l&#39;état sans les détruire/recréer.</p>\n<h3>Préparation de la refactorisation</h3>\n<p>Pour cette démonstration, nous allons partir d&#39;un déploiement existant de la partie 8. Commençons par nous assurer que l&#39;infrastructure est déployée :</p>\n<pre><code class=\"language-bash\"># Assurez-vous d&#39;avoir un état déployé depuis part8\ncd part8_count_loadbalancer\nterraform workspace select multi-server\nterraform plan -var-file=&quot;multi-server.tfvars&quot; -out=tfplan\nterraform apply tfplan\n\n# Listez les ressources actuelles\nterraform state list\n</code></pre>\n<p>Cette commande vous montrera toutes les ressources déployées, organisées de manière monolithique. Vous devriez voir des ressources comme <code>aws_instance.web_server[0]</code>, <code>aws_lb.main[0]</code>, <code>aws_vpc.main</code>, etc.</p>\n<h3>Préparation de la refactorisation</h3>\n<p>Nous allons effectuer la refactorisation directement dans le projet part8. Cette approche est plus réaliste car elle utilise le backend S3 configuré :</p>\n<pre><code class=\"language-bash\"># Travaillons directement depuis la part8 (sur le résultat de tp précédent)\n# faites un commit du résultat avant\n\n# Créons d&#39;abord une sauvegarde de sécurité\nterraform state pull &gt; terraform.tfstate.backup-$(date +%Y%m%d-%H%M%S)\n\n# Copions les modules depuis part9\ncp -r ../part9_refactorisation_modules/modules .\n\n# Remplaçons directement les fichiers par leurs versions modulaires\ncp ../part9_refactorisation_modules/main.tf .\ncp ../part9_refactorisation_modules/outputs.tf .\n\n# Supprimons les anciens fichiers maintenant obsolètes\nrm vpc.tf webserver.tf loadbalancer.tf\n</code></pre>\n<h3>Commentons le code modularisé</h3>\n<p>Notre nouvelle configuration <code>main.tf</code> est maintenant beaucoup plus lisible et organisée. Au lieu d&#39;avoir toutes les ressources déclarées au même endroit, nous utilisons trois modules distincts :</p>\n<pre><code class=\"language-coffee\"># Module VPC\nmodule &quot;vpc&quot; {\n  source = &quot;./modules/vpc&quot;\n  \n  vpc_cidr             = var.vpc_cidr\n  public_subnet_cidr   = var.public_subnet_cidr\n  public_subnet_cidr_2 = var.public_subnet_cidr_2\n  workspace            = terraform.workspace\n  feature_name         = var.feature_name\n  instance_count       = var.instance_count\n}\n\n# Module Webserver\nmodule &quot;webserver&quot; {\n  source = &quot;./modules/webserver&quot;\n  \n  instance_count      = var.instance_count\n  instance_type       = var.instance_type\n  subnet_id           = module.vpc.public_subnet_ids[0]\n  security_group_id   = module.vpc.web_servers_security_group_id\n  ssh_key_path        = var.ssh_key_path\n  workspace           = terraform.workspace\n  feature_name        = var.feature_name\n}\n\n# Module Load Balancer\nmodule &quot;loadbalancer&quot; {\n  source = &quot;./modules/loadbalancer&quot;\n  \n  instance_count       = var.instance_count\n  workspace            = terraform.workspace\n  feature_name         = var.feature_name\n  vpc_id               = module.vpc.vpc_id\n  subnet_ids           = module.vpc.public_subnet_ids\n  security_group_ids   = module.vpc.alb_security_group_ids\n  instance_ids         = module.webserver.instance_ids\n}\n</code></pre>\n<p><strong>Points clés de cette approche modulaire :</strong></p>\n<p><strong>Séparation des responsabilités :</strong> Chaque module a une responsabilité claire - le VPC gère le réseau, webserver gère les instances, loadbalancer gère la répartition de charge.</p>\n<p><strong>Communication entre modules :</strong> Notez comment les modules communiquent entre eux via les outputs. Par exemple, <code>module.vpc.public_subnet_ids[0]</code> permet au module webserver d&#39;utiliser le subnet créé par le module VPC.</p>\n<p><strong>Réutilisabilité :</strong> Ces modules peuvent maintenant être réutilisés dans d&#39;autres projets en changeant simplement les variables d&#39;entrée.</p>\n<p><strong>Lisibilité :</strong> Le fichier principal montre clairement l&#39;architecture globale sans se perdre dans les détails de chaque ressource.</p>\n<h3>Test avant migration</h3>\n<p>Nos modules sont déjà créés dans <code>modules/</code> et notre configuration utilise déjà ces modules. Voyons ce que Terraform pense de nos changements :</p>\n<pre><code class=\"language-bash\"># Terraform inir est nécessaire quand on créé de nouveau modules\nterraform init\n\n# Voyons ce que Terraform pense de nos changements\nterraform plan -var-file=&quot;multi-server.tfvars&quot;\n</code></pre>\n<p>Terraform va montrer qu&#39;il veut détruire toutes les ressources existantes et en créer de nouvelles. C&#39;est exactement le problème qu&#39;on veut résoudre ! La solution est d&#39;utiliser <code>terraform state mv</code> pour déplacer les ressources vers leur nouvelle adresse dans les modules.</p>\n<h2>Migration avec terraform state mv</h2>\n<p>Nous allons maintenant migrer chaque ressource de son ancienne adresse vers sa nouvelle adresse dans les modules. Cette opération doit être effectuée avec précaution et il est fortement recommandé de sauvegarder l&#39;état avant de commencer.</p>\n<h3>Migration étape par étape</h3>\n<p>La migration s&#39;effectue en trois étapes principales : le module VPC, le module webserver, et le module loadbalancer.</p>\n<p><strong>Migration du module VPC :</strong></p>\n<pre><code class=\"language-bash\"># VPC principal\nterraform state mv aws_vpc.main module.vpc.aws_vpc.main\n\n# Internet Gateway\nterraform state mv aws_internet_gateway.main module.vpc.aws_internet_gateway.main\n\n# Subnets\nterraform state mv aws_subnet.public module.vpc.aws_subnet.public\nterraform state mv aws_subnet.public_2 module.vpc.aws_subnet.public_2\n\n# Route table et associations\nterraform state mv aws_route_table.public module.vpc.aws_route_table.public\nterraform state mv aws_route_table_association.public module.vpc.aws_route_table_association.public\nterraform state mv aws_route_table_association.public_2 module.vpc.aws_route_table_association.public_2\n\n# Security groups\nterraform state mv aws_security_group.web_servers module.vpc.aws_security_group.web_servers\nterraform state mv &#39;aws_security_group.alb[0]&#39; &#39;module.vpc.aws_security_group.alb[0]&#39;\n\n# Data source (sera recréé automatiquement)\nterraform state mv data.aws_availability_zones.available module.vpc.data.aws_availability_zones.available\n</code></pre>\n<p><strong>Migration du module webserver :</strong></p>\n<pre><code class=\"language-bash\"># Instances web\nterraform state mv &#39;aws_instance.web_server[0]&#39; &#39;module.webserver.aws_instance.web_server[0]&#39;\nterraform state mv &#39;aws_instance.web_server[1]&#39; &#39;module.webserver.aws_instance.web_server[1]&#39;\nterraform state mv &#39;aws_instance.web_server[2]&#39; &#39;module.webserver.aws_instance.web_server[2]&#39;\n\n# Data source AMI\nterraform state mv data.aws_ami.custom_ubuntu module.webserver.data.aws_ami.custom_ubuntu\n</code></pre>\n<p><strong>Migration du module loadbalancer :</strong></p>\n<pre><code class=\"language-bash\"># Load balancer\nterraform state mv &#39;aws_lb.main[0]&#39; &#39;module.loadbalancer.aws_lb.main[0]&#39;\n\n# Target group\nterraform state mv &#39;aws_lb_target_group.web_servers[0]&#39; &#39;module.loadbalancer.aws_lb_target_group.web_servers[0]&#39;\n\n# Target group attachments\nterraform state mv &#39;aws_lb_target_group_attachment.web_servers[0]&#39; &#39;module.loadbalancer.aws_lb_target_group_attachment.web_servers[0]&#39;\nterraform state mv &#39;aws_lb_target_group_attachment.web_servers[1]&#39; &#39;module.loadbalancer.aws_lb_target_group_attachment.web_servers[1]&#39;\nterraform state mv &#39;aws_lb_target_group_attachment.web_servers[2]&#39; &#39;module.loadbalancer.aws_lb_target_group_attachment.web_servers[2]&#39;\n\n# Listener\nterraform state mv &#39;aws_lb_listener.web[0]&#39; &#39;module.loadbalancer.aws_lb_listener.web[0]&#39;\n</code></pre>\n<h3>Vérification de la migration</h3>\n<p>Une fois toutes les ressources migrées, vérifiez que l&#39;opération s&#39;est bien déroulée :</p>\n<pre><code class=\"language-bash\"># Vérifiez que toutes les ressources ont été déplacées\nterraform state list\n\n# Testez le plan - il ne devrait plus y avoir de destruction/création\nterraform plan -var-file=&quot;multi-server.tfvars&quot;\n</code></pre>\n<p>Si tout s&#39;est bien passé, le plan devrait montrer &quot;No changes&quot; ou seulement des modifications mineures ! Cela confirme que nos ressources sont maintenant correctement organisées en modules sans risque de destruction.</p>\n<h2>terraform import</h2>\n<p>Parfois, vous avez des ressources AWS créées manuellement ou par d&#39;autres outils que vous voulez intégrer à Terraform. C&#39;est là qu&#39;intervient <code>terraform import</code>.</p>\n<h3>Exemple pratique : Importation d&#39;un bucket S3</h3>\n<p>Supposons que vous ayez créé manuellement un bucket S3 pour stocker des logs ou des fichiers, et que vous vouliez maintenant le gérer avec Terraform.</p>\n<p><strong>Création manuelle d&#39;un bucket S3 :</strong></p>\n<pre><code class=\"language-bash\"># Créons un bucket S3 avec un nom unique\naws s3 mb s3://terraform-demo-logs --region eu-west-3 --profile &lt;awsprofile-votreprenom&gt;\n\n# Vérifions que le bucket existe\naws s3 ls s3://terraform-demo-logs --profile &lt;awsprofile-votreprenom&gt;\n# bucket vide -&gt; par de sortie\n</code></pre>\n<p><strong>Déclaration dans Terraform :</strong></p>\n<p>Ajoutons maintenant la déclaration de ce bucket dans notre configuration Terraform. Ajoutez cette section à votre <code>main.tf</code> :</p>\n<pre><code class=\"language-coffee\"># Bucket S3 pour les logs (créé manuellement, à importer)\nresource &quot;aws_s3_bucket&quot; &quot;logs&quot; {\n  bucket = &quot;terraform-demo-logs&quot;  # Remplacez par votre nom de bucket\n}\n\nresource &quot;aws_s3_bucket_versioning&quot; &quot;logs&quot; {\n  bucket = aws_s3_bucket.logs.id\n  versioning_configuration {\n    status = &quot;Enabled&quot;\n  }\n}\n</code></pre>\n<p><strong>Importation dans Terraform :</strong></p>\n<pre><code class=\"language-bash\"># Importez le bucket existant\nterraform import aws_s3_bucket.logs terraform-demo-logs\n\n# Vérifiez que l&#39;import a fonctionné\nterraform state list\nterraform plan -var-file=&quot;multi-server.tfvars&quot; -out=new-bucket.tfplan\n</code></pre>\n<p>Le plan devrait montrer que Terraform veut ajouter le versioning (qui n&#39;existait pas sur le bucket créé manuellement) mais ne veut pas recréer le bucket lui-même.</p>\n<p><strong>Finalisation de la configuration :</strong></p>\n<pre><code class=\"language-bash\"># Appliquez les modifications pour ajouter le versioning\nterraform apply new-bucket.tfplan\n\n# Vérifiez que tout fonctionne\nterraform state list | grep aws_s3_bucket.logs\n</code></pre>\n<p>Cette opération intègre une ressource existante dans l&#39;état Terraform, permettant de la gérer ensuite via les fichiers de configuration. C&#39;est particulièrement utile pour intégrer des ressources créées avant l&#39;adoption de Terraform ou par d&#39;autres équipes.</p>\n<h2>Validation finale</h2>\n<p>Une fois la migration terminée, il est important de valider que tout fonctionne correctement :</p>\n<pre><code class=\"language-bash\"># Plan final - devrait montrer &quot;No changes&quot;\nterraform plan -var-file=&quot;multi-server.tfvars&quot;\n\n# Test de l&#39;application\ncurl $(terraform output -raw web_url)\n\n# Vérification des modules\nterraform validate\n</code></pre>\n<h2>Bonnes pratiques pour la gestion d&#39;état</h2>\n<p>Avant toute refactorisation ou manipulation d&#39;état, il est essentiel de suivre certaines bonnes pratiques pour éviter toute perte de données ou interruption de service.</p>\n<h3>Avant toute refactorisation</h3>\n<p><strong>Sauvegardez toujours l&#39;état :</strong></p>\n<pre><code class=\"language-bash\"># Sauvegarde manuelle\ncp terraform.tfstate terraform.tfstate.backup-$(date +%Y%m%d-%H%M%S)\n\n# Ou exportez l&#39;état\nterraform show -json &gt; state-backup-$(date +%Y%m%d-%H%M%S).json\n</code></pre>\n<p><strong>Idéalement testez sur un workspace séparé :</strong></p>\n<p>...donc sur une copie infra de test c&#39;est l&#39;avantage du cloud et de &#39;IaC de pouvoir faire des copies temporaire d&#39;une infra. Mais ce n&#39;est pas toujours possible selon les dépendances de votre système et les outils de réplication de son état s&#39;il est stateful.</p>\n<pre><code class=\"language-bash\"># Créez un workspace de test\nterraform workspace new refactoring-test\nterraform apply -var-file=&quot;multi-server.tfvars&quot;\n\n# Effectuez la migration sur ce workspace \n# Si ça marche, appliquez sur le workspace principal\n</code></pre>\n<p><strong>Planifiez la migration :</strong></p>\n<pre><code class=\"language-bash\"># Listez toutes les ressources\nterraform state list &gt; resources-before.txt\n\n# Après migration, comparez\nterraform state list &gt; resources-after.txt\ndiff resources-before.txt resources-after.txt\n</code></pre>\n<h2>Conclusion</h2>\n<p>Cette partie vous a montré comment refactoriser une infrastructure monolithique en modules sans interruption de service. Nous avons exploré deux techniques essentielles : <code>terraform state mv</code> pour déplacer des ressources dans l&#39;état et <code>terraform import</code> pour intégrer des ressources existantes.</p>\n<p>Les points clés à retenir sont la nécessité de toujours sauvegarder l&#39;état avant une refactorisation, de tester sur un workspace séparé, et d&#39;utiliser ces commandes pour éviter les destructions/recréations non désirées. La modularisation améliore considérablement la réutilisabilité et la maintenance des infrastructures Terraform.</p>\n<p>Ces techniques sont essentielles pour maintenir et faire évoluer des infrastructures Terraform en production de manière sûre et contrôlée.</p>\n"
    }
  ],
  "slugs": [
    "part1_simple_aws_server",
    "part2_packer_ami",
    "part3_terraform_provisioner",
    "part4_simple_vpc",
    "part5_terraform_organization",
    "partmaybe_vpc_networking",
    "part6_terraform_state_workspaces",
    "part7_backend_s3",
    "part8_count_loadbalancer",
    "part9_refactorisation_modules"
  ],
  "lastGenerated": "2025-07-05T12:15:54.441Z"
}